{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Jupyter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigo/.local/share/virtualenvs/tfm-experiments-K5nk3NK1/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import date\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from arguments_rl import get_args\n",
    "\n",
    "from collections import deque\n",
    "from rl_module.a2c_ppo_acktr.envs import make_vec_envs\n",
    "from rl_module.a2c_ppo_acktr.storage import RolloutStorage\n",
    "from rl_module.train_ppo import train_ppo\n",
    "from rl_module.a2c_ppo_acktr.utils import seed\n",
    "from torch_ac.utils import DictList\n",
    "\n",
    "# Gym MiniGrid specific\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper, RGBImgObsWrapper\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppo', 'experiment': 'minigrid', 'approach': 'fine-tuning', 'optimizer': 'RMSProp', 'gail': False, 'gail_experts_dir': './gail_experts', 'gail_batch_size': 128, 'gail_epoch': 5, 'lr': 0.0007, 'eps': 1e-08, 'gamma': 0.99, 'use_gae': True, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'seed': 1, 'cuda_deterministic': False, 'num_processes': 16, 'num_steps': 128, 'ppo_epoch': 4, 'num_mini_batch': 256, 'clip_param': 0.2, 'log_interval': 1, 'save_interval': 10, 'eval_interval': 10, 'num_env_steps': 100000.0, 'env_name': 'PongNoFrameskip-v4', 'log_dir': './logs/', 'save_dir': './trained_models/', 'no_cuda': True, 'use_proper_time_limits': False, 'recurrent_policy': False, 'use_linear_lr_decay': False, 'ewc_lambda': 5000.0, 'ewc_online': False, 'ewc_epochs': 100, 'num_ewc_steps': 20, 'save_name': None, 'date': datetime.date(2023, 1, 10), 'task_id': None, 'single_task': False, 'F_prior': 1e-15, 'input_padding': False, 'sample': False, 'samples': 1, 'render_ckpt_path': '', 'render_task_idx': 0, 'num_render_traj': 1000, 'cuda': False}\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "'algo':'ppo',\n",
    "'experiment':'minigrid',\n",
    "'approach':'fine-tuning',#'blip',\n",
    "'optimizer':'RMSProp',#'Adam',\n",
    "'gail':False,\n",
    "'gail_experts_dir':'./gail_experts',\n",
    "'gail_batch_size':128,\n",
    "'gail_epoch':5,\n",
    "'lr':7e-4,#2.5e-4,\n",
    "'eps':1e-8,#1e-5,\n",
    "'gamma':0.99,\n",
    "'use_gae':True,\n",
    "'gae_lambda':0.99,#0.95,\n",
    "'entropy_coef':0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'seed':1,\n",
    "'cuda_deterministic':False,\n",
    "'num_processes':16,\n",
    "'num_steps':128,#5,\n",
    "'ppo_epoch':4,\n",
    "'num_mini_batch':256,#8,#32,\n",
    "'clip_param':0.2,#0.1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'eval_interval':10,\n",
    "'num_env_steps':1e5,\n",
    "'env_name':'PongNoFrameskip-v4',\n",
    "'log_dir':'./logs/',\n",
    "'save_dir':'./trained_models/',\n",
    "'no_cuda':True,\n",
    "'use_proper_time_limits':False,\n",
    "'recurrent_policy':False,\n",
    "'use_linear_lr_decay':False,\n",
    "'ewc_lambda':5000.0,\n",
    "'ewc_online':False,\n",
    "'ewc_epochs':100,\n",
    "'num_ewc_steps':20,\n",
    "'save_name':None,\n",
    "'date':date.today(),\n",
    "'task_id':None,\n",
    "'single_task':False,\n",
    "'F_prior':1e-15,\n",
    "'input_padding':False,\n",
    "'sample':False,\n",
    "'samples':1,\n",
    "# render arguments\n",
    "'render_ckpt_path':'',\n",
    "'render_task_idx':0,\n",
    "'num_render_traj':1000\n",
    "}\n",
    "\n",
    "args = DictList(args)\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "assert args.algo in ['a2c', 'ppo', 'acktr']\n",
    "if args.recurrent_policy:\n",
    "    assert args.algo in ['a2c', 'ppo'], \\\n",
    "        'Recurrent policy is not implemented for ACKTR'\n",
    "conv_experiment = [\n",
    "    'atari',\n",
    "]\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "\n",
    "if args.approach == 'fine-tuning' or args.approach == 'ft-fix':\n",
    "    log_name = '{}_{}_{}_{}'.format(args.date, args.experiment, args.approach,args.seed)\n",
    "elif args.approach == 'ewc' in args.approach:\n",
    "    log_name = '{}_{}_{}_{}_lamb_{}'.format(args.date, args.experiment, args.approach, args.seed, args.ewc_lambda)\n",
    "elif args.approach == 'blip':\n",
    "    log_name = '{}_{}_{}_{}_F_prior_{}'.format(args.date, args.experiment, args.approach, args.seed, args.F_prior)\n",
    "\n",
    "if args.experiment in conv_experiment:\n",
    "    log_name = log_name + '_conv'\n",
    "\n",
    "# Seed\n",
    "seed(args.seed)\n",
    "\n",
    "# Inits\n",
    "if args.cuda:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "#taskcla = [(0,14), (1,18), (2,18), (3,18), (4,18), (5,6)]\n",
    "#task_sequences = [(0, 'KungFuMasterNoFrameskip-v4'), (1, 'BoxingNoFrameskip-v4'), (2, 'JamesbondNoFrameskip-v4'), (3, 'KrullNoFrameskip-v4'), (4, 'RiverraidNoFrameskip-v4'), (5, 'SpaceInvadersNoFrameskip-v4')]\n",
    "\n",
    "taskcla = [(0,7), (1,7)]\n",
    "task_sequences = [(0, 'MiniGrid-Empty-5x5-v0'), (1, 'MiniGrid-Empty-Random-6x6-v0')]\n",
    "\n",
    "# hard coded for atari environment\n",
    "#obs_shape = (4,84,84)\n",
    "\n",
    "# for FlatObsWrapper Minigrid environment\n",
    "obs_shape = (2739,)\n",
    "\n",
    "if args.approach == 'blip':\n",
    "    from rl_module.ppo_model import QPolicy\n",
    "    print('using fisher prior of: ', args.F_prior)\n",
    "    actor_critic = QPolicy(obs_shape,\n",
    "        taskcla,\n",
    "        base_kwargs={'F_prior': args.F_prior, 'recurrent': args.recurrent_policy}).to(device)\n",
    "else:\n",
    "    from rl_module.ppo_model import Policy\n",
    "    actor_critic = Policy(obs_shape,\n",
    "        taskcla,\n",
    "        base_kwargs={'recurrent': args.recurrent_policy}).to(device)\n",
    "\n",
    "# Args -- Approach\n",
    "if args.approach == 'fine-tuning' or args.approach == 'ft-fix':\n",
    "    from rl_module.ppo import PPO as approach\n",
    "\n",
    "    agent = approach(actor_critic,\n",
    "            args.clip_param,\n",
    "            args.ppo_epoch,\n",
    "            args.num_mini_batch,\n",
    "            args.value_loss_coef,\n",
    "            args.entropy_coef,\n",
    "            lr=args.lr,\n",
    "            eps=args.eps,\n",
    "            max_grad_norm=args.max_grad_norm,\n",
    "            use_clipped_value_loss=True,\n",
    "            optimizer=args.optimizer)\n",
    "elif args.approach == 'ewc':\n",
    "    from rl_module.ppo_ewc import PPO_EWC as approach\n",
    "\n",
    "    agent = approach(\n",
    "        actor_critic,\n",
    "        args.clip_param,\n",
    "        args.ppo_epoch,\n",
    "        args.num_mini_batch,\n",
    "        args.value_loss_coef,\n",
    "        args.entropy_coef,\n",
    "        lr=args.lr,\n",
    "        eps=args.eps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        use_clipped_value_loss=True,\n",
    "        ewc_lambda= args.ewc_lambda,\n",
    "        online = args.ewc_online)\n",
    "\n",
    "elif args.approach == 'blip':\n",
    "    from rl_module.ppo_blip import PPO_BLIP as approach\n",
    "\n",
    "    agent = approach(\n",
    "        actor_critic,\n",
    "        args.clip_param,\n",
    "        args.ppo_epoch,\n",
    "        args.num_mini_batch,\n",
    "        args.value_loss_coef,\n",
    "        args.entropy_coef,\n",
    "        lr=args.lr,\n",
    "        eps=args.eps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        use_clipped_value_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniGrid-Empty-5x5-v0\n",
      " Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/48 [00:03<02:31,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 0, num timesteps 2048, FPS 633 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.6\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/48 [00:06<02:27,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 1, num timesteps 4096, FPS 637 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 3/48 [00:09<02:23,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 2, num timesteps 6144, FPS 641 \n",
      " Last 10 training episodes: mean/median reward 0.3/0.2, min/max reward 0.0/0.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/48 [00:12<02:20,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 3, num timesteps 8192, FPS 640 \n",
      " Last 10 training episodes: mean/median reward 0.5/0.5, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/48 [00:15<02:16,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 4, num timesteps 10240, FPS 642 \n",
      " Last 10 training episodes: mean/median reward 0.7/0.7, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 6/48 [00:19<02:12,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 5, num timesteps 12288, FPS 644 \n",
      " Last 10 training episodes: mean/median reward 0.8/0.9, min/max reward 0.3/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 7/48 [00:22<02:09,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 6, num timesteps 14336, FPS 645 \n",
      " Last 10 training episodes: mean/median reward 0.8/0.8, min/max reward 0.5/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 8/48 [00:25<02:06,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 7, num timesteps 16384, FPS 644 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 9/48 [00:28<02:03,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 8, num timesteps 18432, FPS 645 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/0.9\n",
      "\n",
      "Updates 9, num timesteps 20480, FPS 644 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 10/48 [00:37<03:07,  4.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 11/48 [00:40<02:45,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 10, num timesteps 22528, FPS 551 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 12/48 [00:44<02:26,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 11, num timesteps 24576, FPS 558 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 13/48 [00:47<02:13,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 12, num timesteps 26624, FPS 563 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 14/48 [00:50<02:03,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 13, num timesteps 28672, FPS 568 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 15/48 [00:53<01:55,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 14, num timesteps 30720, FPS 572 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16/48 [00:56<01:49,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 15, num timesteps 32768, FPS 576 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 17/48 [01:00<01:44,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 16, num timesteps 34816, FPS 578 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 18/48 [01:03<01:39,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 17, num timesteps 36864, FPS 582 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 19/48 [01:06<01:34,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 18, num timesteps 38912, FPS 585 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n",
      "Updates 19, num timesteps 40960, FPS 587 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 20/48 [01:15<02:19,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 21/48 [01:18<02:01,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 20, num timesteps 43008, FPS 545 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 22/48 [01:21<01:46,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 21, num timesteps 45056, FPS 549 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 23/48 [01:25<01:35,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 22, num timesteps 47104, FPS 552 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 24/48 [01:28<01:27,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 23, num timesteps 49152, FPS 556 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 25/48 [01:31<01:20,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 24, num timesteps 51200, FPS 559 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 26/48 [01:34<01:14,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 25, num timesteps 53248, FPS 562 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 27/48 [01:37<01:10,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 26, num timesteps 55296, FPS 564 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 28/48 [01:41<01:05,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 27, num timesteps 57344, FPS 567 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 29/48 [01:44<01:01,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 28, num timesteps 59392, FPS 569 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n",
      "Updates 29, num timesteps 61440, FPS 571 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 30/48 [01:52<01:26,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation using 10 episodes: mean reward 0.95500 \n",
      "\n",
      "len task_sequences :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 31/48 [01:56<01:14,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 30, num timesteps 63488, FPS 546 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 32/48 [01:59<01:04,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 31, num timesteps 65536, FPS 549 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 33/48 [02:02<00:56,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 32, num timesteps 67584, FPS 551 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 34/48 [02:05<00:50,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 33, num timesteps 69632, FPS 554 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 35/48 [02:08<00:45,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 34, num timesteps 71680, FPS 556 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 36/48 [02:12<00:40,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 35, num timesteps 73728, FPS 558 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 37/48 [02:15<00:36,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 36, num timesteps 75776, FPS 560 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 38/48 [02:18<00:32,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 37, num timesteps 77824, FPS 562 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 39/48 [02:21<00:29,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 38, num timesteps 79872, FPS 563 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n",
      "Updates 39, num timesteps 81920, FPS 565 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 40/48 [02:29<00:38,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation using 10 episodes: mean reward 0.95500 \n",
      "\n",
      "len task_sequences :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 41/48 [02:33<00:30,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 40, num timesteps 83968, FPS 547 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 42/48 [02:36<00:24,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 41, num timesteps 86016, FPS 549 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 43/48 [02:39<00:18,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 42, num timesteps 88064, FPS 551 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 44/48 [02:42<00:14,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 43, num timesteps 90112, FPS 553 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 45/48 [02:46<00:10,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 44, num timesteps 92160, FPS 554 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 46/48 [02:49<00:06,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 45, num timesteps 94208, FPS 556 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 47/48 [02:52<00:03,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 46, num timesteps 96256, FPS 558 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.7/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [02:55<00:00,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 47, num timesteps 98304, FPS 559 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n",
      "MiniGrid-Empty-Random-6x6-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation using 10 episodes: mean reward 0.95500 \n",
      "\n",
      " Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/48 [00:03<02:37,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 0, num timesteps 2048, FPS 612 \n",
      " Last 7 training episodes: mean/median reward 0.8/0.8, min/max reward 0.5/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/48 [00:06<02:30,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 1, num timesteps 4096, FPS 622 \n",
      " Last 10 training episodes: mean/median reward 0.7/0.7, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 3/48 [00:09<02:27,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 2, num timesteps 6144, FPS 622 \n",
      " Last 10 training episodes: mean/median reward 0.6/0.7, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/48 [00:13<02:23,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 3, num timesteps 8192, FPS 625 \n",
      " Last 10 training episodes: mean/median reward 0.6/0.7, min/max reward 0.1/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/48 [00:16<02:20,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 4, num timesteps 10240, FPS 626 \n",
      " Last 10 training episodes: mean/median reward 0.7/0.8, min/max reward 0.3/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 6/48 [00:19<02:17,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 5, num timesteps 12288, FPS 625 \n",
      " Last 10 training episodes: mean/median reward 0.7/0.8, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 7/48 [00:22<02:15,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 6, num timesteps 14336, FPS 623 \n",
      " Last 10 training episodes: mean/median reward 0.8/0.9, min/max reward 0.6/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 8/48 [00:26<02:11,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 7, num timesteps 16384, FPS 623 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.7/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 9/48 [00:29<02:08,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 8, num timesteps 18432, FPS 622 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.6/1.0\n",
      "\n",
      "Updates 9, num timesteps 20480, FPS 621 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 0.95500 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 10/48 [00:44<04:18,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation using 10 episodes: mean reward 0.39062 \n",
      "\n",
      "len task_sequences :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 11/48 [00:47<03:33,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 10, num timesteps 22528, FPS 472 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 12/48 [00:51<03:02,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 11, num timesteps 24576, FPS 480 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 13/48 [00:54<02:39,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 12, num timesteps 26624, FPS 488 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 14/48 [00:57<02:21,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 13, num timesteps 28672, FPS 496 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 15/48 [01:01<02:09,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 14, num timesteps 30720, FPS 502 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 16/48 [01:04<01:59,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 15, num timesteps 32768, FPS 508 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 17/48 [01:07<01:52,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 16, num timesteps 34816, FPS 513 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 18/48 [01:11<01:46,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 17, num timesteps 36864, FPS 518 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 19/48 [01:14<01:40,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 18, num timesteps 38912, FPS 522 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n",
      "Updates 19, num timesteps 40960, FPS 526 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 0.95500 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 20/48 [01:28<03:06,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation using 10 episodes: mean reward 0.97062 \n",
      "\n",
      "len task_sequences :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 21/48 [01:32<02:34,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 20, num timesteps 43008, FPS 467 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 22/48 [01:35<02:10,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 21, num timesteps 45056, FPS 472 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 23/48 [01:38<01:52,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 22, num timesteps 47104, FPS 477 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 24/48 [01:42<01:39,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 23, num timesteps 49152, FPS 481 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 25/48 [01:45<01:30,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 24, num timesteps 51200, FPS 485 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 26/48 [01:48<01:22,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 25, num timesteps 53248, FPS 489 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 27/48 [01:52<01:16,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 26, num timesteps 55296, FPS 493 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 28/48 [01:55<01:12,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 27, num timesteps 57344, FPS 495 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 29/48 [01:59<01:07,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 28, num timesteps 59392, FPS 498 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n",
      "Updates 29, num timesteps 61440, FPS 501 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 0.95500 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 30/48 [02:13<02:00,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation using 10 episodes: mean reward 0.97313 \n",
      "\n",
      "len task_sequences :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 31/48 [02:16<01:37,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 30, num timesteps 63488, FPS 464 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 32/48 [02:19<01:20,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 31, num timesteps 65536, FPS 468 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 33/48 [02:23<01:07,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 32, num timesteps 67584, FPS 471 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 34/48 [02:26<00:58,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 33, num timesteps 69632, FPS 474 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 35/48 [02:29<00:50,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 34, num timesteps 71680, FPS 477 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 36/48 [02:33<00:44,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 35, num timesteps 73728, FPS 480 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 37/48 [02:36<00:39,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 36, num timesteps 75776, FPS 483 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 38/48 [02:40<00:35,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 37, num timesteps 77824, FPS 486 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 39/48 [02:43<00:31,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 38, num timesteps 79872, FPS 488 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n",
      "Updates 39, num timesteps 81920, FPS 491 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n",
      " Evaluation using 10 episodes: mean reward 0.95500 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 40/48 [02:57<00:53,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Evaluation using 10 episodes: mean reward 0.97437 \n",
      "\n",
      "len task_sequences :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 41/48 [03:01<00:40,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 40, num timesteps 83968, FPS 463 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 42/48 [03:04<00:29,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 41, num timesteps 86016, FPS 466 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 43/48 [03:07<00:22,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 42, num timesteps 88064, FPS 469 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 44/48 [03:11<00:16,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 43, num timesteps 90112, FPS 471 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 45/48 [03:14<00:11,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 44, num timesteps 92160, FPS 474 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 46/48 [03:17<00:07,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 45, num timesteps 94208, FPS 476 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 47/48 [03:20<00:03,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 46, num timesteps 96256, FPS 478 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [03:24<00:00,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 47, num timesteps 98304, FPS 480 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tr_reward_arr = []\n",
    "te_reward_arr = {}\n",
    "\n",
    "for _type in (['mean', 'max', 'min']):\n",
    "    te_reward_arr[_type] = {}\n",
    "    for idx in range(len(taskcla)):\n",
    "        te_reward_arr[_type]['task' + str(idx)] = []\n",
    "\n",
    "for task_idx,env_name in task_sequences:\n",
    "    print(env_name)\n",
    "    # renew optimizer\n",
    "    agent.renew_optimizer()\n",
    "\n",
    "    #envs = make_vec_envs(env_name, args.seed, args.num_processes, args.gamma, args.log_dir, device, False)\n",
    "\n",
    "    # FlatObsWrapper for MiniGrid\n",
    "    envs = make_vec_envs(env_name, args.seed, args.num_processes, args.gamma, args.log_dir, device, False, wrapper_class=FlatObsWrapper)\n",
    "    obs = envs.reset()\n",
    "\n",
    "    rollouts = RolloutStorage(args.num_steps, args.num_processes,\n",
    "                                    obs_shape, envs.action_space,\n",
    "                                    actor_critic.recurrent_hidden_state_size)\n",
    "\n",
    "    rollouts.obs[0].copy_(obs)\n",
    "    rollouts.to(device)\n",
    "\n",
    "    episode_rewards = deque(maxlen=10)\n",
    "    num_updates = int(args.num_env_steps) // args.num_steps // args.num_processes\n",
    "\n",
    "    train_ppo(actor_critic, agent, rollouts, task_idx, env_name, task_sequences, envs,  obs_shape, args, episode_rewards, tr_reward_arr, te_reward_arr, num_updates, log_name, device, wrapper_class=FlatObsWrapper)\n",
    "\n",
    "    # post-processing\n",
    "    if args.approach == 'fine-tuning':\n",
    "        if args.single_task == True:\n",
    "            envs.close()\n",
    "            break\n",
    "        else:\n",
    "            envs.close()\n",
    "    elif args.approach == 'ft-fix':\n",
    "        # fix the backbone\n",
    "        for param in actor_critic.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        if args.single_task == True:\n",
    "            envs.close()\n",
    "            break\n",
    "        else:\n",
    "            envs.close()\n",
    "    elif args.approach == 'ewc':\n",
    "        agent.update_fisher(rollouts, task_idx)\n",
    "        envs.close()\n",
    "    elif args.approach == 'blip':\n",
    "        agent.ng_post_processing(rollouts, task_idx)\n",
    "        # save the model here so that bit allocation is saved\n",
    "        save_path = os.path.join(args.save_dir, args.algo)\n",
    "        torch.save(actor_critic.state_dict(),\n",
    "            os.path.join(save_path, log_name + '_task_' + str(task_idx) + \".pth\"))\n",
    "        envs.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check MiniGrid vectorized environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper, RGBImgObsWrapper\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env_name = 'MiniGrid-DoorKey-6x6-v0'\n",
    "\n",
    "# Create vectorized environment with wrapper class\n",
    "vec_env = make_vec_envs(env_name, args.seed, args.num_processes, args.gamma, args.log_dir, device, False, wrapper_class=FlatObsWrapper)\n",
    "\n",
    "# Plot snapshot of vectorized environment and check randomized init\n",
    "vec_env.reset()\n",
    "before_img = vec_env.render('rgb_array')\n",
    "\n",
    "plt.figure(figsize = (6.,6.))\n",
    "plt.imshow(before_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "import numpy as np\n",
    "a = np.arange(20)\n",
    "mdic = {\"a\": a, \"label\": \"experiment\"}\n",
    "print(mdic)\n",
    "savemat(\"./res/matlab_matrix.mat\", mdic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm-experiments",
   "language": "python",
   "name": "tfm-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (default, Nov 28 2021, 11:57:37) \n[Clang 12.0.5 (clang-1205.0.22.11)]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
