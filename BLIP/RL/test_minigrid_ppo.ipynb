{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Jupyter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigo/.local/share/virtualenvs/tfm-experiments-K5nk3NK1/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import date\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from arguments_rl import get_args\n",
    "\n",
    "from collections import deque\n",
    "from rl_module.a2c_ppo_acktr.envs import make_vec_envs\n",
    "from rl_module.a2c_ppo_acktr.storage import RolloutStorage\n",
    "from rl_module.train_ppo import train_ppo\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from torch_ac.utils import DictList\n",
    "\n",
    "# Gym MiniGrid specific\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper, RGBImgObsWrapper\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppo', 'experiment': 'minigrid-adam-doorkey-wallgap-lavagap', 'approach': 'fine-tuning', 'optimizer': 'Adam', 'gail': False, 'gail_experts_dir': './gail_experts', 'gail_batch_size': 128, 'gail_epoch': 5, 'lr': 0.00025, 'eps': 1e-08, 'gamma': 0.99, 'use_gae': True, 'gae_lambda': 0.95, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'seed': 1, 'cuda_deterministic': False, 'num_processes': 16, 'num_steps': 128, 'ppo_epoch': 4, 'num_mini_batch': 256, 'clip_param': 0.2, 'log_interval': 10, 'save_interval': 10, 'eval_interval': 100, 'num_env_steps': 500000.0, 'env_name': 'PongNoFrameskip-v4', 'log_dir': './logs/', 'save_dir': './trained_models/', 'no_cuda': True, 'use_proper_time_limits': False, 'recurrent_policy': False, 'use_linear_lr_decay': False, 'ewc_lambda': 5000.0, 'ewc_online': False, 'ewc_epochs': 100, 'num_ewc_steps': 20, 'save_name': None, 'date': datetime.date(2023, 1, 11), 'task_id': None, 'single_task': False, 'F_prior': 1e-15, 'input_padding': False, 'sample': False, 'samples': 1, 'render_ckpt_path': '', 'render_task_idx': 0, 'num_render_traj': 1000, 'cuda': False}\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "'algo':'ppo',\n",
    "'experiment':'minigrid-adam-doorkey-wallgap-lavagap',\n",
    "'approach':'fine-tuning',#'blip',\n",
    "'optimizer':'Adam',#'RMSProp',#'Adam',\n",
    "'gail':False,\n",
    "'gail_experts_dir':'./gail_experts',\n",
    "'gail_batch_size':128,\n",
    "'gail_epoch':5,\n",
    "'lr':2.5e-4,#7e-4,#1e-4,\n",
    "'eps':1e-8,#1e-5,\n",
    "'gamma':0.99,\n",
    "'use_gae':True,\n",
    "'gae_lambda':0.95,#0.99,\n",
    "'entropy_coef':0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'seed':1,\n",
    "'cuda_deterministic':False,\n",
    "'num_processes':16,\n",
    "'num_steps':128,#5,\n",
    "'ppo_epoch':4,\n",
    "'num_mini_batch':256,#8,#32,\n",
    "'clip_param':0.2,#0.1,\n",
    "'log_interval':10,\n",
    "'save_interval':10,\n",
    "'eval_interval':100,\n",
    "'num_env_steps':5e5,\n",
    "'env_name':'PongNoFrameskip-v4',\n",
    "'log_dir':'./logs/',\n",
    "'save_dir':'./trained_models/',\n",
    "'no_cuda':True,\n",
    "'use_proper_time_limits':False,\n",
    "'recurrent_policy':False,\n",
    "'use_linear_lr_decay':False,\n",
    "'ewc_lambda':5000.0,\n",
    "'ewc_online':False,\n",
    "'ewc_epochs':100,\n",
    "'num_ewc_steps':20,\n",
    "'save_name':None,\n",
    "'date':date.today(),\n",
    "'task_id':None,\n",
    "'single_task':False,\n",
    "'F_prior':1e-15,\n",
    "'input_padding':False,\n",
    "'sample':False,\n",
    "'samples':1,\n",
    "# render arguments\n",
    "'render_ckpt_path':'',\n",
    "'render_task_idx':0,\n",
    "'num_render_traj':1000\n",
    "}\n",
    "\n",
    "args = DictList(args)\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "assert args.algo in ['a2c', 'ppo', 'acktr']\n",
    "if args.recurrent_policy:\n",
    "    assert args.algo in ['a2c', 'ppo'], \\\n",
    "        'Recurrent policy is not implemented for ACKTR'\n",
    "conv_experiment = [\n",
    "    'atari',\n",
    "]\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "\n",
    "if args.approach == 'fine-tuning' or args.approach == 'ft-fix':\n",
    "    log_name = '{}_{}_{}_{}'.format(args.date, args.experiment, args.approach,args.seed)\n",
    "elif args.approach == 'ewc' in args.approach:\n",
    "    log_name = '{}_{}_{}_{}_lamb_{}'.format(args.date, args.experiment, args.approach, args.seed, args.ewc_lambda)\n",
    "elif args.approach == 'blip':\n",
    "    log_name = '{}_{}_{}_{}_F_prior_{}'.format(args.date, args.experiment, args.approach, args.seed, args.F_prior)\n",
    "\n",
    "if args.experiment in conv_experiment:\n",
    "    log_name = log_name + '_conv'\n",
    "\n",
    "# Seed\n",
    "set_random_seed(args.seed)\n",
    "\n",
    "# Inits\n",
    "if args.cuda:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "#taskcla = [(0,14), (1,18), (2,18), (3,18), (4,18), (5,6)]\n",
    "#task_sequences = [(0, 'KungFuMasterNoFrameskip-v4'), (1, 'BoxingNoFrameskip-v4'), (2, 'JamesbondNoFrameskip-v4'), (3, 'KrullNoFrameskip-v4'), (4, 'RiverraidNoFrameskip-v4'), (5, 'SpaceInvadersNoFrameskip-v4')]\n",
    "\n",
    "taskcla = [(0,7), (1,7), (2,7)]\n",
    "task_sequences = [(0, 'MiniGrid-DoorKey-6x6-v0'), (1, 'MiniGrid-WallGapS6-v0'), (2, 'MiniGrid-LavaGapS6-v0')]\n",
    "\n",
    "# hard coded for atari environment\n",
    "#obs_shape = (4,84,84)\n",
    "\n",
    "# for FlatObsWrapper Minigrid environment\n",
    "obs_shape = (2739,)\n",
    "\n",
    "if args.approach == 'blip':\n",
    "    from rl_module.ppo_model import QPolicy\n",
    "    print('using fisher prior of: ', args.F_prior)\n",
    "    actor_critic = QPolicy(obs_shape,\n",
    "        taskcla,\n",
    "        base_kwargs={'F_prior': args.F_prior, 'recurrent': args.recurrent_policy}).to(device)\n",
    "else:\n",
    "    from rl_module.ppo_model import Policy\n",
    "    actor_critic = Policy(obs_shape,\n",
    "        taskcla,\n",
    "        base_kwargs={'recurrent': args.recurrent_policy}).to(device)\n",
    "\n",
    "# Args -- Approach\n",
    "if args.approach == 'fine-tuning' or args.approach == 'ft-fix':\n",
    "    from rl_module.ppo import PPO as approach\n",
    "\n",
    "    agent = approach(actor_critic,\n",
    "            args.clip_param,\n",
    "            args.ppo_epoch,\n",
    "            args.num_mini_batch,\n",
    "            args.value_loss_coef,\n",
    "            args.entropy_coef,\n",
    "            lr=args.lr,\n",
    "            eps=args.eps,\n",
    "            max_grad_norm=args.max_grad_norm,\n",
    "            use_clipped_value_loss=True,\n",
    "            optimizer=args.optimizer)\n",
    "elif args.approach == 'ewc':\n",
    "    from rl_module.ppo_ewc import PPO_EWC as approach\n",
    "\n",
    "    agent = approach(\n",
    "        actor_critic,\n",
    "        args.clip_param,\n",
    "        args.ppo_epoch,\n",
    "        args.num_mini_batch,\n",
    "        args.value_loss_coef,\n",
    "        args.entropy_coef,\n",
    "        lr=args.lr,\n",
    "        eps=args.eps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        use_clipped_value_loss=True,\n",
    "        ewc_lambda= args.ewc_lambda,\n",
    "        online = args.ewc_online)\n",
    "\n",
    "elif args.approach == 'blip':\n",
    "    from rl_module.ppo_blip import PPO_BLIP as approach\n",
    "\n",
    "    agent = approach(\n",
    "        actor_critic,\n",
    "        args.clip_param,\n",
    "        args.ppo_epoch,\n",
    "        args.num_mini_batch,\n",
    "        args.value_loss_coef,\n",
    "        args.entropy_coef,\n",
    "        lr=args.lr,\n",
    "        eps=args.eps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        use_clipped_value_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniGrid-DoorKey-6x6-v0\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 9/244 [00:30<13:22,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 9, num timesteps 20480, FPS 599 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 10/244 [00:40<20:53,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 11/244 [00:47<16:40,  4.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mj/7zvl797s38qgzlm091ctt_gw0000gn/T/ipykernel_74613/865099462.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mnum_updates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_env_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrollouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mobs_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_reward_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_reward_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_updates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFlatObsWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# post-processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/tfm-experiments/BLIP/RL/rl_module/train_ppo.py\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m(actor_critic, agent, rollouts, task_idx, env_name, task_sequences, envs, obs_shape, args, episode_rewards, tr_reward_arr, te_reward_arr, num_updates, log_name, device, wrapper_class)\u001b[0m\n\u001b[1;32m     73\u001b[0m                                  args.gae_lambda, args.use_proper_time_limits)\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mvalue_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mrollouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/tfm-experiments/BLIP/RL/rl_module/ppo.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, rollouts, task_num)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 (value_loss * self.value_loss_coef + action_loss -\n\u001b[0;32m---> 94\u001b[0;31m                  dist_entropy * self.entropy_coef).backward()\n\u001b[0m\u001b[1;32m     95\u001b[0m                 nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n\u001b[1;32m     96\u001b[0m                                          self.max_grad_norm)\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tfm-experiments-K5nk3NK1/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[1;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         )\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/tfm-experiments-K5nk3NK1/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_reward_arr = []\n",
    "te_reward_arr = {}\n",
    "\n",
    "for _type in (['mean', 'max', 'min']):\n",
    "    te_reward_arr[_type] = {}\n",
    "    for idx in range(len(taskcla)):\n",
    "        te_reward_arr[_type]['task' + str(idx)] = []\n",
    "\n",
    "for task_idx,env_name in task_sequences:\n",
    "    print(env_name)\n",
    "    # renew optimizer\n",
    "    agent.renew_optimizer()\n",
    "\n",
    "    #envs = make_vec_envs(env_name, args.seed, args.num_processes, args.gamma, args.log_dir, device, False)\n",
    "\n",
    "    # FlatObsWrapper for MiniGrid\n",
    "    envs = make_vec_envs(env_name, args.seed, args.num_processes, args.gamma, args.log_dir, device, False, wrapper_class=FlatObsWrapper)\n",
    "    obs = envs.reset()\n",
    "\n",
    "    rollouts = RolloutStorage(args.num_steps, args.num_processes,\n",
    "                                    obs_shape, envs.action_space,\n",
    "                                    actor_critic.recurrent_hidden_state_size)\n",
    "\n",
    "    rollouts.obs[0].copy_(obs)\n",
    "    rollouts.to(device)\n",
    "\n",
    "    episode_rewards = deque(maxlen=10)\n",
    "    num_updates = int(args.num_env_steps) // args.num_steps // args.num_processes\n",
    "\n",
    "    train_ppo(actor_critic, agent, rollouts, task_idx, env_name, task_sequences, envs,  obs_shape, args, episode_rewards, tr_reward_arr, te_reward_arr, num_updates, log_name, device, wrapper_class=FlatObsWrapper)\n",
    "\n",
    "    # post-processing\n",
    "    if args.approach == 'fine-tuning':\n",
    "        if args.single_task == True:\n",
    "            envs.close()\n",
    "            break\n",
    "        else:\n",
    "            envs.close()\n",
    "    elif args.approach == 'ft-fix':\n",
    "        # fix the backbone\n",
    "        for param in actor_critic.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        if args.single_task == True:\n",
    "            envs.close()\n",
    "            break\n",
    "        else:\n",
    "            envs.close()\n",
    "    elif args.approach == 'ewc':\n",
    "        agent.update_fisher(rollouts, task_idx)\n",
    "        envs.close()\n",
    "    elif args.approach == 'blip':\n",
    "        agent.ng_post_processing(rollouts, task_idx)\n",
    "        # save the model here so that bit allocation is saved\n",
    "        save_path = os.path.join(args.save_dir, args.algo)\n",
    "        torch.save(actor_critic.state_dict(),\n",
    "            os.path.join(save_path, log_name + '_task_' + str(task_idx) + \".pth\"))\n",
    "        envs.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_module.evaluation import evaluate\n",
    "#args.seed = 3\n",
    "print(task_sequences)\n",
    "task_idx = task_sequences[-1][0]\n",
    "ob_rms = None\n",
    "\n",
    "print('Evaluating tasks:')\n",
    "eval_episode_mean_rewards = evaluate(actor_critic, ob_rms, task_sequences, args.seed,\n",
    "                            args.num_processes, args.log_dir, device, obs_shape, task_idx, args.gamma, wrapper_class=FlatObsWrapper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check MiniGrid vectorized environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper, RGBImgObsWrapper\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env_name = 'MiniGrid-DoorKey-6x6-v0'\n",
    "\n",
    "# Create vectorized environment with wrapper class\n",
    "vec_env = make_vec_envs(env_name, args.seed, args.num_processes, args.gamma, args.log_dir, device, False, wrapper_class=FlatObsWrapper)\n",
    "\n",
    "# Plot snapshot of vectorized environment and check randomized init\n",
    "vec_env.reset()\n",
    "before_img = vec_env.render('rgb_array')\n",
    "\n",
    "plt.figure(figsize = (6.,6.))\n",
    "plt.imshow(before_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat\n",
    "import numpy as np\n",
    "a = np.arange(20)\n",
    "mdic = {\"a\": a, \"label\": \"experiment\"}\n",
    "print(mdic)\n",
    "savemat(\"./res/matlab_matrix.mat\", mdic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm-experiments",
   "language": "python",
   "name": "tfm-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
