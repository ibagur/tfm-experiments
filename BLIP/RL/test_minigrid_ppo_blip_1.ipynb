{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default Jupyter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import date\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from arguments_rl import get_args\n",
    "\n",
    "from collections import deque\n",
    "from rl_module.a2c_ppo_acktr.envs import make_vec_envs\n",
    "from rl_module.a2c_ppo_acktr.storage import RolloutStorage\n",
    "from rl_module.train_ppo import train_ppo\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from torch_ac.utils import DictList\n",
    "import tensorboardX\n",
    "\n",
    "# Gym MiniGrid specific\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper, RGBImgObsWrapper\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define wrapper for CNN Policies\n",
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))\n",
    "\n",
    "def get_tb_dir(result_dir, log_name, stage='train',task_idx=0):\n",
    "    tb_dir = os.path.join(result_dir, log_name, stage, \"task_\"+str(task_idx))\n",
    "    return tb_dir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MiniGrid sequential tasks with PPO + BLIP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppo', 'experiment': 'minigrid-blip-5e5-doorkey-wallgap-lavagap', 'approach': 'blip', 'optimizer': 'Adam', 'gail': False, 'gail_experts_dir': './gail_experts', 'gail_batch_size': 128, 'gail_epoch': 5, 'lr': 0.00025, 'eps': 1e-08, 'gamma': 0.99, 'use_gae': True, 'gae_lambda': 0.95, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'seed': 1, 'cuda_deterministic': False, 'num_processes': 16, 'num_steps': 128, 'ppo_epoch': 4, 'num_mini_batch': 256, 'clip_param': 0.2, 'log_interval': 10, 'save_interval': 10, 'eval_interval': 100, 'num_env_steps': 500000.0, 'env_name': 'PongNoFrameskip-v4', 'log_dir': './logs/', 'save_dir': './trained_models/', 'no_cuda': True, 'use_proper_time_limits': False, 'recurrent_policy': False, 'use_linear_lr_decay': False, 'ewc_lambda': 5000.0, 'ewc_online': False, 'ewc_epochs': 100, 'num_ewc_steps': 20, 'save_name': None, 'date': datetime.date(2023, 1, 14), 'task_id': None, 'single_task': False, 'F_prior': 5e-18, 'input_padding': False, 'sample': False, 'samples': 1, 'render_ckpt_path': '', 'render_task_idx': 0, 'num_render_traj': 1000, 'cuda': False}\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "'algo':'ppo',\n",
    "'experiment':'minigrid-tb-blip-5e5-doorkey-wallgap-lavagap',\n",
    "'approach':'blip', #'fine-tuning','blip','ewc'\n",
    "'wrapper':'flat',#'img'\n",
    "'optimizer':'Adam',#'RMSProp',#'Adam',\n",
    "'gail':False,\n",
    "'gail_experts_dir':'./gail_experts',\n",
    "'gail_batch_size':128,\n",
    "'gail_epoch':5,\n",
    "'lr':2.5e-4,#7e-4,#1e-4,\n",
    "'eps':1e-8,#1e-5,\n",
    "'gamma':0.99,\n",
    "'use_gae':True,\n",
    "'gae_lambda':0.95,#0.99,\n",
    "'entropy_coef':0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'seed':1,\n",
    "'cuda_deterministic':False,\n",
    "'num_processes':16,\n",
    "'num_steps':128,#5,\n",
    "'ppo_epoch':4,\n",
    "'num_mini_batch':256,#8,#32,\n",
    "'clip_param':0.2,#0.1,\n",
    "'log_interval':10,\n",
    "'save_interval':10,\n",
    "'eval_interval':100,\n",
    "'num_env_steps':5e5,\n",
    "'env_name':'PongNoFrameskip-v4',\n",
    "'log_dir':'./logs/',\n",
    "'save_dir':'./trained_models/',\n",
    "'no_cuda':True,\n",
    "'use_proper_time_limits':False,\n",
    "'recurrent_policy':False,\n",
    "'use_linear_lr_decay':False,\n",
    "'ewc_lambda':5000.0,\n",
    "'ewc_online':False,\n",
    "'ewc_epochs':100,\n",
    "'num_ewc_steps':20,\n",
    "'save_name':None,\n",
    "'date':date.today(),\n",
    "'task_id':None,\n",
    "'single_task':False,\n",
    "'F_prior':5e-18,#1e-15,\n",
    "'input_padding':False,\n",
    "'sample':False,\n",
    "'samples':1,\n",
    "# render arguments\n",
    "'render_ckpt_path':'',\n",
    "'render_task_idx':0,\n",
    "'num_render_traj':1000\n",
    "}\n",
    "\n",
    "args = DictList(args)\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "assert args.algo in ['a2c', 'ppo', 'acktr']\n",
    "if args.recurrent_policy:\n",
    "    assert args.algo in ['a2c', 'ppo'], \\\n",
    "        'Recurrent policy is not implemented for ACKTR'\n",
    "conv_experiment = [\n",
    "    'atari',\n",
    "]\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tasks and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fisher prior of:  5e-18\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "\n",
    "if args.approach == 'fine-tuning' or args.approach == 'ft-fix':\n",
    "    log_name = '{}_{}_{}_{}'.format(args.date, args.experiment, args.approach,args.seed)\n",
    "elif args.approach == 'ewc' in args.approach:\n",
    "    log_name = '{}_{}_{}_{}_lamb_{}'.format(args.date, args.experiment, args.approach, args.seed, args.ewc_lambda)\n",
    "elif args.approach == 'blip':\n",
    "    log_name = '{}_{}_{}_{}_F_prior_{}'.format(args.date, args.experiment, args.approach, args.seed, args.F_prior)\n",
    "\n",
    "if args.experiment in conv_experiment:\n",
    "    log_name = log_name + '_conv'\n",
    "\n",
    "# Seed\n",
    "set_random_seed(args.seed)\n",
    "\n",
    "# Inits\n",
    "if args.cuda:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "#taskcla = [(0,14), (1,18), (2,18), (3,18), (4,18), (5,6)]\n",
    "#task_sequences = [(0, 'KungFuMasterNoFrameskip-v4'), (1, 'BoxingNoFrameskip-v4'), (2, 'JamesbondNoFrameskip-v4'), (3, 'KrullNoFrameskip-v4'), (4, 'RiverraidNoFrameskip-v4'), (5, 'SpaceInvadersNoFrameskip-v4')]\n",
    "\n",
    "taskcla = [(0,7), (1,7), (2,7)]\n",
    "task_sequences = [\n",
    "    (0, 'MiniGrid-DoorKey-6x6-v0'), \n",
    "    (1, 'MiniGrid-WallGapS6-v0'), \n",
    "    (2, 'MiniGrid-LavaGapS6-v0')\n",
    "    ]\n",
    "\n",
    "# hard coded for atari environment\n",
    "#obs_shape = (4,84,84)\n",
    "\n",
    "# for FlatObsWrapper Minigrid environment\n",
    "if args.wrapper == 'flat':\n",
    "    wrapper_class = FlatObsWrapper\n",
    "    obs_shape = (2739,)\n",
    "# for ImgRGBImgPartialObsWrapper Minigrid environment\n",
    "elif args.wrapper == 'img':\n",
    "    wrapper_class = ImgRGBImgPartialObsWrapper\n",
    "    obs_shape = (12, 56, 56)\n",
    "\n",
    "if args.approach == 'blip':\n",
    "    from rl_module.ppo_model import QPolicy\n",
    "    print('using fisher prior of: ', args.F_prior)\n",
    "    actor_critic = QPolicy(obs_shape,\n",
    "        taskcla,\n",
    "        base_kwargs={'F_prior': args.F_prior, 'recurrent': args.recurrent_policy}).to(device)\n",
    "else:\n",
    "    from rl_module.ppo_model import Policy\n",
    "    actor_critic = Policy(obs_shape,\n",
    "        taskcla,\n",
    "        base_kwargs={'recurrent': args.recurrent_policy}).to(device)\n",
    "\n",
    "# Args -- Approach\n",
    "if args.approach == 'fine-tuning' or args.approach == 'ft-fix':\n",
    "    from rl_module.ppo import PPO as approach\n",
    "\n",
    "    agent = approach(actor_critic,\n",
    "            args.clip_param,\n",
    "            args.ppo_epoch,\n",
    "            args.num_mini_batch,\n",
    "            args.value_loss_coef,\n",
    "            args.entropy_coef,\n",
    "            lr=args.lr,\n",
    "            eps=args.eps,\n",
    "            max_grad_norm=args.max_grad_norm,\n",
    "            use_clipped_value_loss=True,\n",
    "            optimizer=args.optimizer)\n",
    "elif args.approach == 'ewc':\n",
    "    from rl_module.ppo_ewc import PPO_EWC as approach\n",
    "\n",
    "    agent = approach(\n",
    "        actor_critic,\n",
    "        args.clip_param,\n",
    "        args.ppo_epoch,\n",
    "        args.num_mini_batch,\n",
    "        args.value_loss_coef,\n",
    "        args.entropy_coef,\n",
    "        lr=args.lr,\n",
    "        eps=args.eps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        use_clipped_value_loss=True,\n",
    "        ewc_lambda= args.ewc_lambda,\n",
    "        online = args.ewc_online,\n",
    "        optimizer=args.optimizer)\n",
    "\n",
    "elif args.approach == 'blip':\n",
    "    from rl_module.ppo_blip import PPO_BLIP as approach\n",
    "\n",
    "    agent = approach(\n",
    "        actor_critic,\n",
    "        args.clip_param,\n",
    "        args.ppo_epoch,\n",
    "        args.num_mini_batch,\n",
    "        args.value_loss_coef,\n",
    "        args.entropy_coef,\n",
    "        lr=args.lr,\n",
    "        eps=args.eps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        use_clipped_value_loss=True,\n",
    "        optimizer=args.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_reward_arr = []\n",
    "te_reward_arr = {}\n",
    "\n",
    "for _type in (['mean', 'max', 'min']):\n",
    "    te_reward_arr[_type] = {}\n",
    "    for idx in range(len(taskcla)):\n",
    "        te_reward_arr[_type]['task' + str(idx)] = []\n",
    "\n",
    "tb_writer_train = []\n",
    "tb_writer_eval = []\n",
    "tb_header_train = [\"update\", \"num_timesteps\", \"FPS\", \"mean_reward\", \"median_reward\", \"min_reward\", \"max_reward\", \"entropy\", \"value_loss\", \"action_loss\"]\n",
    "tb_header_eval = [\"update\", \"num_timesteps\", \"mean_reward\"]\n",
    "\n",
    "for idx in range(len(taskcla)):\n",
    "    tb_writer_train.append(tensorboardX.SummaryWriter(get_tb_dir(\"./result_data/\", log_name, 'train', idx)))\n",
    "    tb_writer_eval.append(tensorboardX.SummaryWriter(get_tb_dir(\"./result_data/\", log_name, 'eval', idx)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach:  blip\n",
      "MiniGrid-DoorKey-6x6-v0\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 10/244 [00:42<16:34,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 9, num timesteps 20480, FPS 482 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/244 [01:25<15:51,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 19, num timesteps 40960, FPS 480 \n",
      " Last 10 training episodes: mean/median reward 0.4/0.5, min/max reward 0.0/0.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 30/244 [02:08<15:40,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 29, num timesteps 61440, FPS 479 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 40/244 [02:52<15:01,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 39, num timesteps 81920, FPS 476 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 50/244 [03:36<14:22,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 49, num timesteps 102400, FPS 472 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 60/244 [04:25<15:22,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 59, num timesteps 122880, FPS 462 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 70/244 [05:13<12:44,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 69, num timesteps 143360, FPS 457 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 80/244 [05:56<11:41,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 79, num timesteps 163840, FPS 459 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 90/244 [06:39<11:01,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 89, num timesteps 184320, FPS 461 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 99/244 [07:17<10:14,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 99, num timesteps 204800, FPS 463 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 100/244 [07:27<14:12,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Evaluation using 10 episodes: mean reward 0.96300 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 110/244 [08:11<09:44,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 109, num timesteps 225280, FPS 458 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 120/244 [08:54<08:57,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 119, num timesteps 245760, FPS 459 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 130/244 [09:37<08:02,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 129, num timesteps 266240, FPS 461 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 140/244 [10:19<07:19,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 139, num timesteps 286720, FPS 462 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 150/244 [11:01<06:38,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 149, num timesteps 307200, FPS 464 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 160/244 [11:44<05:56,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 159, num timesteps 327680, FPS 465 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 170/244 [12:26<05:12,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 169, num timesteps 348160, FPS 466 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 180/244 [13:08<04:29,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 179, num timesteps 368640, FPS 467 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 190/244 [13:51<03:47,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 189, num timesteps 389120, FPS 468 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 199/244 [14:29<03:10,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 199, num timesteps 409600, FPS 468 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 200/244 [14:38<04:12,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Evaluation using 10 episodes: mean reward 0.96375 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 210/244 [15:21<02:26,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 209, num timesteps 430080, FPS 466 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 220/244 [16:03<01:41,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 219, num timesteps 450560, FPS 467 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 230/244 [16:46<00:59,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 229, num timesteps 471040, FPS 468 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 240/244 [17:28<00:16,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 239, num timesteps 491520, FPS 468 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [17:45<00:00,  4.37s/it]\n",
      "/Users/inigo/.local/share/virtualenvs/tfm-experiments-K5nk3NK1/lib/python3.7/site-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniGrid-WallGapS6-v0\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "Task 1: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 10/244 [00:42<16:34,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 9, num timesteps 20480, FPS 477 \n",
      " Last 10 training episodes: mean/median reward 0.4/0.3, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/244 [01:25<15:51,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 19, num timesteps 40960, FPS 479 \n",
      " Last 10 training episodes: mean/median reward 0.8/0.9, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 30/244 [02:07<15:07,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 29, num timesteps 61440, FPS 481 \n",
      " Last 10 training episodes: mean/median reward 0.6/0.8, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 40/244 [02:53<15:31,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 39, num timesteps 81920, FPS 473 \n",
      " Last 10 training episodes: mean/median reward 0.7/0.8, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 50/244 [03:38<14:28,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 49, num timesteps 102400, FPS 469 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.7/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 60/244 [04:22<13:38,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 59, num timesteps 122880, FPS 467 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.6/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 70/244 [05:07<12:59,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 69, num timesteps 143360, FPS 465 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 80/244 [05:52<12:13,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 79, num timesteps 163840, FPS 464 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.7/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 90/244 [06:37<11:28,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 89, num timesteps 184320, FPS 463 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 99/244 [07:17<10:49,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 99, num timesteps 204800, FPS 463 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 100/244 [07:33<19:05,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: Evaluation using 10 episodes: mean reward 0.94000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 110/244 [08:18<10:09,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 109, num timesteps 225280, FPS 451 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 120/244 [09:03<09:13,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 119, num timesteps 245760, FPS 452 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 130/244 [09:48<08:36,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 129, num timesteps 266240, FPS 452 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 140/244 [10:33<07:46,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 139, num timesteps 286720, FPS 452 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 150/244 [11:18<07:01,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 149, num timesteps 307200, FPS 452 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 160/244 [12:03<06:16,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 159, num timesteps 327680, FPS 453 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 170/244 [12:48<05:34,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 169, num timesteps 348160, FPS 453 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 180/244 [13:32<04:45,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 179, num timesteps 368640, FPS 453 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 190/244 [14:16<03:57,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 189, num timesteps 389120, FPS 454 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 199/244 [14:55<03:13,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 199, num timesteps 409600, FPS 455 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 200/244 [15:10<05:29,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: Evaluation using 10 episodes: mean reward 0.94688 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 210/244 [15:53<02:29,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 209, num timesteps 430080, FPS 451 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 220/244 [16:36<01:42,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 219, num timesteps 450560, FPS 452 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 230/244 [17:19<00:59,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 229, num timesteps 471040, FPS 453 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 240/244 [18:01<00:17,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 239, num timesteps 491520, FPS 454 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [18:19<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniGrid-LavaGapS6-v0\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "Task 1: Evaluation using 10 episodes: mean reward 0.92812 \n",
      "\n",
      "Task 2: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 10/244 [00:44<17:22,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 9, num timesteps 20480, FPS 460 \n",
      " Last 10 training episodes: mean/median reward 0.0/0.0, min/max reward 0.0/0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/244 [01:29<16:40,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 19, num timesteps 40960, FPS 459 \n",
      " Last 10 training episodes: mean/median reward 0.0/0.0, min/max reward 0.0/0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 30/244 [02:13<15:59,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 29, num timesteps 61440, FPS 459 \n",
      " Last 10 training episodes: mean/median reward 0.0/0.0, min/max reward 0.0/0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 40/244 [02:58<15:15,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 39, num timesteps 81920, FPS 459 \n",
      " Last 10 training episodes: mean/median reward 0.0/0.0, min/max reward 0.0/0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 50/244 [03:44<14:50,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 49, num timesteps 102400, FPS 456 \n",
      " Last 10 training episodes: mean/median reward 0.0/0.0, min/max reward 0.0/0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 60/244 [04:29<13:59,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 59, num timesteps 122880, FPS 455 \n",
      " Last 10 training episodes: mean/median reward 0.0/0.0, min/max reward 0.0/0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 70/244 [05:15<13:18,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 69, num timesteps 143360, FPS 454 \n",
      " Last 10 training episodes: mean/median reward 0.0/0.0, min/max reward 0.0/0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 80/244 [06:00<12:26,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 79, num timesteps 163840, FPS 454 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 90/244 [06:46<11:43,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 89, num timesteps 184320, FPS 453 \n",
      " Last 10 training episodes: mean/median reward 0.2/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 99/244 [07:27<10:55,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 99, num timesteps 204800, FPS 453 \n",
      " Last 10 training episodes: mean/median reward 0.3/0.0, min/max reward 0.0/1.0\n",
      "\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "Task 1: Evaluation using 10 episodes: mean reward 0.92812 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 100/244 [07:47<21:56,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 110/244 [08:32<10:19,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 109, num timesteps 225280, FPS 439 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 120/244 [09:17<09:22,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 119, num timesteps 245760, FPS 441 \n",
      " Last 10 training episodes: mean/median reward 0.3/0.0, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 130/244 [10:01<08:32,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 129, num timesteps 266240, FPS 442 \n",
      " Last 10 training episodes: mean/median reward 0.4/0.3, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 140/244 [10:46<07:41,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 139, num timesteps 286720, FPS 443 \n",
      " Last 10 training episodes: mean/median reward 0.4/0.2, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 150/244 [11:31<07:00,  4.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 149, num timesteps 307200, FPS 444 \n",
      " Last 10 training episodes: mean/median reward 0.5/0.4, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 160/244 [12:16<06:21,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 159, num timesteps 327680, FPS 445 \n",
      " Last 10 training episodes: mean/median reward 0.8/0.9, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 170/244 [13:01<05:34,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 169, num timesteps 348160, FPS 445 \n",
      " Last 10 training episodes: mean/median reward 0.6/0.9, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 180/244 [13:46<04:45,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 179, num timesteps 368640, FPS 446 \n",
      " Last 10 training episodes: mean/median reward 0.3/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 190/244 [14:30<04:01,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 189, num timesteps 389120, FPS 446 \n",
      " Last 10 training episodes: mean/median reward 0.5/0.5, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 199/244 [15:11<03:21,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 199, num timesteps 409600, FPS 447 \n",
      " Last 10 training episodes: mean/median reward 0.7/0.9, min/max reward 0.0/1.0\n",
      "\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "Task 1: Evaluation using 10 episodes: mean reward 0.92812 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 200/244 [15:31<06:46,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Evaluation using 10 episodes: mean reward 0.46688 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 210/244 [16:16<02:38,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 209, num timesteps 430080, FPS 440 \n",
      " Last 10 training episodes: mean/median reward 0.5/0.9, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 220/244 [17:03<01:55,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 219, num timesteps 450560, FPS 440 \n",
      " Last 10 training episodes: mean/median reward 0.6/0.9, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 230/244 [17:50<01:05,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 229, num timesteps 471040, FPS 439 \n",
      " Last 10 training episodes: mean/median reward 0.8/0.9, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 240/244 [18:38<00:18,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 239, num timesteps 491520, FPS 439 \n",
      " Last 10 training episodes: mean/median reward 0.6/0.9, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [18:56<00:00,  4.66s/it]\n"
     ]
    }
   ],
   "source": [
    "print('Approach: ', args.approach)\n",
    "\n",
    "tr_reward_arr = []\n",
    "te_reward_arr = {}\n",
    "\n",
    "for _type in (['mean', 'max', 'min']):\n",
    "    te_reward_arr[_type] = {}\n",
    "    for idx in range(len(taskcla)):\n",
    "        te_reward_arr[_type]['task' + str(idx)] = []\n",
    "        tb_writer_train.append(tensorboardX.SummaryWriter(get_tb_dir(\"./result_data/\", log_name, 'train', idx)))\n",
    "        tb_writer_eval.append(tensorboardX.SummaryWriter(get_tb_dir(\"./result_data/\", log_name, 'eval', idx)))\n",
    "\n",
    "# Define Tensorboard loggers\n",
    "\n",
    "tb_logger_train = {\n",
    "    'header':[\"update\", \"num_timesteps\", \"FPS\", \"mean_reward\", \"median_reward\", \"min_reward\", \"max_reward\", \"entropy\", \"value_loss\", \"action_loss\"],\n",
    "    'writer':[]\n",
    "}\n",
    "tb_logger_eval = {\n",
    "    'header':[\"update\", \"num_timesteps\", \"mean_reward\"],\n",
    "    'writer':[]\n",
    "}\n",
    "\n",
    "for idx in range(len(taskcla)):\n",
    "    tb_logger_train['writer'].append(tensorboardX.SummaryWriter(get_tb_dir(\"./result_data/\", log_name, 'train', idx)))\n",
    "    tb_logger_eval['writer'].append(tensorboardX.SummaryWriter(get_tb_dir(\"./result_data/\", log_name, 'eval', idx)))\n",
    "\n",
    "# Start training \n",
    "\n",
    "for task_idx,env_name in task_sequences:\n",
    "    print(env_name)\n",
    "    # renew optimizer\n",
    "    agent.renew_optimizer()\n",
    "\n",
    "    # FlatObsWrapper for MiniGrid\n",
    "    envs = make_vec_envs(env_name, args.seed, args.num_processes, args.gamma, args.log_dir, device, False, wrapper_class=wrapper_class)\n",
    "    obs = envs.reset()\n",
    "\n",
    "    rollouts = RolloutStorage(args.num_steps, args.num_processes,\n",
    "                                    obs_shape, envs.action_space,\n",
    "                                    actor_critic.recurrent_hidden_state_size)\n",
    "\n",
    "    rollouts.obs[0].copy_(obs)\n",
    "    rollouts.to(device)\n",
    "\n",
    "    episode_rewards = deque(maxlen=10)\n",
    "    num_updates = int(args.num_env_steps) // args.num_steps // args.num_processes\n",
    "\n",
    "    train_ppo(actor_critic, agent, rollouts, task_idx, env_name, task_sequences, envs,  obs_shape, args, episode_rewards, tr_reward_arr, te_reward_arr, tb_logger_train, tb_logger_eval, num_updates, log_name, device, wrapper_class=wrapper_class)\n",
    "\n",
    "    # post-processing\n",
    "    if args.approach == 'fine-tuning':\n",
    "        if args.single_task == True:\n",
    "            envs.close()\n",
    "            break\n",
    "        else:\n",
    "            envs.close()\n",
    "    elif args.approach == 'ft-fix':\n",
    "        # fix the backbone\n",
    "        for param in actor_critic.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        if args.single_task == True:\n",
    "            envs.close()\n",
    "            break\n",
    "        else:\n",
    "            envs.close()\n",
    "    elif args.approach == 'ewc':\n",
    "        agent.update_fisher(rollouts, task_idx)\n",
    "        envs.close()\n",
    "    elif args.approach == 'blip':\n",
    "        agent.ng_post_processing(rollouts, task_idx)\n",
    "        # save the model here so that bit allocation is saved\n",
    "        save_path = os.path.join(args.save_dir, args.algo)\n",
    "        torch.save(actor_critic.state_dict(),\n",
    "            os.path.join(save_path, log_name + '_task_' + str(task_idx) + \".pth\"))\n",
    "        envs.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'MiniGrid-DoorKey-6x6-v0'), (1, 'MiniGrid-WallGapS6-v0'), (2, 'MiniGrid-LavaGapS6-v0')]\n",
      "Evaluating tasks:\n",
      "Task 0: Evaluation using 100 episodes: mean reward 0.54955 \n",
      "\n",
      "Task 1: Evaluation using 100 episodes: mean reward 0.92456 \n",
      "\n",
      "Task 2: Evaluation using 100 episodes: mean reward 0.54506 \n",
      "\n",
      "Task 0: Evaluation using 100 episodes: mean reward 0.39190 \n",
      "\n",
      "Task 1: Evaluation using 100 episodes: mean reward 0.94006 \n",
      "\n",
      "Task 2: Evaluation using 100 episodes: mean reward 0.64850 \n",
      "\n",
      "Task 0: Evaluation using 100 episodes: mean reward 0.96495 \n",
      "\n",
      "Task 1: Evaluation using 100 episodes: mean reward 0.92906 \n",
      "\n",
      "Task 2: Evaluation using 100 episodes: mean reward 0.61312 \n",
      "\n",
      "Final evaluation\n",
      "Task  0 : median reward (3 eval)  0.54955\n",
      "Task  1 : median reward (3 eval)  0.9290624999999999\n",
      "Task  2 : median reward (3 eval)  0.6131249999999999\n"
     ]
    }
   ],
   "source": [
    "from rl_module.evaluation import evaluate\n",
    "args.seed = 2\n",
    "print(task_sequences)\n",
    "task_idx = task_sequences[-1][0]\n",
    "ob_rms = None\n",
    "seed_list = [100,200,300]\n",
    "\n",
    "print('Evaluating tasks:')\n",
    "tot_eval_episode_mean_rewards = []\n",
    "for i in range(3):\n",
    "    eval_episode_mean_rewards = evaluate(actor_critic, ob_rms, task_sequences, seed_list[i],\n",
    "                                args.num_processes, args.log_dir, device, obs_shape, task_idx, args.gamma, wrapper_class=wrapper_class, episodes=100)\n",
    "    tot_eval_episode_mean_rewards.append(eval_episode_mean_rewards)\n",
    "\n",
    "tot_eval_episode_mean_rewards_median = np.median(np.array(tot_eval_episode_mean_rewards, ndmin=2), axis=0)\n",
    "\n",
    "print('Final evaluation')\n",
    "for i, value in enumerate(tot_eval_episode_mean_rewards_median):\n",
    "    print('Task ',i,': median reward (3 eval) ', value)                               "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MiniGrid PPO + BLIP (diff. F-prior)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppo', 'experiment': 'minigrid-blip-5e5-doorkey-wallgap-lavagap', 'approach': 'blip', 'optimizer': 'Adam', 'gail': False, 'gail_experts_dir': './gail_experts', 'gail_batch_size': 128, 'gail_epoch': 5, 'lr': 0.00025, 'eps': 1e-08, 'gamma': 0.99, 'use_gae': True, 'gae_lambda': 0.95, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'seed': 1, 'cuda_deterministic': False, 'num_processes': 16, 'num_steps': 128, 'ppo_epoch': 4, 'num_mini_batch': 256, 'clip_param': 0.2, 'log_interval': 10, 'save_interval': 10, 'eval_interval': 100, 'num_env_steps': 500000.0, 'env_name': 'PongNoFrameskip-v4', 'log_dir': './logs/', 'save_dir': './trained_models/', 'no_cuda': True, 'use_proper_time_limits': False, 'recurrent_policy': False, 'use_linear_lr_decay': False, 'ewc_lambda': 5000.0, 'ewc_online': False, 'ewc_epochs': 100, 'num_ewc_steps': 20, 'save_name': None, 'date': datetime.date(2023, 1, 14), 'task_id': None, 'single_task': False, 'F_prior': 1e-15, 'input_padding': False, 'sample': False, 'samples': 1, 'render_ckpt_path': '', 'render_task_idx': 0, 'num_render_traj': 1000, 'cuda': False}\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "'algo':'ppo',\n",
    "'experiment':'minigrid-blip-5e5-doorkey-wallgap-lavagap',\n",
    "'approach':'blip', #'fine-tuning','blip','ewc'\n",
    "'optimizer':'Adam',#'RMSProp',#'Adam',\n",
    "'gail':False,\n",
    "'gail_experts_dir':'./gail_experts',\n",
    "'gail_batch_size':128,\n",
    "'gail_epoch':5,\n",
    "'lr':2.5e-4,#7e-4,#1e-4,\n",
    "'eps':1e-8,#1e-5,\n",
    "'gamma':0.99,\n",
    "'use_gae':True,\n",
    "'gae_lambda':0.95,#0.99,\n",
    "'entropy_coef':0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'seed':1,\n",
    "'cuda_deterministic':False,\n",
    "'num_processes':16,\n",
    "'num_steps':128,#5,\n",
    "'ppo_epoch':4,\n",
    "'num_mini_batch':256,#8,#32,\n",
    "'clip_param':0.2,#0.1,\n",
    "'log_interval':10,\n",
    "'save_interval':10,\n",
    "'eval_interval':100,\n",
    "'num_env_steps':5e5,\n",
    "'env_name':'PongNoFrameskip-v4',\n",
    "'log_dir':'./logs/',\n",
    "'save_dir':'./trained_models/',\n",
    "'no_cuda':True,\n",
    "'use_proper_time_limits':False,\n",
    "'recurrent_policy':False,\n",
    "'use_linear_lr_decay':False,\n",
    "'ewc_lambda':5000.0,\n",
    "'ewc_online':False,\n",
    "'ewc_epochs':100,\n",
    "'num_ewc_steps':20,\n",
    "'save_name':None,\n",
    "'date':date.today(),\n",
    "'task_id':None,\n",
    "'single_task':False,\n",
    "'F_prior':1e-15,#5e-18,#1e-15,\n",
    "'input_padding':False,\n",
    "'sample':False,\n",
    "'samples':1,\n",
    "# render arguments\n",
    "'render_ckpt_path':'',\n",
    "'render_task_idx':0,\n",
    "'num_render_traj':1000\n",
    "}\n",
    "\n",
    "args = DictList(args)\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "assert args.algo in ['a2c', 'ppo', 'acktr']\n",
    "if args.recurrent_policy:\n",
    "    assert args.algo in ['a2c', 'ppo'], \\\n",
    "        'Recurrent policy is not implemented for ACKTR'\n",
    "conv_experiment = [\n",
    "    'atari',\n",
    "]\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tasks and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fisher prior of:  1e-15\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "\n",
    "if args.approach == 'fine-tuning' or args.approach == 'ft-fix':\n",
    "    log_name = '{}_{}_{}_{}'.format(args.date, args.experiment, args.approach,args.seed)\n",
    "elif args.approach == 'ewc' in args.approach:\n",
    "    log_name = '{}_{}_{}_{}_lamb_{}'.format(args.date, args.experiment, args.approach, args.seed, args.ewc_lambda)\n",
    "elif args.approach == 'blip':\n",
    "    log_name = '{}_{}_{}_{}_F_prior_{}'.format(args.date, args.experiment, args.approach, args.seed, args.F_prior)\n",
    "\n",
    "if args.experiment in conv_experiment:\n",
    "    log_name = log_name + '_conv'\n",
    "\n",
    "# Seed\n",
    "set_random_seed(args.seed)\n",
    "\n",
    "# Inits\n",
    "if args.cuda:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "#taskcla = [(0,14), (1,18), (2,18), (3,18), (4,18), (5,6)]\n",
    "#task_sequences = [(0, 'KungFuMasterNoFrameskip-v4'), (1, 'BoxingNoFrameskip-v4'), (2, 'JamesbondNoFrameskip-v4'), (3, 'KrullNoFrameskip-v4'), (4, 'RiverraidNoFrameskip-v4'), (5, 'SpaceInvadersNoFrameskip-v4')]\n",
    "\n",
    "taskcla = [(0,7), (1,7), (2,7)]\n",
    "task_sequences = [\n",
    "    (0, 'MiniGrid-DoorKey-6x6-v0'), \n",
    "    (1, 'MiniGrid-WallGapS6-v0'), \n",
    "    (2, 'MiniGrid-LavaGapS6-v0')\n",
    "    ]\n",
    "\n",
    "# hard coded for atari environment\n",
    "#obs_shape = (4,84,84)\n",
    "\n",
    "# for FlatObsWrapper Minigrid environment\n",
    "obs_shape = (2739,)\n",
    "\n",
    "if args.approach == 'blip':\n",
    "    from rl_module.ppo_model import QPolicy\n",
    "    print('using fisher prior of: ', args.F_prior)\n",
    "    actor_critic = QPolicy(obs_shape,\n",
    "        taskcla,\n",
    "        base_kwargs={'F_prior': args.F_prior, 'recurrent': args.recurrent_policy}).to(device)\n",
    "else:\n",
    "    from rl_module.ppo_model import Policy\n",
    "    actor_critic = Policy(obs_shape,\n",
    "        taskcla,\n",
    "        base_kwargs={'recurrent': args.recurrent_policy}).to(device)\n",
    "\n",
    "# Args -- Approach\n",
    "if args.approach == 'fine-tuning' or args.approach == 'ft-fix':\n",
    "    from rl_module.ppo import PPO as approach\n",
    "\n",
    "    agent = approach(actor_critic,\n",
    "            args.clip_param,\n",
    "            args.ppo_epoch,\n",
    "            args.num_mini_batch,\n",
    "            args.value_loss_coef,\n",
    "            args.entropy_coef,\n",
    "            lr=args.lr,\n",
    "            eps=args.eps,\n",
    "            max_grad_norm=args.max_grad_norm,\n",
    "            use_clipped_value_loss=True,\n",
    "            optimizer=args.optimizer)\n",
    "elif args.approach == 'ewc':\n",
    "    from rl_module.ppo_ewc import PPO_EWC as approach\n",
    "\n",
    "    agent = approach(\n",
    "        actor_critic,\n",
    "        args.clip_param,\n",
    "        args.ppo_epoch,\n",
    "        args.num_mini_batch,\n",
    "        args.value_loss_coef,\n",
    "        args.entropy_coef,\n",
    "        lr=args.lr,\n",
    "        eps=args.eps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        use_clipped_value_loss=True,\n",
    "        ewc_lambda= args.ewc_lambda,\n",
    "        online = args.ewc_online,\n",
    "        optimizer=args.optimizer)\n",
    "\n",
    "elif args.approach == 'blip':\n",
    "    from rl_module.ppo_blip import PPO_BLIP as approach\n",
    "\n",
    "    agent = approach(\n",
    "        actor_critic,\n",
    "        args.clip_param,\n",
    "        args.ppo_epoch,\n",
    "        args.num_mini_batch,\n",
    "        args.value_loss_coef,\n",
    "        args.entropy_coef,\n",
    "        lr=args.lr,\n",
    "        eps=args.eps,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        use_clipped_value_loss=True,\n",
    "        optimizer=args.optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach:  blip\n",
      "MiniGrid-DoorKey-6x6-v0\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 10/244 [00:41<16:10,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 9, num timesteps 20480, FPS 491 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/244 [01:23<15:31,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 19, num timesteps 40960, FPS 491 \n",
      " Last 10 training episodes: mean/median reward 0.4/0.5, min/max reward 0.0/0.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 30/244 [02:04<14:50,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 29, num timesteps 61440, FPS 492 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 40/244 [02:46<14:11,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 39, num timesteps 81920, FPS 491 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 50/244 [03:28<13:38,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 49, num timesteps 102400, FPS 490 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 60/244 [04:10<12:53,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 59, num timesteps 122880, FPS 489 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 70/244 [04:52<12:10,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 69, num timesteps 143360, FPS 489 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 80/244 [05:34<11:27,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 79, num timesteps 163840, FPS 489 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 90/244 [06:16<10:51,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 89, num timesteps 184320, FPS 488 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 99/244 [06:54<10:09,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 99, num timesteps 204800, FPS 488 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 100/244 [07:04<13:52,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Evaluation using 10 episodes: mean reward 0.96300 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 110/244 [07:46<09:28,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 109, num timesteps 225280, FPS 483 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 120/244 [08:28<08:40,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 119, num timesteps 245760, FPS 483 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 130/244 [09:10<07:58,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 129, num timesteps 266240, FPS 483 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 140/244 [09:52<07:16,  4.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 139, num timesteps 286720, FPS 484 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 150/244 [10:34<06:34,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 149, num timesteps 307200, FPS 484 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 160/244 [11:14<05:44,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 159, num timesteps 327680, FPS 485 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 170/244 [11:55<04:59,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 169, num timesteps 348160, FPS 486 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 180/244 [12:35<04:17,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 179, num timesteps 368640, FPS 487 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 190/244 [13:15<03:38,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 189, num timesteps 389120, FPS 488 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 199/244 [13:52<03:01,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 199, num timesteps 409600, FPS 489 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 200/244 [14:00<03:57,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Evaluation using 10 episodes: mean reward 0.96375 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 210/244 [14:41<02:18,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 209, num timesteps 430080, FPS 488 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 220/244 [15:21<01:36,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 219, num timesteps 450560, FPS 489 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 230/244 [16:01<00:56,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 229, num timesteps 471040, FPS 489 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 240/244 [16:42<00:16,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 239, num timesteps 491520, FPS 490 \n",
      " Last 10 training episodes: mean/median reward 1.0/1.0, min/max reward 1.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [16:58<00:00,  4.17s/it]\n",
      "/Users/inigo/.local/share/virtualenvs/tfm-experiments-K5nk3NK1/lib/python3.7/site-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniGrid-WallGapS6-v0\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "Task 1: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 10/244 [00:42<16:29,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 9, num timesteps 20480, FPS 484 \n",
      " Last 10 training episodes: mean/median reward 0.3/0.1, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/244 [01:24<15:45,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 19, num timesteps 40960, FPS 483 \n",
      " Last 10 training episodes: mean/median reward 0.3/0.1, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 30/244 [02:07<15:11,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 29, num timesteps 61440, FPS 482 \n",
      " Last 10 training episodes: mean/median reward 0.4/0.4, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 40/244 [02:49<14:30,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 39, num timesteps 81920, FPS 482 \n",
      " Last 10 training episodes: mean/median reward 0.5/0.6, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 50/244 [03:32<13:40,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 49, num timesteps 102400, FPS 481 \n",
      " Last 10 training episodes: mean/median reward 0.8/0.8, min/max reward 0.7/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 60/244 [04:15<13:04,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 59, num timesteps 122880, FPS 481 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 70/244 [04:58<12:35,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 69, num timesteps 143360, FPS 480 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 80/244 [05:40<11:37,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 79, num timesteps 163840, FPS 481 \n",
      " Last 10 training episodes: mean/median reward 0.8/0.9, min/max reward 0.7/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 90/244 [06:24<11:10,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 89, num timesteps 184320, FPS 479 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.7/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 99/244 [07:03<10:30,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 99, num timesteps 204800, FPS 479 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 100/244 [07:17<17:55,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: Evaluation using 10 episodes: mean reward 0.94063 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 110/244 [08:00<09:49,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 109, num timesteps 225280, FPS 468 \n",
      " Last 10 training episodes: mean/median reward 0.9/1.0, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 120/244 [08:43<08:48,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 119, num timesteps 245760, FPS 469 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 130/244 [09:26<08:06,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 129, num timesteps 266240, FPS 470 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 140/244 [10:09<07:24,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 139, num timesteps 286720, FPS 470 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 150/244 [10:52<06:45,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 149, num timesteps 307200, FPS 471 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 160/244 [11:34<05:56,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 159, num timesteps 327680, FPS 471 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 170/244 [12:17<05:15,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 169, num timesteps 348160, FPS 472 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 180/244 [13:00<04:36,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 179, num timesteps 368640, FPS 472 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 190/244 [13:43<03:50,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 189, num timesteps 389120, FPS 472 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 199/244 [14:21<03:12,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 199, num timesteps 409600, FPS 472 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 200/244 [14:36<05:26,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: Evaluation using 10 episodes: mean reward 0.94125 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 210/244 [15:21<02:36,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 209, num timesteps 430080, FPS 466 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 220/244 [16:07<01:50,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 219, num timesteps 450560, FPS 465 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 230/244 [16:52<01:03,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 229, num timesteps 471040, FPS 464 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.9/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 240/244 [17:38<00:18,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 239, num timesteps 491520, FPS 464 \n",
      " Last 10 training episodes: mean/median reward 0.9/0.9, min/max reward 0.8/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [17:57<00:00,  4.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniGrid-LavaGapS6-v0\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "Task 1: Evaluation using 10 episodes: mean reward 0.94000 \n",
      "\n",
      "Task 2: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 10/244 [00:47<18:37,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 9, num timesteps 20480, FPS 428 \n",
      " Last 10 training episodes: mean/median reward 0.0/0.0, min/max reward 0.0/0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 20/244 [01:37<18:36,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 19, num timesteps 40960, FPS 422 \n",
      " Last 10 training episodes: mean/median reward 0.0/0.0, min/max reward 0.0/0.3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 30/244 [02:26<17:18,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 29, num timesteps 61440, FPS 419 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 40/244 [03:14<16:26,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 39, num timesteps 81920, FPS 421 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 50/244 [04:02<15:37,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 49, num timesteps 102400, FPS 421 \n",
      " Last 10 training episodes: mean/median reward 0.2/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 60/244 [04:50<14:22,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 59, num timesteps 122880, FPS 423 \n",
      " Last 10 training episodes: mean/median reward 0.0/0.0, min/max reward 0.0/0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 70/244 [05:37<13:56,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 69, num timesteps 143360, FPS 424 \n",
      " Last 10 training episodes: mean/median reward 0.2/0.0, min/max reward 0.0/0.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 80/244 [06:25<13:18,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 79, num timesteps 163840, FPS 424 \n",
      " Last 10 training episodes: mean/median reward 0.2/0.0, min/max reward 0.0/0.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 90/244 [07:13<12:00,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 89, num timesteps 184320, FPS 425 \n",
      " Last 10 training episodes: mean/median reward 0.2/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 99/244 [07:55<11:24,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 99, num timesteps 204800, FPS 426 \n",
      " Last 10 training episodes: mean/median reward 0.2/0.0, min/max reward 0.0/0.9\n",
      "\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "Task 1: Evaluation using 10 episodes: mean reward 0.94000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 100/244 [08:17<23:25,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 110/244 [09:04<10:43,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 109, num timesteps 225280, FPS 414 \n",
      " Last 10 training episodes: mean/median reward 0.2/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 120/244 [09:50<09:41,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 119, num timesteps 245760, FPS 415 \n",
      " Last 10 training episodes: mean/median reward 0.2/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 130/244 [10:37<08:59,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 129, num timesteps 266240, FPS 417 \n",
      " Last 10 training episodes: mean/median reward 0.3/0.2, min/max reward 0.0/0.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 140/244 [11:25<08:11,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 139, num timesteps 286720, FPS 418 \n",
      " Last 10 training episodes: mean/median reward 0.4/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 150/244 [12:12<07:25,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 149, num timesteps 307200, FPS 419 \n",
      " Last 10 training episodes: mean/median reward 0.5/0.9, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 160/244 [13:00<06:35,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 159, num timesteps 327680, FPS 420 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 170/244 [13:46<05:44,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 169, num timesteps 348160, FPS 421 \n",
      " Last 10 training episodes: mean/median reward 0.1/0.0, min/max reward 0.0/0.8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 180/244 [14:33<04:59,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 179, num timesteps 368640, FPS 421 \n",
      " Last 10 training episodes: mean/median reward 0.2/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 190/244 [15:20<04:12,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 189, num timesteps 389120, FPS 422 \n",
      " Last 10 training episodes: mean/median reward 0.4/0.4, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 199/244 [16:02<03:30,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 199, num timesteps 409600, FPS 423 \n",
      " Last 10 training episodes: mean/median reward 0.5/0.8, min/max reward 0.0/0.9\n",
      "\n",
      "Task 0: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "Task 1: Evaluation using 10 episodes: mean reward 0.94000 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 200/244 [16:23<07:06,  9.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2: Evaluation using 10 episodes: mean reward 0.00000 \n",
      "\n",
      "len task_sequences :  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 210/244 [17:11<02:45,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 209, num timesteps 430080, FPS 417 \n",
      " Last 10 training episodes: mean/median reward 0.4/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 220/244 [17:58<01:52,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 219, num timesteps 450560, FPS 417 \n",
      " Last 10 training episodes: mean/median reward 0.2/0.0, min/max reward 0.0/1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 230/244 [18:45<01:06,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 229, num timesteps 471040, FPS 418 \n",
      " Last 10 training episodes: mean/median reward 0.5/0.6, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 240/244 [19:33<00:18,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updates 239, num timesteps 491520, FPS 419 \n",
      " Last 10 training episodes: mean/median reward 0.3/0.0, min/max reward 0.0/0.9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [19:52<00:00,  4.89s/it]\n"
     ]
    }
   ],
   "source": [
    "print('Approach: ', args.approach)\n",
    "\n",
    "tr_reward_arr = []\n",
    "te_reward_arr = {}\n",
    "\n",
    "for _type in (['mean', 'max', 'min']):\n",
    "    te_reward_arr[_type] = {}\n",
    "    for idx in range(len(taskcla)):\n",
    "        te_reward_arr[_type]['task' + str(idx)] = []\n",
    "\n",
    "for task_idx,env_name in task_sequences:\n",
    "    print(env_name)\n",
    "    # renew optimizer\n",
    "    agent.renew_optimizer()\n",
    "\n",
    "    # FlatObsWrapper for MiniGrid\n",
    "    envs = make_vec_envs(env_name, args.seed, args.num_processes, args.gamma, args.log_dir, device, False, wrapper_class=FlatObsWrapper)\n",
    "    obs = envs.reset()\n",
    "\n",
    "    rollouts = RolloutStorage(args.num_steps, args.num_processes,\n",
    "                                    obs_shape, envs.action_space,\n",
    "                                    actor_critic.recurrent_hidden_state_size)\n",
    "\n",
    "    rollouts.obs[0].copy_(obs)\n",
    "    rollouts.to(device)\n",
    "\n",
    "    episode_rewards = deque(maxlen=10)\n",
    "    num_updates = int(args.num_env_steps) // args.num_steps // args.num_processes\n",
    "\n",
    "    train_ppo(actor_critic, agent, rollouts, task_idx, env_name, task_sequences, envs,  obs_shape, args, episode_rewards, tr_reward_arr, te_reward_arr, num_updates, log_name, device, wrapper_class=FlatObsWrapper)\n",
    "\n",
    "    # post-processing\n",
    "    if args.approach == 'fine-tuning':\n",
    "        if args.single_task == True:\n",
    "            envs.close()\n",
    "            break\n",
    "        else:\n",
    "            envs.close()\n",
    "    elif args.approach == 'ft-fix':\n",
    "        # fix the backbone\n",
    "        for param in actor_critic.features.parameters():\n",
    "            param.requires_grad = False\n",
    "        if args.single_task == True:\n",
    "            envs.close()\n",
    "            break\n",
    "        else:\n",
    "            envs.close()\n",
    "    elif args.approach == 'ewc':\n",
    "        agent.update_fisher(rollouts, task_idx)\n",
    "        envs.close()\n",
    "    elif args.approach == 'blip':\n",
    "        agent.ng_post_processing(rollouts, task_idx)\n",
    "        # save the model here so that bit allocation is saved\n",
    "        save_path = os.path.join(args.save_dir, args.algo)\n",
    "        torch.save(actor_critic.state_dict(),\n",
    "            os.path.join(save_path, log_name + '_task_' + str(task_idx) + \".pth\"))\n",
    "        envs.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'MiniGrid-DoorKey-6x6-v0'), (1, 'MiniGrid-WallGapS6-v0'), (2, 'MiniGrid-LavaGapS6-v0')]\n",
      "Evaluating tasks:\n",
      "Task 0: Evaluation using 100 episodes: mean reward 0.54978 \n",
      "\n",
      "Task 1: Evaluation using 100 episodes: mean reward 0.93962 \n",
      "\n",
      "Task 2: Evaluation using 100 episodes: mean reward 0.39306 \n",
      "\n",
      "Task 0: Evaluation using 100 episodes: mean reward 0.39195 \n",
      "\n",
      "Task 1: Evaluation using 100 episodes: mean reward 0.94038 \n",
      "\n",
      "Task 2: Evaluation using 100 episodes: mean reward 0.26112 \n",
      "\n",
      "Task 0: Evaluation using 100 episodes: mean reward 0.96495 \n",
      "\n",
      "Task 1: Evaluation using 100 episodes: mean reward 0.93900 \n",
      "\n",
      "Task 2: Evaluation using 100 episodes: mean reward 0.26087 \n",
      "\n",
      "Final evaluation\n",
      "Task  0 : median reward (3 eval)  0.549775\n",
      "Task  1 : median reward (3 eval)  0.9396249999999999\n",
      "Task  2 : median reward (3 eval)  0.261125\n"
     ]
    }
   ],
   "source": [
    "from rl_module.evaluation import evaluate\n",
    "#args.seed = 2\n",
    "print(task_sequences)\n",
    "task_idx = task_sequences[-1][0]\n",
    "ob_rms = None\n",
    "seed_list = [100,200,300]\n",
    "\n",
    "print('Evaluating tasks:')\n",
    "tot_eval_episode_mean_rewards = []\n",
    "for i in range(3):\n",
    "    eval_episode_mean_rewards = evaluate(actor_critic, ob_rms, task_sequences, seed_list[i],\n",
    "                                args.num_processes, args.log_dir, device, obs_shape, task_idx, args.gamma, wrapper_class=FlatObsWrapper, episodes=100)\n",
    "    tot_eval_episode_mean_rewards.append(eval_episode_mean_rewards)\n",
    "\n",
    "tot_eval_episode_mean_rewards_median = np.median(np.array(tot_eval_episode_mean_rewards, ndmin=2), axis=0)\n",
    "\n",
    "print('Final evaluation')\n",
    "for i, value in enumerate(tot_eval_episode_mean_rewards_median):\n",
    "    print('Task ',i,': median reward (3 eval) ', value)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-loading a model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded:  ./trained_models/ppo/2023-01-14_minigrid-blip-5e5-doorkey-wallgap-lavagap_blip_1_F_prior_1e-15_task_2.pth\n"
     ]
    }
   ],
   "source": [
    "# ./trained_models/ppo/2023-01-14_minigrid-blip-5e5-doorkey-wallgap-lavagap_blip_1_F_prior_1e-15_task_2.pth\n",
    "saved_model = os.path.join(save_path, log_name + '_task_' + str(task_idx) + \".pth\")\n",
    "actor_critic3 = QPolicy(obs_shape,\n",
    "    taskcla,\n",
    "    base_kwargs={'F_prior': args.F_prior, 'recurrent': args.recurrent_policy}).to(device)   \n",
    "actor_critic3.load_state_dict(torch.load(saved_model))\n",
    "print('Model loaded: ', saved_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'MiniGrid-DoorKey-6x6-v0'), (1, 'MiniGrid-WallGapS6-v0'), (2, 'MiniGrid-LavaGapS6-v0')]\n",
      "Evaluating tasks:\n",
      "Task 0: Evaluation using 100 episodes: mean reward 0.54978 \n",
      "\n",
      "Task 1: Evaluation using 100 episodes: mean reward 0.93962 \n",
      "\n",
      "Task 2: Evaluation using 100 episodes: mean reward 0.39306 \n",
      "\n",
      "Task 0: Evaluation using 100 episodes: mean reward 0.39195 \n",
      "\n",
      "Task 1: Evaluation using 100 episodes: mean reward 0.94038 \n",
      "\n",
      "Task 2: Evaluation using 100 episodes: mean reward 0.26112 \n",
      "\n",
      "Task 0: Evaluation using 100 episodes: mean reward 0.96495 \n",
      "\n",
      "Task 1: Evaluation using 100 episodes: mean reward 0.93900 \n",
      "\n",
      "Task 2: Evaluation using 100 episodes: mean reward 0.26087 \n",
      "\n",
      "Final evaluation\n",
      "Task  0 : median reward (3 eval)  0.549775\n",
      "Task  1 : median reward (3 eval)  0.9396249999999999\n",
      "Task  2 : median reward (3 eval)  0.261125\n"
     ]
    }
   ],
   "source": [
    "from rl_module.evaluation import evaluate\n",
    "#args.seed = 2\n",
    "print(task_sequences)\n",
    "task_idx = task_sequences[-1][0]\n",
    "ob_rms = None\n",
    "seed_list = [100,200,300]\n",
    "\n",
    "print('Evaluating tasks:')\n",
    "tot_eval_episode_mean_rewards = []\n",
    "for i in range(3):\n",
    "    eval_episode_mean_rewards = evaluate(actor_critic3, ob_rms, task_sequences, seed_list[i],\n",
    "                                args.num_processes, args.log_dir, device, obs_shape, task_idx, args.gamma, wrapper_class=FlatObsWrapper, episodes=100)\n",
    "    tot_eval_episode_mean_rewards.append(eval_episode_mean_rewards)\n",
    "\n",
    "tot_eval_episode_mean_rewards_median = np.median(np.array(tot_eval_episode_mean_rewards, ndmin=2), axis=0)\n",
    "\n",
    "print('Final evaluation')\n",
    "for i, value in enumerate(tot_eval_episode_mean_rewards_median):\n",
    "    print('Task ',i,': median reward (3 eval) ', value)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check MiniGrid vectorized environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper, RGBImgObsWrapper\n",
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env_name = 'MiniGrid-DoorKey-6x6-v0'\n",
    "\n",
    "# Create vectorized environment with wrapper class\n",
    "vec_env = make_vec_envs(env_name, args.seed, args.num_processes, args.gamma, args.log_dir, device, False, wrapper_class=FlatObsWrapper)\n",
    "\n",
    "# Plot snapshot of vectorized environment and check randomized init\n",
    "vec_env.reset()\n",
    "before_img = vec_env.render('rgb_array')\n",
    "\n",
    "plt.figure(figsize = (6.,6.))\n",
    "plt.imshow(before_img);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm-experiments",
   "language": "python",
   "name": "tfm-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
