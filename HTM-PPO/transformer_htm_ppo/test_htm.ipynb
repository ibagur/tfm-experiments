{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from htm_pytorch import HTMAttention\n",
    "\n",
    "attn = HTMAttention(\n",
    "    dim = 512,\n",
    "    heads = 8,               # number of heads for within-memory attention\n",
    "    dim_head = 64,           # dimension per head for within-memory attention\n",
    "    topk_mems = 8,           # how many memory chunks to select for\n",
    "    mem_chunk_size = 32,     # number of tokens in each memory chunk\n",
    "    add_pos_enc = True       # whether to add positional encoding to the memories\n",
    ")\n",
    "\n",
    "queries = torch.randn(1, 128, 512)     # queries\n",
    "memories = torch.randn(1, 20000, 512)  # memories, of any size\n",
    "mask = torch.ones(1, 20000).bool()     # memory mask\n",
    "\n",
    "attended = attn(queries, memories, mask = mask) # (1, 128, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm_pytorch import HTMBlock\n",
    "\n",
    "block = HTMBlock(\n",
    "    dim = 512,\n",
    "    topk_mems = 8,\n",
    "    mem_chunk_size = 32,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "queries = torch.randn(1, 128, 512)\n",
    "memories = torch.randn(1, 20000, 512)\n",
    "mask = torch.ones(1, 20000).bool()\n",
    "\n",
    "out = block(queries, memories, mask = mask) # (1, 128, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm_pytorch import HTMBlockReLU\n",
    "\n",
    "blockrelu = HTMBlockReLU(\n",
    "    dim = 512,\n",
    "    topk_mems = 8,\n",
    "    mem_chunk_size = 32,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "queries = torch.randn(1, 128, 512)\n",
    "memories = torch.randn(1, 20000, 512)\n",
    "mask = torch.ones(1, 20000).bool()\n",
    "\n",
    "out = blockrelu(queries, memories, mask = mask) # (1, 128, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htm_pytorch import HTMBlockReLU\n",
    "\n",
    "blockrelu = HTMBlockReLU(\n",
    "    dim = 512,\n",
    "    topk_mems = 8,\n",
    "    mem_chunk_size = 16,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "queries = torch.randn(1, 128, 512)\n",
    "memories = torch.randn(1, 128, 512)\n",
    "mask = torch.ones(1, 128).bool()\n",
    "\n",
    "out = blockrelu(queries, memories, mask = mask) # (1, 128, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_htm import HTMTransformerBlock, HTMTransformer\n",
    "\n",
    "config = {\n",
    "    \"num_blocks\":4, \n",
    "    \"embed_dim\": 512, \n",
    "    \"num_heads\":8, \n",
    "    \"layer_norm\":\"pre\", \n",
    "    \"identity_map_reordering\":True, \n",
    "    \"topk_mems\":8, \n",
    "    \"mem_chunk_size\":16\n",
    "    }\n",
    "\n",
    "htmtransformerblock = HTMTransformerBlock(\n",
    "    embed_dim=config[\"embed_dim\"],\n",
    "    num_heads=config[\"num_heads\"],  \n",
    "    config=config\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"num_blocks\":4, \n",
    "    \"embed_dim\": 512, \n",
    "    \"num_heads\":8, \n",
    "    \"layer_norm\":\"pre\", \n",
    "    \"identity_map_reordering\":False, \n",
    "    \"topk_mems\":8, \n",
    "    \"mem_chunk_size\":16\n",
    "    }\n",
    "\n",
    "htmtransformerblock_norelu = HTMTransformerBlock(\n",
    "    embed_dim=config[\"embed_dim\"],\n",
    "    num_heads=config[\"num_heads\"],  \n",
    "    config=config\n",
    ")\n",
    "\n",
    "queries = torch.randn(1, 128, 512)\n",
    "values = queries\n",
    "keys = queries\n",
    "mask = torch.ones(1, 128).bool()\n",
    "\n",
    "out, attn_weights = htmtransformerblock(values, keys, queries, mask)\n",
    "out2, attn_weights2 = htmtransformerblock_norelu(values, keys, queries, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_htm import HTMTransformerBlock, HTMTransformer\n",
    "\n",
    "config = {\n",
    "    \"num_blocks\":4, \n",
    "    \"embed_dim\": 512, \n",
    "    \"num_heads\":8, \n",
    "    \"layer_norm\":\"pre\", \n",
    "    \"identity_map_reordering\":True, \n",
    "    \"topk_mems\":8, \n",
    "    \"mem_chunk_size\":16,\n",
    "    \"positional_encoding\":\"relative\"\n",
    "    }\n",
    "\n",
    "htmtransformer = HTMTransformer(\n",
    "    input_dim = 512,\n",
    "    max_episode_steps = 128,\n",
    "    config = config\n",
    ")\n",
    "\n",
    "queries = torch.randn(1, 512) # flattened input\n",
    "memories = torch.randn(1, 4, 512)\n",
    "mask = torch.ones(1, 128).bool()\n",
    "memory_indices = torch.randperm(128, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "h, out_memories = htmtransformer(queries, memories, mask, memory_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "print(h.shape)\n",
    "print(out_memories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.6566e-01,  6.4929e-02, -5.2239e-01, -6.3507e-01, -9.6712e-02,\n",
      "         -1.9467e-01, -7.2196e-03,  2.4872e+00, -9.9464e-01,  4.6380e-01,\n",
      "         -7.4534e-01, -6.4544e-01, -4.4616e-01, -2.8897e-01, -7.3916e-01,\n",
      "          1.1044e+00,  1.5525e+00, -3.1247e-01, -9.2064e-01,  1.0516e+00,\n",
      "         -2.3508e-01,  4.5104e-01, -1.0103e+00, -2.1380e-04, -3.5806e-01,\n",
      "          3.0700e-01, -8.2619e-01, -7.1348e-01,  2.1025e+00,  2.9316e-01,\n",
      "         -1.1990e-01,  7.6287e-01, -6.8626e-01,  7.0067e-01, -9.9076e-01,\n",
      "         -1.3339e+00, -5.4188e-01,  5.0785e-01, -7.9272e-01,  1.7046e+00,\n",
      "         -4.2457e-01, -3.9765e-01, -7.8544e-01,  5.2377e-01, -1.1061e+00,\n",
      "         -3.8341e-01, -3.4125e-01, -3.9336e-01,  1.0108e+00,  1.4276e-01,\n",
      "          6.9876e-01,  4.7560e-01, -3.0044e-01,  1.7225e+00,  5.7128e-01,\n",
      "         -2.0701e-01,  3.3593e-01, -4.8509e-01, -2.6782e-01,  4.6120e-01,\n",
      "          1.2577e+00, -3.3717e-01, -7.5854e-01, -6.9199e-02,  1.1319e+00,\n",
      "          1.7431e-01, -6.6581e-01,  2.0799e+00, -6.5028e-01,  1.1546e+00,\n",
      "         -4.3091e-01,  2.2711e+00, -1.0126e-01, -5.0417e-01, -2.2832e-01,\n",
      "          1.3772e-01,  6.2058e-02, -1.4949e-01, -4.1970e-01,  2.2792e+00,\n",
      "         -5.7878e-01, -1.0031e+00, -3.6446e-01,  6.6615e-01, -7.1518e-01,\n",
      "          2.8003e-01, -4.7108e-01, -5.4949e-01, -7.4943e-01, -2.6441e-01,\n",
      "         -5.0828e-01,  3.7102e-01,  1.6184e+00, -4.7947e-01, -8.0665e-01,\n",
      "         -1.0392e-01, -5.3435e-01,  3.9319e-02,  3.1546e-01, -4.6549e-01,\n",
      "          1.0920e+00, -4.7623e-01, -1.0501e+00, -2.3440e-01, -3.5169e-01,\n",
      "          1.0030e-02, -3.4048e-02, -1.0217e+00,  2.6878e+00, -1.0464e-01,\n",
      "         -1.5343e-01,  3.2552e+00, -7.4668e-01, -6.4693e-01,  3.4144e-01,\n",
      "          7.0472e-02,  2.3812e+00,  3.7043e+00,  2.9094e-02, -3.8001e-01,\n",
      "          4.7785e-01,  8.9537e-01, -5.1562e-01,  5.8513e-01,  6.5625e-01,\n",
      "          2.9023e-01,  5.4932e-01, -3.7770e-01,  3.9941e-02, -9.2985e-01,\n",
      "         -1.2724e-01,  9.5413e-01, -5.5636e-01, -8.4544e-01,  4.2799e-01,\n",
      "         -5.0957e-01,  9.9368e-01, -3.2391e-02, -5.0183e-01,  5.0349e-01,\n",
      "         -2.9099e-01, -3.6532e-01, -1.1491e+00,  4.5911e-01, -7.4940e-01,\n",
      "          9.9019e-02,  7.7127e-01, -2.4057e-01, -3.8652e-01,  2.2055e+00,\n",
      "         -6.5406e-01, -4.8067e-01, -8.1668e-01, -4.7573e-01,  1.5455e+00,\n",
      "          6.6070e-01, -5.1024e-01, -5.6462e-01, -1.3485e-01,  5.0739e-01,\n",
      "          8.6395e-01,  2.3597e-01,  6.3918e-01, -4.5932e-02, -7.0924e-01,\n",
      "          3.6471e+00, -9.9442e-01,  1.4412e-01, -4.3312e-01, -5.7417e-01,\n",
      "         -3.2353e-01,  9.9960e-01, -2.3511e-01, -2.8627e-01, -6.6874e-01,\n",
      "         -9.2323e-01,  1.3978e-01,  2.0402e+00,  1.5428e+00, -6.6440e-01,\n",
      "         -5.3066e-01,  3.0046e+00,  1.8305e+00,  6.4608e-01,  2.6450e-01,\n",
      "          2.8251e+00, -5.7813e-01,  3.1596e-01,  1.3282e-01,  2.4353e-01,\n",
      "         -4.5587e-01, -7.5958e-01, -6.8618e-01,  3.1532e-01, -3.6843e-01,\n",
      "          6.9104e-02,  1.1718e-01,  2.1480e+00, -5.1503e-01, -4.8202e-01,\n",
      "          1.4905e+00, -8.1828e-01, -5.2447e-01, -5.4028e-02,  1.0616e-01,\n",
      "         -5.3953e-01,  3.5023e+00, -7.3726e-02,  3.9106e-01, -3.2001e-01,\n",
      "          7.9671e-01, -1.6693e-01,  6.2742e-01,  3.0355e-02,  8.9691e-01,\n",
      "          1.3697e+00, -3.6754e-02,  4.7435e-01, -7.2421e-01, -5.1180e-01,\n",
      "          1.8298e-01, -4.8977e-01, -4.2948e-01, -3.2351e-01, -2.7168e-01,\n",
      "          1.6968e+00, -8.5526e-02, -9.9429e-01,  1.6349e-01,  2.4921e+00,\n",
      "         -1.3458e-02, -3.6289e-01, -7.7873e-01, -6.3435e-01,  1.7065e-01,\n",
      "          9.7268e-01, -6.6730e-01,  3.9569e-02, -3.9928e-01, -3.8269e-01,\n",
      "         -7.3827e-01, -2.5629e-01, -4.0801e-01,  4.0697e-02, -4.9090e-01,\n",
      "          4.0354e-01,  3.8150e+00, -4.1826e-01,  3.0958e-01, -9.0560e-01,\n",
      "          1.5837e+00, -1.9002e-01, -1.2463e+00,  1.6255e+00, -3.8052e-04,\n",
      "         -3.9970e-01, -1.0589e+00,  9.5510e-01,  1.3422e+00, -6.9437e-01,\n",
      "         -5.4776e-01,  2.9557e+00, -3.4500e-01, -5.3003e-01,  4.7778e-01,\n",
      "         -6.7899e-01, -5.4351e-01,  4.2810e-01,  3.7532e-01,  4.0771e-01,\n",
      "          2.1827e-01, -7.3133e-01, -9.4350e-01,  6.2965e-01, -1.8654e-01,\n",
      "         -6.5002e-02, -1.7263e-01, -8.9875e-01,  6.6743e-02, -4.6177e-01,\n",
      "          2.1816e-01,  1.0319e+00, -4.7443e-01, -2.0874e-01,  5.6678e-01,\n",
      "          3.1674e+00, -3.0622e-01,  6.2420e-02, -7.1357e-01, -8.2477e-01,\n",
      "          1.7873e+00,  3.6483e+00, -3.2582e-01,  4.8829e-01, -6.1052e-01,\n",
      "         -2.1802e-01,  2.8372e-01,  1.8171e+00,  1.6886e-01, -2.9960e-01,\n",
      "         -5.8185e-01, -6.8042e-01,  1.4526e-01, -9.2190e-02,  1.7397e+00,\n",
      "         -1.7441e-02,  1.0840e+00, -7.5528e-01, -4.9873e-02, -1.6541e-01,\n",
      "         -2.3648e-01, -8.1107e-01,  3.5903e+00, -5.0745e-01,  1.6550e+00,\n",
      "         -7.9329e-01, -5.7476e-01, -7.2867e-01,  4.8880e-01, -1.1089e+00,\n",
      "         -3.6346e-01, -5.9999e-01, -9.8170e-01,  1.7089e-01, -5.5022e-01,\n",
      "         -8.5288e-01, -7.5859e-01,  4.3574e-01, -7.6214e-02,  3.8437e-01,\n",
      "          6.4601e-01, -9.5619e-01, -6.7737e-01, -1.2984e-01,  1.6278e-01,\n",
      "          8.9353e-01,  2.8712e-01,  2.4936e-01,  1.1076e-01, -7.5950e-01,\n",
      "          1.0931e+00,  8.6311e-01,  2.7163e+00, -8.9881e-01,  1.1603e+00,\n",
      "          1.0447e+00,  2.2086e+00, -8.8480e-01, -4.7796e-01, -4.1018e-01,\n",
      "         -1.0751e-01,  1.5001e+00, -5.2134e-01, -9.8318e-02, -5.1821e-01,\n",
      "         -3.6546e-01,  8.4916e-01, -4.1255e-01, -4.0097e-01, -6.0430e-01,\n",
      "         -1.0532e+00, -2.3385e-01, -4.1938e-01, -3.1618e-01, -4.6039e-01,\n",
      "          1.1283e+00,  9.5004e-01, -1.0773e-01, -4.1577e-01, -1.7551e-02,\n",
      "          4.6575e-02, -4.6034e-01, -3.8357e-01, -9.7742e-01,  1.2903e-01,\n",
      "          1.9016e+00,  1.1419e+00,  2.0530e+00,  2.6392e+00, -5.5773e-01,\n",
      "          9.9933e-01, -8.9925e-01,  7.9865e-01,  9.8238e-01,  1.3069e+00,\n",
      "          1.8876e+00, -4.6148e-01, -9.7199e-01,  5.6881e-01,  1.5072e-02,\n",
      "          5.3612e-01,  5.9069e-01,  3.8452e-01, -9.3413e-01, -8.1365e-02,\n",
      "          1.7991e+00,  1.0754e+00,  3.3854e-01, -3.9043e-01,  3.5004e-01,\n",
      "         -7.4797e-01, -3.8525e-01, -4.5539e-01, -7.3017e-01,  2.9445e+00,\n",
      "          1.0445e+00,  7.2158e-01, -3.5838e-01,  1.0223e-01,  4.4171e-01,\n",
      "          1.6265e-01,  1.7496e-01,  4.6400e+00,  9.3691e-01, -5.1478e-01,\n",
      "          1.7519e+00, -5.9910e-02,  2.1747e-01, -7.6395e-01,  2.3618e+00,\n",
      "         -3.8465e-01, -5.7397e-01, -3.1414e-01, -7.7603e-01,  1.0452e+00,\n",
      "          1.3476e+00, -1.0012e+00,  1.2897e+00, -8.1287e-01,  5.5440e-02,\n",
      "          1.1183e+00,  7.5918e-01,  3.1951e-01, -2.3976e-01, -1.0257e+00,\n",
      "         -3.6518e-01, -8.7870e-01,  1.0985e+00, -6.9724e-01, -4.6877e-01,\n",
      "          3.5416e+00,  2.8204e-01,  1.0043e+00, -8.3484e-01,  1.6337e+00,\n",
      "          2.9681e-01,  3.8298e-02, -2.2472e-01, -1.0602e+00, -3.2590e-01,\n",
      "         -4.8235e-01, -4.1009e-01, -1.7292e-01, -2.3364e-01, -4.1125e-01,\n",
      "         -4.0781e-01,  6.6485e-01,  1.8220e+00, -8.3836e-01,  2.0796e+00,\n",
      "         -3.1329e-01,  1.1465e+00,  1.7514e+00, -9.0481e-01, -3.1671e-01,\n",
      "          7.3766e-01, -1.1750e-02, -9.0719e-01, -6.7529e-01,  5.1086e-01,\n",
      "          2.7442e-01,  1.2035e+00, -1.5767e-01,  1.2848e+00,  1.7944e+00,\n",
      "          1.1612e+00, -8.8630e-01, -3.1781e-01, -8.6665e-01,  4.4778e-01,\n",
      "         -5.0904e-01,  1.9570e+00, -8.0287e-01, -9.3483e-01,  6.5042e-01,\n",
      "          3.6216e-01,  1.9916e-01,  2.0555e+00,  2.6473e+00,  1.4097e+00,\n",
      "         -2.5806e-01, -4.0798e-01,  1.1631e+00, -9.3077e-01, -9.1415e-01,\n",
      "         -8.9461e-01, -1.0045e+00, -1.0410e+00, -2.9792e-01,  3.1149e-01,\n",
      "          2.0094e+00, -9.0956e-01, -4.5369e-01, -7.3782e-01, -1.3127e-01,\n",
      "          8.6823e-01,  9.5453e-01,  2.9111e+00,  3.2546e-01,  2.5389e-01,\n",
      "         -4.5069e-01, -6.5555e-01]], grad_fn=<UnsqueezeBackward0>)\n",
      "tensor([[[ 0.0000,  0.0000,  0.1103,  ...,  0.9733,  0.2630,  0.1149],\n",
      "         [-0.6136, -0.0547, -0.4539,  ...,  0.5111, -0.3802, -0.5660],\n",
      "         [-0.5213, -0.1445, -0.3769,  ...,  0.2743, -0.4164, -0.6063],\n",
      "         [-0.6877,  0.0509, -0.4279,  ...,  0.2612, -0.2972, -0.6301]]])\n"
     ]
    }
   ],
   "source": [
    "print(h)\n",
    "print(out_memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "in_features_next_layer = 576\n",
    "memory_layer_size = 256\n",
    "obs = torch.randn(1, 3, 56, 56) \n",
    "\n",
    "conv1 = nn.Conv2d(obs.shape[1], 32, 8, 4,)\n",
    "conv2 = nn.Conv2d(32, 64, 4, 2, 0)\n",
    "conv3 = nn.Conv2d(64, 64, 3, 1, 0)\n",
    "lin_hidden = nn.Linear(in_features_next_layer, memory_layer_size)\n",
    "\n",
    "h = obs\n",
    "batch_size = h.shape[0]\n",
    "# Propagate input through the visual encoder\n",
    "h = F.relu(conv1(h))\n",
    "h = F.relu(conv2(h))\n",
    "h = F.relu(conv3(h))\n",
    "# Flatten the output of the convolutional layers\n",
    "h = h.reshape((batch_size, -1))\n",
    "\n",
    "h2 = F.relu(lin_hidden(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 576])\n",
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(h.shape)\n",
    "print(h2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "obs = torch.randn(1, 384) \n",
    "print(obs.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm-experiments",
   "language": "python",
   "name": "tfm-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
