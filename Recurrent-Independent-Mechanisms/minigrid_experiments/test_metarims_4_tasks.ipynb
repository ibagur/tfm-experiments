{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNF5-qJ7B0Q3"
   },
   "source": [
    "# Meta-RIMs learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647123362972,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "aycUmr6OB0Q8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigo/.local/share/virtualenvs/tfm-experiments-K5nk3NK1/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# JUPYTER SETTINGS\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# IMPORT LIBRARIES\n",
    "\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import base64\n",
    "import datetime\n",
    "import torch\n",
    "import torch_ac\n",
    "import tensorboardX\n",
    "import sys\n",
    "import utils\n",
    "from model import ACModel\n",
    "from torch_ac.utils import DictList, ParallelEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "\n",
    "def make_envs(env_id, procs, seed=None):\n",
    "    envs = []\n",
    "    for i in range(procs):\n",
    "        if seed:\n",
    "            e = utils.make_env(env_id, seed + 10000 * i)\n",
    "        else:\n",
    "            e = utils.make_env(env_id)\n",
    "        envs.append(e)\n",
    "    env = ParallelEnv(envs)\n",
    "    return env\n",
    "\n",
    "def sample_tasks(n_tasks):\n",
    "    tasks_list = []\n",
    "    for i in range(n_tasks):\n",
    "        random_data = os.urandom(4)\n",
    "        seed = int.from_bytes(random_data, byteorder=\"big\")\n",
    "        tasks_list.append(seed)\n",
    "    return tasks_list\n",
    "\n",
    "def env_snapshot(env:ParallelEnv):\n",
    "    im_list = []\n",
    "    for e in env.envs:\n",
    "        #print(type(e.render('rgb_array')))\n",
    "        #e.reset()\n",
    "        im_list.append(e.render('rgb_array'))\n",
    "\n",
    "    fig = plt.figure(figsize=(8., 8.))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                    nrows_ncols=(4, 4),  # creates 2x2 grid of axes\n",
    "                    axes_pad=0.1,  # pad between axes in inch.\n",
    "                    )\n",
    "\n",
    "    for ax, im in zip(grid, im_list):\n",
    "        # Iterating over the grid returns the Axes.\n",
    "        ax.imshow(im)\n",
    "\n",
    "def set_freeze_status(model, params, freeze=True):\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(ext in name for ext in params):\n",
    "            param.requires_grad = False if freeze else True\n",
    "            #param.grad = None if freeze else param.grad\n",
    "\n",
    "# Function to concatenate two tasks rollout exps dictionaries\n",
    "def cat_exps(exps_1, exps_2):\n",
    "    exp_out = {}\n",
    "    for (k,v), (k2,v2) in zip(exps_1.items(), exps_2.items()):\n",
    "        if k == 'obs':\n",
    "            obs = k\n",
    "            exp_out[obs] = {}\n",
    "            for (k,v), (k2,v2) in zip(exps_1.obs.items(), exps_2.obs.items()):\n",
    "                exp_out[obs][k] = torch.cat((v, v2), 0)\n",
    "        else:\n",
    "            exp_out[k] = torch.cat((v, v2), 0)\n",
    "    return exp_out\n",
    "\n",
    "# Function to concatenate two tasks rollout logs dictionaries\n",
    "def cat_logs(logs_1, logs_2):\n",
    "    logs_out = {}\n",
    "    for (k,v), (k2,v2) in zip(logs_1.items(), logs_2.items()):\n",
    "        logs_out[k] = v + v2\n",
    "    return logs_out\n",
    "\n",
    "def change_multienv_seed(env, seed):\n",
    "    for i, e in enumerate(env.envs):\n",
    "        e.seed(seed + 10000 * i)\n",
    "        e.reset()\n",
    "    return env\n",
    "        \n",
    "def sample_tasks_experiences(agent, tasks):\n",
    "    seed_list = tasks['seed_list']\n",
    "    exps_batch = []\n",
    "    logs1_batch = []\n",
    "    for seed in seed_list:\n",
    "        agent.env = change_multienv_seed(agent.env, seed)\n",
    "        #agent.env = make_envs(env_id, procs, seed)\n",
    "        exps, logs1 = agent.collect_experiences() \n",
    "        exps_batch.append(exps)\n",
    "        logs1_batch.append(logs1)\n",
    "    return exps_batch, logs1_batch\n",
    "\n",
    "# Function to collect and concatenate all tasks rollout exps dictionaries\n",
    "def collect_tasks_meta_experiences(agent, tasks):\n",
    "    seed_list = tasks['seed_list']\n",
    "    for i, seed in enumerate(seed_list):\n",
    "        #agent.env = make_envs(env_id, procs, seed)\n",
    "        agent.env = change_multienv_seed(agent.env, seed)\n",
    "        exps, logs1 = agent.collect_experiences()\n",
    "        concat_exps = exps if i==0 else cat_exps(concat_exps, exps)\n",
    "        concat_exps = DictList(concat_exps)\n",
    "        concat_exps.obs = DictList(concat_exps.obs)\n",
    "        concat_logs1 = logs1 if i==0 else cat_logs(concat_logs1, logs1)\n",
    "    return concat_exps, concat_logs1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "\n",
    "# List of tasks\n",
    "\n",
    "sequence = 2\n",
    "\n",
    "if sequence == 0:\n",
    "    ## Experiment 0\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-RedBlueDoors-6x6-v0'), \n",
    "        (1, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (2, 'MiniGrid-WallGapS6-v0'),\n",
    "        (3, 'MiniGrid-LavaGapS6-v0')      \n",
    "        ]\n",
    "elif sequence == 1:\n",
    "    ## Experiment 1\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (1, 'MiniGrid-RedBlueDoors-6x6-v0'), \n",
    "        (2, 'MiniGrid-WallGapS6-v0'),\n",
    "        (3, 'MiniGrid-LavaGapS6-v0')\n",
    "        ]\n",
    "elif sequence == 2:\n",
    "    ## Experiment 3\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-WallGapS6-v0'),\n",
    "        (1, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (2, 'MiniGrid-RedBlueDoors-6x6-v0'), \n",
    "        (3, 'MiniGrid-SimpleCrossingS9N1-v0')   \n",
    "        ]\n",
    "elif sequence == 3:\n",
    "    ## Experiment 4\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-WallGapS6-v0'),\n",
    "        (1, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (2, 'MiniGrid-SimpleCrossingS9N1-v0'),\n",
    "        (3, 'MiniGrid-UnlockPickup-v0'), \n",
    "        ]\n",
    "elif sequence == 4:\n",
    "    ## Experiment 4\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-UnlockPickup-v0'),\n",
    "        (1, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (2, 'MiniGrid-WallGapS6-v0'),\n",
    "        (3, 'MiniGrid-SimpleCrossingS9N1-v0'), \n",
    "        ]\n",
    "elif sequence == 5:\n",
    "    ## Experiment 3\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-LavaGapS5-v0'),\n",
    "        (1, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (2, 'MiniGrid-RedBlueDoors-6x6-v0'), \n",
    "        (3, 'MiniGrid-WallGapS6-v0')   \n",
    "        ]\n",
    "\n",
    "seed_list = [123456, 789012, 345678]\n",
    "\n",
    "model = \"wallgap-doorkey-redblue-crossing\"\n",
    "frames_per_proc = 128\n",
    "processes = 16\n",
    "reshape = False\n",
    "\n",
    "for seed in seed_list:\n",
    "    \n",
    "    # START TRAINING 1st ENVIRONMENT ----------------------\n",
    "\n",
    "    # LOAD PARAMETERS\n",
    "    index = 0\n",
    "    env_id = tasks_sequence[index][1]\n",
    "\n",
    "    frames = 5e5\n",
    "    add_frames = 5e5\n",
    "\n",
    "    ## Hyper-parameters\n",
    "    args = {\n",
    "    # General parameters\n",
    "    'algo':\"ppo\",\n",
    "    'env':env_id,\n",
    "    'model':model,\n",
    "    'early_stop':False,\n",
    "    'seed':seed,\n",
    "    'log_interval':1,\n",
    "    'save_interval':10,\n",
    "    'procs':processes,\n",
    "    'frames':int(frames), # default 1e7\n",
    "    # Parameters for main algorithm\n",
    "    'epochs':4,\n",
    "    'batch_size':256,\n",
    "    'frames_per_proc':frames_per_proc, # 128 for PPO and 5 per A2C\n",
    "    'discount':0.99,\n",
    "    #'lr':2.5e-4,#0.0001, # for Adam\n",
    "    'lr':0.0007, # for RMSProp\n",
    "    #'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "    'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "    'entropy_coef': 0.01,\n",
    "    'value_loss_coef':0.5,\n",
    "    'max_grad_norm':0.5,\n",
    "    'optim_eps':1e-8,\n",
    "    'optim_alpha':0.99,\n",
    "    'clip_eps':0.2,\n",
    "    'recurrence':32, # if > 1, a LSTM is added\n",
    "    'text':False, # add a GRU for text input\n",
    "    # Model Parameters\n",
    "    'use_rim':True, # action = 'store_true'\n",
    "    'meta_learn':True,\n",
    "    'reshape_reward':reshape,\n",
    "    'date':datetime.date.today()\n",
    "    }\n",
    "\n",
    "    #args = utils.dotdict(args)\n",
    "    args = DictList(args)\n",
    "\n",
    "    args.mem = args.recurrence > 1\n",
    "\n",
    "    # RIM specific hyperparameters\n",
    "    if args.use_rim:\n",
    "        args.num_units = 6\n",
    "        args.k = 4\n",
    "        args.input_heads = 1\n",
    "\n",
    "    if args.meta_learn:\n",
    "        args.lr_alpha= args.lr\n",
    "        args.lr_beta= args.lr\n",
    "        args.inner_recurrence= 8\n",
    "        args.outer_recurrence= 32 # 4x inner_recurrence\n",
    "        args.num_tasks = 2\n",
    "        args.inner_params= ['image_conv', 'i2h', 'h2h', 'actor'] # params to be updated in inner loop\n",
    "        args.outer_params = ['query', 'key', 'value', 'comm', 'critic'] # params to be updated in outer loop\n",
    "\n",
    "\n",
    "    # INITIAL SETTINGS\n",
    "\n",
    "    # Set run dir\n",
    "\n",
    "    date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "    default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "    #model_name = args.model or default_model_name\n",
    "    model_name = '{}_{}_{}_{}_{}_metalearn_{}_rims_{}_k_{}_reshape_{}'.format(args.date, args.model, args.algo, args.seed, args.frames, args.meta_learn, args.num_units, args.k, args.reshape_reward) or default_model_name\n",
    "    model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "    # Load loggers and Tensorboard writer\n",
    "\n",
    "    txt_logger = utils.get_txt_logger(model_dir)\n",
    "    csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "    tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "    # Log command and all script arguments\n",
    "\n",
    "    #txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "    txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "    # Set seed for all randomness sources\n",
    "\n",
    "    utils.seed(args.seed)\n",
    "\n",
    "    # Set device\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    txt_logger.info(f\"Device: {device}\\n\")\n",
    "\n",
    "\n",
    "    # LOAD ENVIRONMENTS AND INITIALIZE MODELS\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "    txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "    # Load training status\n",
    "\n",
    "    try:\n",
    "        status = utils.get_status(model_dir)\n",
    "    except OSError:\n",
    "        status = {\"num_frames\": 0, \"update\": 0}\n",
    "    txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "    # Load observations preprocessor\n",
    "\n",
    "    obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "    if \"vocab\" in status:\n",
    "        preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "    txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "    # Reshape reward function\n",
    "    if args.reshape_reward:\n",
    "        def reshape_reward(obs, action, reward, done):\n",
    "            if not done:\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 1\n",
    "            return reward\n",
    "    else:\n",
    "        reshape_reward = None\n",
    "\n",
    "    # Load model\n",
    "\n",
    "    acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text, use_rim=args.use_rim, num_units=args.num_units, k=args.k, input_heads=args.input_heads)\n",
    "    if \"model_state\" in status:\n",
    "        acmodel.load_state_dict(status[\"model_state\"])\n",
    "    acmodel.to(device)\n",
    "    txt_logger.info(\"Model loaded\\n\")\n",
    "    txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "\n",
    "    # Load algo\n",
    "\n",
    "    if args.algo == \"a2c\":\n",
    "        algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "    elif args.algo == \"ppo\":\n",
    "        algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, reshape_reward)                     \n",
    "    else:\n",
    "        raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "    # change to RMSProp optimizer\n",
    "    algo.optimizer = torch.optim.RMSprop(algo.acmodel.parameters(), args.lr, eps=args.optim_eps)\n",
    "\n",
    "    if \"optimizer_state\" in status:\n",
    "        algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "    txt_logger.info(\"Optimizer loaded\\n\")\n",
    "\n",
    "    # meta-learn initializacion\n",
    "\n",
    "    # delete param_groups after it has been created\n",
    "    for i in range(len(algo.optimizer.param_groups)):\n",
    "        del algo.optimizer.param_groups[0]\n",
    "\n",
    "    # Re-create separate param_groups for different inner and outer loop optimizer lr\n",
    "\n",
    "    # inner loop param group\n",
    "    algo.optimizer.add_param_group({'params': [\n",
    "        *acmodel.image_conv.parameters(), \n",
    "        *acmodel.memory_rnn.rnn.parameters(), \n",
    "        *acmodel.actor.parameters()], 'lr': args.lr_alpha})\n",
    "\n",
    "    # outer loop param group\n",
    "    algo.optimizer.add_param_group({'params': [\n",
    "        *acmodel.critic.parameters(), \n",
    "        *acmodel.memory_rnn.key.parameters(),\n",
    "        *acmodel.memory_rnn.key_.parameters(),\n",
    "        *acmodel.memory_rnn.query.parameters(),\n",
    "        *acmodel.memory_rnn.query_.parameters(),\n",
    "        *acmodel.memory_rnn.value.parameters(),\n",
    "        *acmodel.memory_rnn.value_.parameters(),\n",
    "        *acmodel.memory_rnn.comm_attention_output.parameters()\n",
    "        ], 'lr': args.lr_beta})\n",
    "\n",
    "\n",
    "    # TRAINING LOOP\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    num_frames = status[\"num_frames\"]\n",
    "    update = status[\"update\"]\n",
    "    start_time = time.time()\n",
    "    #nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "    # Moving average parameters\n",
    "    threshold = 0.90\n",
    "    window = 10\n",
    "    rreturn_total = 0\n",
    "    i = 0\n",
    "\n",
    "    # run just once to have initial grads in all parameters and avoid backward error on first pass\n",
    "    exps, _ = algo.collect_experiences()\n",
    "    algo.update_parameters(exps)\n",
    "\n",
    "\n",
    "    while num_frames < args.frames: # STEP 2\n",
    "\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        # Sample batch of tasks: STEP 3\n",
    "        tasks_batch = sample_tasks(n_tasks=args.num_tasks)\n",
    "\n",
    "        for n, task in enumerate(tasks_batch):\n",
    "\n",
    "            algo.env = change_multienv_seed(algo.env, seed=task)\n",
    "            # Sample pre-trajectories from each task: STEP 4\n",
    "            pre_exps, pre_logs1 = algo.collect_experiences()\n",
    "            # Unfreeze inner loop params,so grads can get updated in the inner loop\n",
    "            set_freeze_status(algo.acmodel, args.inner_params, freeze=False)\n",
    "            # Freeze outer loop parameters, so grads do not get updated in the inner loop\n",
    "            set_freeze_status(algo.acmodel, args.outer_params, freeze=True)\n",
    "            # set inner RIM recurence\n",
    "            algo.recurrence = args.inner_recurrence\n",
    "            # Update parameters: STEP 6\n",
    "            algo.update_parameters(pre_exps)\n",
    "            # Sample post-trajectories t_i from tasks T_i: STEP 7\n",
    "            post_exps, post_logs1 = algo.collect_experiences()\n",
    "            # Concatenate to get D_meta: STEP 8\n",
    "            meta_exps = post_exps if n==0 else cat_exps(meta_exps, post_exps)\n",
    "            meta_exps = DictList(meta_exps)\n",
    "            meta_exps.obs = DictList(meta_exps.obs)\n",
    "            meta_logs1 = post_logs1 if n==0 else cat_logs(meta_logs1, post_logs1)\n",
    "\n",
    "        # Unfreeze outer loop params, so so grads can get updated in the outer loop\n",
    "        set_freeze_status(algo.acmodel, args.outer_params, freeze=False)\n",
    "        # Freeze inner loop params, so grads do not get updated in the outer loop\n",
    "        set_freeze_status(algo.acmodel, args.inner_params, freeze=True)   \n",
    "        \n",
    "        # set outer RIM recurence\n",
    "        algo.recurrence = args.outer_recurrence\n",
    "\n",
    "        # Update parameters while keeping inner parametes (module and policy) frozen: STEP 9\n",
    "        meta_logs2 = algo.update_parameters(meta_exps)\n",
    "\n",
    "        meta_logs = {**meta_logs1, **meta_logs2}\n",
    "        update_end_time = time.time()\n",
    "\n",
    "        num_frames += meta_logs[\"num_frames\"]\n",
    "        update += 1    \n",
    "        \n",
    "        # Print logs\n",
    "\n",
    "        if update % args.log_interval == 0:\n",
    "\n",
    "            fps = meta_logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "            duration = int(time.time() - start_time)\n",
    "            return_per_episode = utils.synthesize(meta_logs[\"return_per_episode\"])\n",
    "            rreturn_per_episode = utils.synthesize(meta_logs[\"reshaped_return_per_episode\"])\n",
    "            num_frames_per_episode = utils.synthesize(meta_logs[\"num_frames_per_episode\"])\n",
    "            # Moving average to break loop if mean reward threshold reached\n",
    "            if args.early_stop:\n",
    "                #rreturn_total +=rreturn_per_episode['mean']\n",
    "                rreturn_total +=return_per_episode['mean']\n",
    "                i+=1\n",
    "                if i >= window:\n",
    "                    rreturn_mavg = rreturn_total / i\n",
    "                    if rreturn_mavg >= threshold:\n",
    "                        break\n",
    "                    else:\n",
    "                        i = 0\n",
    "                        rreturn_total = 0\n",
    "\n",
    "            header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "            data = [update, num_frames, fps, duration]\n",
    "            #header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "            #data += rreturn_per_episode.values()\n",
    "            header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()       \n",
    "            header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "            data += num_frames_per_episode.values()\n",
    "            header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "            data += [meta_logs[\"entropy\"], meta_logs[\"value\"], meta_logs[\"policy_loss\"], meta_logs[\"value_loss\"], meta_logs[\"grad_norm\"]]\n",
    "\n",
    "            txt_logger.info(\n",
    "                \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "                .format(*data))\n",
    "\n",
    "            header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "\n",
    "            if status[\"num_frames\"] == 0:\n",
    "                csv_logger.writerow(header)\n",
    "            csv_logger.writerow(data)\n",
    "            csv_file.flush()\n",
    "\n",
    "            for field, value in zip(header, data):\n",
    "                tb_writer.add_scalar(field, value, num_frames)       \n",
    "\n",
    "        # Save status\n",
    "\n",
    "        if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "            status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                    \"model_state\": acmodel.state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "            if hasattr(preprocess_obss, \"vocab\"):\n",
    "                status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "            utils.save_status(status, model_dir)\n",
    "            txt_logger.info(\"Status saved\")\n",
    "    # STEP 10\n",
    "\n",
    "    print(\"Number of frames: \", num_frames)\n",
    "\n",
    "    torch.save(acmodel,\n",
    "            os.path.join(model_dir, model_name + '_fullmodel_task_' + str(index) + \".pth\"))\n",
    "\n",
    "    # CONTINUE TRAINING 2nd ENVIRONMENT ----------------------\n",
    "\n",
    "    index = 1\n",
    "    env_id = tasks_sequence[index][1]\n",
    "    frames = args.frames + add_frames\n",
    "\n",
    "    ## Hyper-parameters\n",
    "    args.env = env_id\n",
    "    args.frames = int(frames)\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "    txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "    # Load training status\n",
    "\n",
    "    try:\n",
    "        status = utils.get_status(model_dir)\n",
    "    except OSError:\n",
    "        status = {\"num_frames\": 0, \"update\": 0}\n",
    "    txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "    # Load observations preprocessor\n",
    "\n",
    "    obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "    if \"vocab\" in status:\n",
    "        preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "    txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "    # Reshape reward function\n",
    "    if args.reshape_reward:\n",
    "        def reshape_reward(obs, action, reward, done):\n",
    "            if not done:\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 1\n",
    "            return reward\n",
    "    else:\n",
    "        reshape_reward = None\n",
    "\n",
    "    # Load model\n",
    "\n",
    "    acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text, use_rim=args.use_rim, num_units=args.num_units, k=args.k, input_heads=args.input_heads)\n",
    "    if \"model_state\" in status:\n",
    "        acmodel.load_state_dict(status[\"model_state\"])\n",
    "    acmodel.to(device)\n",
    "    txt_logger.info(\"Model loaded\\n\")\n",
    "    txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "\n",
    "    # Load algo\n",
    "\n",
    "    if args.algo == \"a2c\":\n",
    "        algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "    elif args.algo == \"ppo\":\n",
    "        algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, reshape_reward)\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "    # change to RMSProp optimizer\n",
    "    algo.optimizer = torch.optim.RMSprop(algo.acmodel.parameters(), args.lr, eps=args.optim_eps)\n",
    "\n",
    "    # delete param_groups after it has been created\n",
    "    for i in range(len(algo.optimizer.param_groups)):\n",
    "        del algo.optimizer.param_groups[0]\n",
    "\n",
    "    # Re-create separate param_groups for different inner and outer loop optimizer lr\n",
    "\n",
    "    # inner loop param group\n",
    "    algo.optimizer.add_param_group({'params': [\n",
    "        *acmodel.image_conv.parameters(), \n",
    "        *acmodel.memory_rnn.rnn.parameters(), \n",
    "        *acmodel.actor.parameters()], 'lr': args.lr_alpha})\n",
    "\n",
    "    # outer loop param group\n",
    "    algo.optimizer.add_param_group({'params': [\n",
    "        *acmodel.critic.parameters(), \n",
    "        *acmodel.memory_rnn.key.parameters(),\n",
    "        *acmodel.memory_rnn.key_.parameters(),\n",
    "        *acmodel.memory_rnn.query.parameters(),\n",
    "        *acmodel.memory_rnn.query_.parameters(),\n",
    "        *acmodel.memory_rnn.value.parameters(),\n",
    "        *acmodel.memory_rnn.value_.parameters(),\n",
    "        *acmodel.memory_rnn.comm_attention_output.parameters()\n",
    "        ], 'lr': args.lr_beta})\n",
    "\n",
    "    if \"optimizer_state\" in status:\n",
    "        algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "    txt_logger.info(\"Optimizer loaded\\n\")\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    num_frames = status[\"num_frames\"]\n",
    "    update = status[\"update\"]\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Moving average parameters\n",
    "    threshold = 0.90\n",
    "    window = 10\n",
    "    rreturn_total = 0\n",
    "    i = 0\n",
    "\n",
    "    # run just once to have initial grads in all parameters and avoid backward error on first pass\n",
    "    exps, _ = algo.collect_experiences()\n",
    "    algo.update_parameters(exps)\n",
    "\n",
    "    while num_frames < args.frames: # STEP 2\n",
    "\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        # Sample batch of tasks: STEP 3\n",
    "        tasks_batch = sample_tasks(n_tasks=args.num_tasks)\n",
    "\n",
    "        for n, task in enumerate(tasks_batch):\n",
    "\n",
    "            algo.env = change_multienv_seed(algo.env, seed=task)\n",
    "            # Sample pre-trajectories from each task: STEP 4\n",
    "            pre_exps, pre_logs1 = algo.collect_experiences()\n",
    "            # Unfreeze inner loop params,so grads can get updated in the inner loop\n",
    "            set_freeze_status(algo.acmodel, args.inner_params, freeze=False)\n",
    "            # Freeze outer loop parameters, so grads do not get updated in the inner loop\n",
    "            set_freeze_status(algo.acmodel, args.outer_params, freeze=True)\n",
    "            # set inner RIM recurence\n",
    "            algo.recurrence = args.inner_recurrence\n",
    "            # Update parameters: STEP 6\n",
    "            algo.update_parameters(pre_exps)\n",
    "            # Sample post-trajectories t_i from tasks T_i: STEP 7\n",
    "            post_exps, post_logs1 = algo.collect_experiences()\n",
    "            # Concatenate to get D_meta: STEP 8\n",
    "            meta_exps = post_exps if n==0 else cat_exps(meta_exps, post_exps)\n",
    "            meta_exps = DictList(meta_exps)\n",
    "            meta_exps.obs = DictList(meta_exps.obs)\n",
    "            meta_logs1 = post_logs1 if n==0 else cat_logs(meta_logs1, post_logs1)\n",
    "\n",
    "        # Unfreeze outer loop params, so so grads can get updated in the outer loop\n",
    "        set_freeze_status(algo.acmodel, args.outer_params, freeze=False)\n",
    "        # Freeze inner loop params, so grads do not get updated in the outer loop\n",
    "        set_freeze_status(algo.acmodel, args.inner_params, freeze=True)   \n",
    "        \n",
    "        # set outer RIM recurence\n",
    "        algo.recurrence = args.outer_recurrence\n",
    "\n",
    "        # Update parameters while keeping inner parametes (module and policy) frozen: STEP 9\n",
    "        meta_logs2 = algo.update_parameters(meta_exps)\n",
    "\n",
    "        meta_logs = {**meta_logs1, **meta_logs2}\n",
    "        update_end_time = time.time()\n",
    "\n",
    "        num_frames += meta_logs[\"num_frames\"]\n",
    "        update += 1    \n",
    "        \n",
    "        # Print logs\n",
    "\n",
    "        if update % args.log_interval == 0:\n",
    "            \n",
    "            fps = meta_logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "            duration = int(time.time() - start_time)\n",
    "            return_per_episode = utils.synthesize(meta_logs[\"return_per_episode\"])\n",
    "            rreturn_per_episode = utils.synthesize(meta_logs[\"reshaped_return_per_episode\"])\n",
    "            num_frames_per_episode = utils.synthesize(meta_logs[\"num_frames_per_episode\"])\n",
    "            # Moving average to break loop if mean reward threshold reached\n",
    "            if args.early_stop:\n",
    "                #rreturn_total +=rreturn_per_episode['mean']\n",
    "                rreturn_total +=return_per_episode['mean']\n",
    "                i+=1\n",
    "                if i >= window:\n",
    "                    rreturn_mavg = rreturn_total / i\n",
    "                    if rreturn_mavg >= threshold:\n",
    "                        break\n",
    "                    else:\n",
    "                        i = 0\n",
    "                        rreturn_total = 0\n",
    "\n",
    "            header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "            data = [update, num_frames, fps, duration]\n",
    "            #header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "            #data += rreturn_per_episode.values()\n",
    "            header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()       \n",
    "            header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "            data += num_frames_per_episode.values()\n",
    "            header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "            data += [meta_logs[\"entropy\"], meta_logs[\"value\"], meta_logs[\"policy_loss\"], meta_logs[\"value_loss\"], meta_logs[\"grad_norm\"]]\n",
    "\n",
    "            txt_logger.info(\n",
    "                \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "                .format(*data))\n",
    "\n",
    "            header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "\n",
    "            if status[\"num_frames\"] == 0:\n",
    "                csv_logger.writerow(header)\n",
    "            csv_logger.writerow(data)\n",
    "            csv_file.flush()\n",
    "\n",
    "            for field, value in zip(header, data):\n",
    "                tb_writer.add_scalar(field, value, num_frames)       \n",
    "\n",
    "        # Save status\n",
    "\n",
    "        if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "            status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                    \"model_state\": acmodel.state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "            if hasattr(preprocess_obss, \"vocab\"):\n",
    "                status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "            utils.save_status(status, model_dir)\n",
    "            txt_logger.info(\"Status saved\")\n",
    "    # STEP 10\n",
    "\n",
    "    print(\"Number of frames: \", num_frames)\n",
    "\n",
    "    torch.save(acmodel,\n",
    "            os.path.join(model_dir, model_name + '_fullmodel_task_' + str(index) + \".pth\"))\n",
    "\n",
    "\n",
    "    # CONTINUE TRAINING 3rd ENVIRONMENT ----------------------\n",
    "\n",
    "    index = 2\n",
    "    env_id = tasks_sequence[index][1]\n",
    "    frames = args.frames + add_frames\n",
    "\n",
    "    ## Hyper-parameters\n",
    "    args.env = env_id\n",
    "    args.frames = int(frames)\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "    txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "    # Load training status\n",
    "\n",
    "    try:\n",
    "        status = utils.get_status(model_dir)\n",
    "    except OSError:\n",
    "        status = {\"num_frames\": 0, \"update\": 0}\n",
    "    txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "    # Load observations preprocessor\n",
    "\n",
    "    obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "    if \"vocab\" in status:\n",
    "        preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "    txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "    # Reshape reward function\n",
    "    if args.reshape_reward:\n",
    "        def reshape_reward(obs, action, reward, done):\n",
    "            if not done:\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 1\n",
    "            return reward\n",
    "    else:\n",
    "        reshape_reward = None\n",
    "\n",
    "    # Load model\n",
    "\n",
    "    acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text, use_rim=args.use_rim, num_units=args.num_units, k=args.k, input_heads=args.input_heads)\n",
    "    if \"model_state\" in status:\n",
    "        acmodel.load_state_dict(status[\"model_state\"])\n",
    "    acmodel.to(device)\n",
    "    txt_logger.info(\"Model loaded\\n\")\n",
    "    txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "\n",
    "    # Load algo\n",
    "\n",
    "    if args.algo == \"a2c\":\n",
    "        algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "    elif args.algo == \"ppo\":\n",
    "        algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, reshape_reward)\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "    # change to RMSProp optimizer\n",
    "    algo.optimizer = torch.optim.RMSprop(algo.acmodel.parameters(), args.lr, eps=args.optim_eps)\n",
    "\n",
    "    # delete param_groups after it has been created\n",
    "    for i in range(len(algo.optimizer.param_groups)):\n",
    "        del algo.optimizer.param_groups[0]\n",
    "\n",
    "    # Re-create separate param_groups for different inner and outer loop optimizer lr\n",
    "\n",
    "    # inner loop param group\n",
    "    algo.optimizer.add_param_group({'params': [\n",
    "        *acmodel.image_conv.parameters(), \n",
    "        *acmodel.memory_rnn.rnn.parameters(), \n",
    "        *acmodel.actor.parameters()], 'lr': args.lr_alpha})\n",
    "\n",
    "    # outer loop param group\n",
    "    algo.optimizer.add_param_group({'params': [\n",
    "        *acmodel.critic.parameters(), \n",
    "        *acmodel.memory_rnn.key.parameters(),\n",
    "        *acmodel.memory_rnn.key_.parameters(),\n",
    "        *acmodel.memory_rnn.query.parameters(),\n",
    "        *acmodel.memory_rnn.query_.parameters(),\n",
    "        *acmodel.memory_rnn.value.parameters(),\n",
    "        *acmodel.memory_rnn.value_.parameters(),\n",
    "        *acmodel.memory_rnn.comm_attention_output.parameters()\n",
    "        ], 'lr': args.lr_beta})\n",
    "\n",
    "    if \"optimizer_state\" in status:\n",
    "        algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "    txt_logger.info(\"Optimizer loaded\\n\")\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    num_frames = status[\"num_frames\"]\n",
    "    update = status[\"update\"]\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Moving average parameters\n",
    "    threshold = 0.90\n",
    "    window = 10\n",
    "    rreturn_total = 0\n",
    "    i = 0\n",
    "\n",
    "    # run just once to have initial grads in all parameters and avoid backward error on first pass\n",
    "    exps, _ = algo.collect_experiences()\n",
    "    algo.update_parameters(exps)\n",
    "\n",
    "    while num_frames < args.frames: # STEP 2\n",
    "\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        # Sample batch of tasks: STEP 3\n",
    "        tasks_batch = sample_tasks(n_tasks=args.num_tasks)\n",
    "\n",
    "        for n, task in enumerate(tasks_batch):\n",
    "\n",
    "            algo.env = change_multienv_seed(algo.env, seed=task)\n",
    "            # Sample pre-trajectories from each task: STEP 4\n",
    "            pre_exps, pre_logs1 = algo.collect_experiences()\n",
    "            # Unfreeze inner loop params,so grads can get updated in the inner loop\n",
    "            set_freeze_status(algo.acmodel, args.inner_params, freeze=False)\n",
    "            # Freeze outer loop parameters, so grads do not get updated in the inner loop\n",
    "            set_freeze_status(algo.acmodel, args.outer_params, freeze=True)\n",
    "            # set inner RIM recurence\n",
    "            algo.recurrence = args.inner_recurrence\n",
    "            # Update parameters: STEP 6\n",
    "            algo.update_parameters(pre_exps)\n",
    "            # Sample post-trajectories t_i from tasks T_i: STEP 7\n",
    "            post_exps, post_logs1 = algo.collect_experiences()\n",
    "            # Concatenate to get D_meta: STEP 8\n",
    "            meta_exps = post_exps if n==0 else cat_exps(meta_exps, post_exps)\n",
    "            meta_exps = DictList(meta_exps)\n",
    "            meta_exps.obs = DictList(meta_exps.obs)\n",
    "            meta_logs1 = post_logs1 if n==0 else cat_logs(meta_logs1, post_logs1)\n",
    "\n",
    "        # Unfreeze outer loop params, so so grads can get updated in the outer loop\n",
    "        set_freeze_status(algo.acmodel, args.outer_params, freeze=False)\n",
    "        # Freeze inner loop params, so grads do not get updated in the outer loop\n",
    "        set_freeze_status(algo.acmodel, args.inner_params, freeze=True)   \n",
    "        \n",
    "        # set outer RIM recurence\n",
    "        algo.recurrence = args.outer_recurrence\n",
    "\n",
    "        # Update parameters while keeping inner parametes (module and policy) frozen: STEP 9\n",
    "        meta_logs2 = algo.update_parameters(meta_exps)\n",
    "\n",
    "        meta_logs = {**meta_logs1, **meta_logs2}\n",
    "        update_end_time = time.time()\n",
    "\n",
    "        num_frames += meta_logs[\"num_frames\"]\n",
    "        update += 1    \n",
    "        \n",
    "        # Print logs\n",
    "\n",
    "        if update % args.log_interval == 0:\n",
    "            \n",
    "            fps = meta_logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "            duration = int(time.time() - start_time)\n",
    "            return_per_episode = utils.synthesize(meta_logs[\"return_per_episode\"])\n",
    "            rreturn_per_episode = utils.synthesize(meta_logs[\"reshaped_return_per_episode\"])\n",
    "            num_frames_per_episode = utils.synthesize(meta_logs[\"num_frames_per_episode\"])\n",
    "            # Moving average to break loop if mean reward threshold reached\n",
    "            if args.early_stop:\n",
    "                #rreturn_total +=rreturn_per_episode['mean']\n",
    "                rreturn_total +=return_per_episode['mean']\n",
    "                i+=1\n",
    "                if i >= window:\n",
    "                    rreturn_mavg = rreturn_total / i\n",
    "                    if rreturn_mavg >= threshold:\n",
    "                        break\n",
    "                    else:\n",
    "                        i = 0\n",
    "                        rreturn_total = 0\n",
    "\n",
    "            header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "            data = [update, num_frames, fps, duration]\n",
    "            #header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "            #data += rreturn_per_episode.values()\n",
    "            header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()       \n",
    "            header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "            data += num_frames_per_episode.values()\n",
    "            header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "            data += [meta_logs[\"entropy\"], meta_logs[\"value\"], meta_logs[\"policy_loss\"], meta_logs[\"value_loss\"], meta_logs[\"grad_norm\"]]\n",
    "\n",
    "            txt_logger.info(\n",
    "                \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "                .format(*data))\n",
    "\n",
    "            header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "\n",
    "            if status[\"num_frames\"] == 0:\n",
    "                csv_logger.writerow(header)\n",
    "            csv_logger.writerow(data)\n",
    "            csv_file.flush()\n",
    "\n",
    "            for field, value in zip(header, data):\n",
    "                tb_writer.add_scalar(field, value, num_frames)       \n",
    "\n",
    "        # Save status\n",
    "\n",
    "        if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "            status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                    \"model_state\": acmodel.state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "            if hasattr(preprocess_obss, \"vocab\"):\n",
    "                status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "            utils.save_status(status, model_dir)\n",
    "            txt_logger.info(\"Status saved\")\n",
    "    # STEP 10\n",
    "\n",
    "    print(\"Number of frames: \", num_frames)\n",
    "\n",
    "    torch.save(acmodel,\n",
    "            os.path.join(model_dir, model_name + '_fullmodel_task_' + str(index) + \".pth\"))\n",
    "\n",
    "\n",
    "# CONTINUE TRAINING 4th ENVIRONMENT ----------------------\n",
    "\n",
    "    index = 3\n",
    "    env_id = tasks_sequence[index][1]\n",
    "    frames = args.frames + add_frames\n",
    "\n",
    "    ## Hyper-parameters\n",
    "    args.env = env_id\n",
    "    args.frames = int(frames)\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "    txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "    # Load training status\n",
    "\n",
    "    try:\n",
    "        status = utils.get_status(model_dir)\n",
    "    except OSError:\n",
    "        status = {\"num_frames\": 0, \"update\": 0}\n",
    "    txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "    # Load observations preprocessor\n",
    "\n",
    "    obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "    if \"vocab\" in status:\n",
    "        preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "    txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "    # Reshape reward function\n",
    "    if args.reshape_reward:\n",
    "        def reshape_reward(obs, action, reward, done):\n",
    "            if not done:\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 1\n",
    "            return reward\n",
    "    else:\n",
    "        reshape_reward = None\n",
    "\n",
    "    # Load model\n",
    "\n",
    "    acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text, use_rim=args.use_rim, num_units=args.num_units, k=args.k, input_heads=args.input_heads)\n",
    "    if \"model_state\" in status:\n",
    "        acmodel.load_state_dict(status[\"model_state\"])\n",
    "    acmodel.to(device)\n",
    "    txt_logger.info(\"Model loaded\\n\")\n",
    "    txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "\n",
    "    # Load algo\n",
    "\n",
    "    if args.algo == \"a2c\":\n",
    "        algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "    elif args.algo == \"ppo\":\n",
    "        algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, reshape_reward)\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "    # change to RMSProp optimizer\n",
    "    algo.optimizer = torch.optim.RMSprop(algo.acmodel.parameters(), args.lr, eps=args.optim_eps)\n",
    "\n",
    "    # delete param_groups after it has been created\n",
    "    for i in range(len(algo.optimizer.param_groups)):\n",
    "        del algo.optimizer.param_groups[0]\n",
    "\n",
    "    # Re-create separate param_groups for different inner and outer loop optimizer lr\n",
    "\n",
    "    # inner loop param group\n",
    "    algo.optimizer.add_param_group({'params': [\n",
    "        *acmodel.image_conv.parameters(), \n",
    "        *acmodel.memory_rnn.rnn.parameters(), \n",
    "        *acmodel.actor.parameters()], 'lr': args.lr_alpha})\n",
    "\n",
    "    # outer loop param group\n",
    "    algo.optimizer.add_param_group({'params': [\n",
    "        *acmodel.critic.parameters(), \n",
    "        *acmodel.memory_rnn.key.parameters(),\n",
    "        *acmodel.memory_rnn.key_.parameters(),\n",
    "        *acmodel.memory_rnn.query.parameters(),\n",
    "        *acmodel.memory_rnn.query_.parameters(),\n",
    "        *acmodel.memory_rnn.value.parameters(),\n",
    "        *acmodel.memory_rnn.value_.parameters(),\n",
    "        *acmodel.memory_rnn.comm_attention_output.parameters()\n",
    "        ], 'lr': args.lr_beta})\n",
    "\n",
    "    if \"optimizer_state\" in status:\n",
    "        algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "    txt_logger.info(\"Optimizer loaded\\n\")\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    num_frames = status[\"num_frames\"]\n",
    "    update = status[\"update\"]\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Moving average parameters\n",
    "    threshold = 0.90\n",
    "    window = 10\n",
    "    rreturn_total = 0\n",
    "    i = 0\n",
    "\n",
    "    # run just once to have initial grads in all parameters and avoid backward error on first pass\n",
    "    exps, _ = algo.collect_experiences()\n",
    "    algo.update_parameters(exps)\n",
    "\n",
    "    while num_frames < args.frames: # STEP 2\n",
    "\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        # Sample batch of tasks: STEP 3\n",
    "        tasks_batch = sample_tasks(n_tasks=args.num_tasks)\n",
    "\n",
    "        for n, task in enumerate(tasks_batch):\n",
    "\n",
    "            algo.env = change_multienv_seed(algo.env, seed=task)\n",
    "            # Sample pre-trajectories from each task: STEP 4\n",
    "            pre_exps, pre_logs1 = algo.collect_experiences()\n",
    "            # Unfreeze inner loop params,so grads can get updated in the inner loop\n",
    "            set_freeze_status(algo.acmodel, args.inner_params, freeze=False)\n",
    "            # Freeze outer loop parameters, so grads do not get updated in the inner loop\n",
    "            set_freeze_status(algo.acmodel, args.outer_params, freeze=True)\n",
    "            # set inner RIM recurence\n",
    "            algo.recurrence = args.inner_recurrence\n",
    "            # Update parameters: STEP 6\n",
    "            algo.update_parameters(pre_exps)\n",
    "            # Sample post-trajectories t_i from tasks T_i: STEP 7\n",
    "            post_exps, post_logs1 = algo.collect_experiences()\n",
    "            # Concatenate to get D_meta: STEP 8\n",
    "            meta_exps = post_exps if n==0 else cat_exps(meta_exps, post_exps)\n",
    "            meta_exps = DictList(meta_exps)\n",
    "            meta_exps.obs = DictList(meta_exps.obs)\n",
    "            meta_logs1 = post_logs1 if n==0 else cat_logs(meta_logs1, post_logs1)\n",
    "\n",
    "        # Unfreeze outer loop params, so so grads can get updated in the outer loop\n",
    "        set_freeze_status(algo.acmodel, args.outer_params, freeze=False)\n",
    "        # Freeze inner loop params, so grads do not get updated in the outer loop\n",
    "        set_freeze_status(algo.acmodel, args.inner_params, freeze=True)   \n",
    "        \n",
    "        # set outer RIM recurence\n",
    "        algo.recurrence = args.outer_recurrence\n",
    "\n",
    "        # Update parameters while keeping inner parametes (module and policy) frozen: STEP 9\n",
    "        meta_logs2 = algo.update_parameters(meta_exps)\n",
    "\n",
    "        meta_logs = {**meta_logs1, **meta_logs2}\n",
    "        update_end_time = time.time()\n",
    "\n",
    "        num_frames += meta_logs[\"num_frames\"]\n",
    "        update += 1    \n",
    "        \n",
    "        # Print logs\n",
    "\n",
    "        if update % args.log_interval == 0:\n",
    "            \n",
    "            fps = meta_logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "            duration = int(time.time() - start_time)\n",
    "            return_per_episode = utils.synthesize(meta_logs[\"return_per_episode\"])\n",
    "            rreturn_per_episode = utils.synthesize(meta_logs[\"reshaped_return_per_episode\"])\n",
    "            num_frames_per_episode = utils.synthesize(meta_logs[\"num_frames_per_episode\"])\n",
    "            # Moving average to break loop if mean reward threshold reached\n",
    "            if args.early_stop:\n",
    "                #rreturn_total +=rreturn_per_episode['mean']\n",
    "                rreturn_total +=return_per_episode['mean']\n",
    "                i+=1\n",
    "                if i >= window:\n",
    "                    rreturn_mavg = rreturn_total / i\n",
    "                    if rreturn_mavg >= threshold:\n",
    "                        break\n",
    "                    else:\n",
    "                        i = 0\n",
    "                        rreturn_total = 0\n",
    "\n",
    "            header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "            data = [update, num_frames, fps, duration]\n",
    "            #header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "            #data += rreturn_per_episode.values()\n",
    "            header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()       \n",
    "            header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "            data += num_frames_per_episode.values()\n",
    "            header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "            data += [meta_logs[\"entropy\"], meta_logs[\"value\"], meta_logs[\"policy_loss\"], meta_logs[\"value_loss\"], meta_logs[\"grad_norm\"]]\n",
    "\n",
    "            txt_logger.info(\n",
    "                \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "                .format(*data))\n",
    "\n",
    "            header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "\n",
    "            if status[\"num_frames\"] == 0:\n",
    "                csv_logger.writerow(header)\n",
    "            csv_logger.writerow(data)\n",
    "            csv_file.flush()\n",
    "\n",
    "            for field, value in zip(header, data):\n",
    "                tb_writer.add_scalar(field, value, num_frames)       \n",
    "\n",
    "        # Save status\n",
    "\n",
    "        if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "            status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                    \"model_state\": acmodel.state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "            if hasattr(preprocess_obss, \"vocab\"):\n",
    "                status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "            utils.save_status(status, model_dir)\n",
    "            txt_logger.info(\"Status saved\")\n",
    "    # STEP 10\n",
    "\n",
    "    print(\"Number of frames: \", num_frames)\n",
    "\n",
    "    torch.save(acmodel,\n",
    "            os.path.join(model_dir, model_name + '_fullmodel_task_' + str(index) + \".pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, Run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax, num_envs=args.procs, use_memory=args.mem, use_rim=args.use_rim, num_units=args.num_units, k=args.k, input_heads=args.input_heads)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "# print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "#       .format(num_frames, fps, duration,\n",
    "#               *return_per_episode.values(),\n",
    "#               *num_frames_per_episode.values()))\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array2gif\n",
    "from pathlib import Path\n",
    "import numpy\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'env':args.env,\n",
    "'model':args.model,\n",
    "'seed':15,\n",
    "'shift':0,\n",
    "'argmax':False,\n",
    "'pause':0.1,\n",
    "'gif':args.model,\n",
    "'episodes':5,\n",
    "# Model Parameters\n",
    "'use_rim':args.use_rim,\n",
    "'num_units':args.num_units,\n",
    "'k':args.k\n",
    "}\n",
    "\n",
    "args = DictList(args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment, agent and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Load environment\n",
    "\n",
    "env = utils.make_env(args.env, args.seed)\n",
    "for _ in range(args.shift):\n",
    "    env.reset()\n",
    "print(\"Environment loaded\\n\")\n",
    "\n",
    "# Load agent\n",
    "\n",
    "model_dir = utils.get_model_dir(args.model)\n",
    "agent = utils.Agent(env.observation_space, env.action_space, model_dir, device, args.argmax, use_rim = args.use_rim, num_units = args.num_units, k = args.k)\n",
    "\n",
    "print(\"Agent loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run the agent\n",
    "\n",
    "if args.gif:\n",
    "   from array2gif import write_gif\n",
    "   frames = []\n",
    "\n",
    "# Create a window to view the environment\n",
    "env.render('human')\n",
    "\n",
    "for episode in range(args.episodes):\n",
    "    obs = env.reset()\n",
    "    done2 = False\n",
    "    while True:\n",
    "        env.render('human')\n",
    "        if args.gif:\n",
    "            frames.append(numpy.moveaxis(env.render(\"rgb_array\"), 2, 0))\n",
    "            \n",
    "\n",
    "        action = agent.get_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        agent.analyze_feedback(reward, done)\n",
    "        \n",
    "        if done or env.window.closed:\n",
    "            if episode == 4:\n",
    "                done2 = True\n",
    "            break\n",
    "    if done2 == True:\n",
    "        env.close()\n",
    "        break\n",
    "    #if env.window.closed:\n",
    "    #    break\n",
    "print('doneeee')\n",
    "if args.gif:\n",
    "    print(\"Saving gif... \", end=\"\")\n",
    "    utils.create_folders_if_necessary(\"./animation\")\n",
    "    #Path(\"./animation\").mkdir(parents=True, exist_ok=True)\n",
    "    write_gif(numpy.array(frames), \"./animation/\"+args.gif+\".gif\")\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_animation(args.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = wrap_env(env)\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(observation)\n",
    "    observation, reward, done, info = test_env.step(action)\n",
    "    episode_reward += reward\n",
    "    episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate 3rd environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id_1 = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "args.model = 'test_metarims_6_4_wallgap_doorkey_crossing_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax, num_envs=args.procs, use_memory=args.mem, use_rim=args.use_rim, num_units=args.num_units, k=args.k, input_heads=args.input_heads)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "# print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "#       .format(num_frames, fps, duration,\n",
    "#               *return_per_episode.values(),\n",
    "#               *num_frames_per_episode.values()))\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfm-experiments",
   "language": "python",
   "name": "tfm-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "d78c29e5a106d8e5aff5a2dd98f2f1ce9953cb30dd1c8e42e77397bf33d62cb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
