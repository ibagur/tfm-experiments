{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNF5-qJ7B0Q3"
   },
   "source": [
    "# MiniGrid settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdM2FnEJB0Q8"
   },
   "source": [
    "## Basic Jupyter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647123362972,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "aycUmr6OB0Q8",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef0DdE0b4pLd"
   },
   "source": [
    "## Initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLM4YYcL5rBt"
   },
   "source": [
    "Import libraries and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YgenDMtf4pLe",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigo/.local/share/virtualenvs/tfm-experiments-K5nk3NK1/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "# import stable_baselines3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint \n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28U_WEp25rBu"
   },
   "source": [
    "Minigrid helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d7eCH8Kf4pLf",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "        \n",
    "def show_animation(experiment):\n",
    "    giflist = glob.glob('animation/*.gif')\n",
    "    if len(giflist) > 0:\n",
    "        matching = [s for s in giflist if experiment in s]\n",
    "        gif_path = matching[0]\n",
    "        b64 = base64.b64encode(open(gif_path,'rb').read()).decode('ascii')\n",
    "        display.display(HTML(f'<img src=\"data:image/gif;base64,{b64}\" height=\"400\" />'))\n",
    "    else:\n",
    "        print(\"Could not find animation\")\n",
    "\n",
    "## Define rendering wrappers\n",
    "\n",
    "# Define wrapper for CNN Policy\n",
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))\n",
    "\n",
    "def gen_wrapped_env_cnn(env_name):\n",
    "    return wrap_env(ImgObsWrapper(RGBImgPartialObsWrapper(gym.make(env_name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KchGuXpd5rBv"
   },
   "source": [
    "Define the rendering wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Gdhk3Oep4pLf",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Define wrapper for CNN Policy\n",
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))\n",
    "\n",
    "def gen_wrapped_env_cnn(env_name):\n",
    "    return wrap_env(ImgObsWrapper(RGBImgPartialObsWrapper(gym.make(env_name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render an environment image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1647083269049,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "B21JwGYn5rBv",
    "outputId": "54dfa526-2621-4f91-df2b-e4ff7a447aad",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-16x16-v0'\n",
    "env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'BreakoutNoFrameskip-v4'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id ='MiniGrid-UnlockPickup-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "eval_env = gym.make(env_id)\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "#random_action = eval_env.action_space.sample()\n",
    "#new_obs, reward, done, info = eval_env.step(random_action)\n",
    "\n",
    "before_img = eval_env.render('rgb_array')\n",
    "\n",
    "plt.figure(figsize = (4.,4.))\n",
    "plt.imshow(before_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umY09KJP5rCI"
   },
   "source": [
    "# Policy Consolidation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPq1XkeL5rCI"
   },
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import base64\n",
    "import datetime\n",
    "import torch\n",
    "import torch_ac\n",
    "import tensorboardX\n",
    "import sys\n",
    "import utils\n",
    "from model import ACModel\n",
    "from torch_ac.utils import DictList, ParallelEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def make_envs(env_id, procs, seed=None):\n",
    "    envs = []\n",
    "    for i in range(procs):\n",
    "        if seed:\n",
    "            e = utils.make_env(env_id, seed + 10000 * i)\n",
    "        else:\n",
    "            e = utils.make_env(env_id)\n",
    "        envs.append(e)\n",
    "    env = ParallelEnv(envs)\n",
    "    return env\n",
    "\n",
    "# functions to calculate decays\n",
    "def constfn(val):\n",
    "    def f(_):\n",
    "        return val\n",
    "    return f\n",
    "\n",
    "def constfn_arr(val,length):\n",
    "    return [val for i in range(length)]\n",
    "\n",
    "def constfn_arr2(val,length):\n",
    "    def f(_):\n",
    "        return [val for i in range(length)]\n",
    "    return f\n",
    "\n",
    "def decayfn_arr_2(start, decay, length):\n",
    "    def f(start):\n",
    "        return [start*decay**i for i in range(length)]\n",
    "    return f\n",
    "\n",
    "def decayfn_arr(start, decay, length):\n",
    "    return [start*decay**i for i in range(length)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render Parallel environment snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "procs = 16\n",
    "max_tasks = 20\n",
    "seed_list = range(procs * max_tasks, (procs + 1) * max_tasks)\n",
    "\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N1-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N3-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS11N5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-LavaCrossingS9N2-v0'\n",
    "env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "seed = 1\n",
    "env = make_envs(env_id, procs, seed)\n",
    "obs = env.reset()\n",
    "\n",
    "im_list = []\n",
    "for e in env.envs:\n",
    "    #print(type(e.render('rgb_array')))\n",
    "    #e.reset()\n",
    "    im_list.append(e.render('rgb_array'))\n",
    "\n",
    "fig = plt.figure(figsize=(8., 8.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(4, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, im_list):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#model = 'MiniGrid-WallGapS6-v0_PPOPC_RMSProp_frames_1M_proc_16_cascade_2_nodecay_next_MiniGrid-DoorKey-6x6-v0_mid'\n",
    "model = 'test_cascade_8_adam_frames_2048_doorkey_wallgap_lavagap_clip_impsampling'\n",
    "\n",
    "processes = 16\n",
    "frames = 5e5\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'early_stop':False,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':frames, # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "'lr':2.5e-4,#0.0001, # for Adam\n",
    "#'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "#'optimizer_type':'rmsprop',\n",
    "'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'equal',\n",
    "'reshape_reward':False\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling', 'seed': 1, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 3000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'equal', 'reshape_reward': False, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x138c2f5f0>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05]}\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set run dir\n",
    "\n",
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "model_name = args.model or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "\n",
    "#txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environments, model, algo and prepare training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden models loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "if \"optimizer_state\" in status:\n",
    "    algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U 91 | F 2981888 | FPS 0702 | D 46 | rR:μσmM 0.83 0.15 0.00 0.98 | F:μσmM 67.3 57.7 10.0 360.0 | H 1.088 | V 0.498 | pL 0.005 | vL 0.019 | ∇ 0.533\n",
      "U 92 | F 3014656 | FPS 0722 | D 92 | rR:μσmM 0.84 0.15 0.00 0.97 | F:μσmM 63.4 59.5 11.0 360.0 | H 1.034 | V 0.504 | pL -0.007 | vL 0.018 | ∇ 0.541\n",
      "U 93 | F 3047424 | FPS 0362 | D 182 | rR:μσmM 0.85 0.15 0.00 0.98 | F:μσmM 61.3 55.3 10.0 360.0 | H 1.051 | V 0.531 | pL -0.005 | vL 0.018 | ∇ 0.538\n",
      "U 94 | F 3080192 | FPS 0548 | D 242 | rR:μσmM 0.85 0.14 0.00 0.97 | F:μσmM 59.2 53.2 11.0 360.0 | H 0.996 | V 0.541 | pL 0.004 | vL 0.018 | ∇ 0.522\n",
      "U 95 | F 3112960 | FPS 0735 | D 287 | rR:μσmM 0.85 0.14 0.00 0.97 | F:μσmM 60.0 54.4 12.0 360.0 | H 1.010 | V 0.535 | pL 0.003 | vL 0.018 | ∇ 0.521\n",
      "U 96 | F 3145728 | FPS 0752 | D 330 | rR:μσmM 0.86 0.13 0.00 0.97 | F:μσmM 57.6 52.1 11.0 360.0 | H 0.988 | V 0.547 | pL -0.009 | vL 0.018 | ∇ 0.533\n",
      "U 97 | F 3178496 | FPS 0732 | D 375 | rR:μσmM 0.86 0.12 0.00 0.98 | F:μσmM 56.0 47.9 10.0 360.0 | H 1.048 | V 0.558 | pL -0.004 | vL 0.016 | ∇ 0.546\n",
      "U 98 | F 3211264 | FPS 0705 | D 421 | rR:μσmM 0.87 0.13 0.00 0.98 | F:μσmM 53.1 48.2 10.0 360.0 | H 1.026 | V 0.579 | pL -0.009 | vL 0.016 | ∇ 0.537\n",
      "U 99 | F 3244032 | FPS 0730 | D 466 | rR:μσmM 0.86 0.13 0.00 0.98 | F:μσmM 53.8 48.9 9.0 360.0 | H 0.983 | V 0.573 | pL -0.005 | vL 0.015 | ∇ 0.552\n",
      "U 100 | F 3276800 | FPS 0749 | D 510 | rR:μσmM 0.87 0.11 0.00 0.98 | F:μσmM 52.0 43.4 10.0 360.0 | H 1.005 | V 0.589 | pL -0.001 | vL 0.014 | ∇ 0.549\n",
      "Status saved\n",
      "U 101 | F 3309568 | FPS 0761 | D 553 | rR:μσmM 0.88 0.08 0.38 0.97 | F:μσmM 48.0 33.0 12.0 249.0 | H 1.007 | V 0.625 | pL -0.026 | vL 0.013 | ∇ 0.555\n",
      "U 102 | F 3342336 | FPS 0755 | D 596 | rR:μσmM 0.88 0.09 0.00 0.98 | F:μσmM 46.8 34.6 9.0 360.0 | H 0.972 | V 0.633 | pL -0.009 | vL 0.012 | ∇ 0.527\n",
      "U 103 | F 3375104 | FPS 0760 | D 640 | rR:μσmM 0.89 0.08 0.00 0.97 | F:μσmM 42.9 30.0 12.0 360.0 | H 0.967 | V 0.665 | pL -0.020 | vL 0.011 | ∇ 0.549\n",
      "U 104 | F 3407872 | FPS 0720 | D 685 | rR:μσmM 0.89 0.08 0.00 0.97 | F:μσmM 45.6 32.2 11.0 360.0 | H 0.918 | V 0.638 | pL 0.007 | vL 0.011 | ∇ 0.528\n",
      "U 105 | F 3440640 | FPS 0712 | D 731 | rR:μσmM 0.89 0.09 0.00 0.98 | F:μσmM 45.8 34.4 10.0 360.0 | H 0.923 | V 0.648 | pL 0.007 | vL 0.011 | ∇ 0.537\n",
      "U 106 | F 3473408 | FPS 0717 | D 777 | rR:μσmM 0.89 0.08 0.46 0.98 | F:μσmM 43.9 30.8 10.0 216.0 | H 0.907 | V 0.654 | pL 0.008 | vL 0.011 | ∇ 0.524\n",
      "U 107 | F 3506176 | FPS 0705 | D 823 | rR:μσmM 0.90 0.08 0.36 0.97 | F:μσmM 41.8 30.7 11.0 256.0 | H 0.884 | V 0.663 | pL -0.009 | vL 0.011 | ∇ 0.536\n",
      "U 108 | F 3538944 | FPS 0713 | D 869 | rR:μσmM 0.90 0.07 0.00 0.98 | F:μσmM 39.5 29.1 9.0 360.0 | H 0.894 | V 0.683 | pL -0.008 | vL 0.010 | ∇ 0.551\n",
      "U 109 | F 3571712 | FPS 0705 | D 916 | rR:μσmM 0.90 0.06 0.57 0.97 | F:μσmM 39.4 24.4 11.0 172.0 | H 0.892 | V 0.694 | pL 0.014 | vL 0.008 | ∇ 0.525\n",
      "U 110 | F 3604480 | FPS 0714 | D 962 | rR:μσmM 0.89 0.08 0.31 0.98 | F:μσmM 42.2 31.7 10.0 276.0 | H 0.883 | V 0.669 | pL 0.015 | vL 0.010 | ∇ 0.533\n",
      "Status saved\n",
      "U 111 | F 3637248 | FPS 0722 | D 1007 | rR:μσmM 0.90 0.07 0.00 0.98 | F:μσmM 39.0 28.8 9.0 360.0 | H 0.881 | V 0.683 | pL -0.014 | vL 0.010 | ∇ 0.550\n",
      "U 112 | F 3670016 | FPS 0719 | D 1053 | rR:μσmM 0.90 0.09 0.00 0.98 | F:μσmM 40.6 33.0 10.0 360.0 | H 0.843 | V 0.671 | pL 0.017 | vL 0.009 | ∇ 0.524\n",
      "U 113 | F 3702784 | FPS 0729 | D 1098 | rR:μσmM 0.91 0.07 0.00 0.98 | F:μσmM 37.0 26.5 9.0 360.0 | H 0.876 | V 0.698 | pL -0.006 | vL 0.009 | ∇ 0.546\n",
      "U 114 | F 3735552 | FPS 0729 | D 1142 | rR:μσmM 0.91 0.07 0.00 0.98 | F:μσmM 35.8 27.6 10.0 360.0 | H 0.870 | V 0.699 | pL 0.001 | vL 0.008 | ∇ 0.538\n",
      "U 115 | F 3768320 | FPS 0732 | D 1187 | rR:μσmM 0.91 0.07 0.00 0.98 | F:μσmM 36.1 27.9 10.0 360.0 | H 0.859 | V 0.712 | pL -0.005 | vL 0.007 | ∇ 0.538\n",
      "U 116 | F 3801088 | FPS 0705 | D 1234 | rR:μσmM 0.91 0.07 0.00 0.98 | F:μσmM 35.1 25.9 10.0 360.0 | H 0.816 | V 0.709 | pL -0.010 | vL 0.007 | ∇ 0.543\n",
      "U 117 | F 3833856 | FPS 0701 | D 1280 | rR:μσmM 0.92 0.07 0.00 0.98 | F:μσmM 32.7 25.6 10.0 360.0 | H 0.814 | V 0.724 | pL -0.002 | vL 0.008 | ∇ 0.554\n",
      "U 118 | F 3866624 | FPS 0754 | D 1324 | rR:μσmM 0.91 0.08 0.00 0.98 | F:μσmM 34.8 32.2 9.0 360.0 | H 0.728 | V 0.685 | pL -0.001 | vL 0.009 | ∇ 0.528\n",
      "U 119 | F 3899392 | FPS 0799 | D 1365 | rR:μσmM 0.91 0.09 0.00 0.98 | F:μσmM 35.8 35.5 9.0 360.0 | H 0.721 | V 0.681 | pL 0.005 | vL 0.010 | ∇ 0.517\n",
      "U 120 | F 3932160 | FPS 0814 | D 1405 | rR:μσmM 0.92 0.07 0.00 0.98 | F:μσmM 31.4 27.8 9.0 360.0 | H 0.741 | V 0.710 | pL -0.015 | vL 0.009 | ∇ 0.527\n",
      "Status saved\n",
      "U 121 | F 3964928 | FPS 0801 | D 1446 | rR:μσmM 0.92 0.06 0.00 0.98 | F:μσmM 31.9 25.3 9.0 360.0 | H 0.737 | V 0.728 | pL -0.009 | vL 0.007 | ∇ 0.524\n",
      "U 122 | F 3997696 | FPS 0811 | D 1487 | rR:μσmM 0.92 0.06 0.25 0.98 | F:μσmM 31.5 24.3 9.0 298.0 | H 0.740 | V 0.729 | pL -0.008 | vL 0.009 | ∇ 0.524\n",
      "U 123 | F 4030464 | FPS 0824 | D 1526 | rR:μσmM 0.92 0.07 0.00 0.98 | F:μσmM 31.8 25.2 8.0 360.0 | H 0.719 | V 0.726 | pL 0.002 | vL 0.008 | ∇ 0.512\n",
      "U 124 | F 4063232 | FPS 0819 | D 1566 | rR:μσmM 0.92 0.09 0.00 0.98 | F:μσmM 33.6 34.7 8.0 360.0 | H 0.696 | V 0.705 | pL 0.031 | vL 0.009 | ∇ 0.502\n",
      "U 125 | F 4096000 | FPS 0816 | D 1607 | rR:μσmM 0.92 0.07 0.00 0.98 | F:μσmM 31.6 27.1 10.0 360.0 | H 0.690 | V 0.717 | pL 0.000 | vL 0.008 | ∇ 0.497\n",
      "U 126 | F 4128768 | FPS 0814 | D 1647 | rR:μσmM 0.92 0.07 0.00 0.98 | F:μσmM 31.2 25.7 9.0 360.0 | H 0.727 | V 0.730 | pL 0.002 | vL 0.008 | ∇ 0.509\n",
      "U 127 | F 4161536 | FPS 0772 | D 1689 | rR:μσmM 0.93 0.05 0.46 0.98 | F:μσmM 28.7 19.1 9.0 217.0 | H 0.719 | V 0.758 | pL 0.000 | vL 0.006 | ∇ 0.502\n",
      "U 128 | F 4194304 | FPS 0764 | D 1732 | rR:μσmM 0.92 0.06 0.21 0.98 | F:μσmM 31.1 23.5 9.0 315.0 | H 0.691 | V 0.732 | pL 0.007 | vL 0.007 | ∇ 0.488\n",
      "U 129 | F 4227072 | FPS 0801 | D 1773 | rR:μσmM 0.92 0.05 0.46 0.98 | F:μσmM 30.0 20.7 8.0 218.0 | H 0.725 | V 0.747 | pL 0.000 | vL 0.006 | ∇ 0.514\n",
      "Number of frames:  4259840\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 20\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Evaluate decay lr and cliprange    \n",
    "    # frac = 1.0 - (update - 1.0) / nupdates\n",
    "    # algo.lrs = lrs(frac)\n",
    "    # algo.clipranges = clip_epss(frac)\n",
    "    # algo.kl_betas = None # Clipped-PPO, not full KL\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()        \n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling', 'seed': 2, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 5000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'equal', 'reshape_reward': False, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x138c2f5f0>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_ac.utils.penv import ParallelEnv\n",
    "\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, Run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-DoorKey-6x6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 2926.0 | FPS 3391 | D 0.0 | R:μσmM 0.93 0.04 0.75 0.97 | F:μσmM 27.6 16.8 11.0 102.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import array2gif\n",
    "from pathlib import Path\n",
    "import numpy\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'env':args.env,\n",
    "'model':args.model,\n",
    "'seed':15,\n",
    "'shift':0,\n",
    "'argmax':False,\n",
    "'pause':0.1,\n",
    "'gif':args.model,\n",
    "'episodes':5\n",
    "}\n",
    "\n",
    "args = DictList(args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment, agent and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Load environment\n",
    "\n",
    "env = utils.make_env(args.env, args.seed)\n",
    "for _ in range(args.shift):\n",
    "    env.reset()\n",
    "print(\"Environment loaded\\n\")\n",
    "\n",
    "# Load agent\n",
    "\n",
    "model_dir = utils.get_model_dir(args.model)\n",
    "agent = utils.Agent(env.observation_space, env.action_space, model_dir, device, args.argmax)\n",
    "\n",
    "print(\"Agent loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run the agent\n",
    "\n",
    "if args.gif:\n",
    "   from array2gif import write_gif\n",
    "   frames = []\n",
    "\n",
    "# Create a window to view the environment\n",
    "env.render('human')\n",
    "\n",
    "for episode in range(args.episodes):\n",
    "    obs = env.reset()\n",
    "    done2 = False\n",
    "    while True:\n",
    "        env.render('human')\n",
    "        if args.gif:\n",
    "            frames.append(numpy.moveaxis(env.render(\"rgb_array\"), 2, 0))\n",
    "            \n",
    "\n",
    "        action = agent.get_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        agent.analyze_feedback(reward, done)\n",
    "        \n",
    "        if done or env.window.closed:\n",
    "            if episode == 4:\n",
    "                done2 = True\n",
    "            break\n",
    "    if done2 == True:\n",
    "        env.close()\n",
    "        break\n",
    "    #if env.window.closed:\n",
    "    #    break\n",
    "print('doneeee')\n",
    "if args.gif:\n",
    "    print(\"Saving gif... \", end=\"\")\n",
    "    utils.create_folders_if_necessary(\"./animation\")\n",
    "    #Path(\"./animation\").mkdir(parents=True, exist_ok=True)\n",
    "    write_gif(numpy.array(frames), \"./animation/\"+args.gif+\".gif\")\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "show_animation(args.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test_env = wrap_env(env)\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(observation)\n",
    "    observation, reward, done, info = test_env.step(action)\n",
    "    episode_reward += reward\n",
    "    episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 2nd environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "\n",
    "add_frames = 5e5\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'early_stop':False,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':frames, # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "'lr':2.5e-4,#0.0001, # for Adam\n",
    "#'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "#'optimizer_type':'rmsprop',\n",
    "'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':False\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previous loggers and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling', 'seed': 1, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 6000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': False, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x13b107440>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05]}\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "model_name = args.model or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "\n",
    "#txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing environments, model and training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden models loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "if \"optimizer_state\" in status:\n",
    "    algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U 121 | F 3964928 | FPS 0681 | D 48 | rR:μσmM 0.31 0.44 0.00 0.99 | F:μσmM 99.3 62.2 2.0 144.0 | H 0.202 | V 0.482 | pL 0.456 | vL 0.098 | ∇ 0.211\n",
      "U 122 | F 3997696 | FPS 0734 | D 92 | rR:μσmM 0.26 0.41 0.00 0.98 | F:μσmM 107.3 58.4 3.0 144.0 | H 0.200 | V 0.276 | pL 0.304 | vL 0.030 | ∇ 0.197\n",
      "U 123 | F 4030464 | FPS 0688 | D 140 | rR:μσmM 0.26 0.41 0.00 0.99 | F:μσmM 106.5 59.1 2.0 144.0 | H 0.201 | V 0.124 | pL 0.167 | vL 0.006 | ∇ 0.207\n",
      "U 124 | F 4063232 | FPS 0696 | D 187 | rR:μσmM 0.26 0.41 0.00 0.98 | F:μσmM 107.3 58.2 3.0 144.0 | H 0.187 | V 0.065 | pL 0.061 | vL 0.005 | ∇ 0.201\n",
      "U 125 | F 4096000 | FPS 0731 | D 232 | rR:μσmM 0.23 0.40 0.00 0.99 | F:μσmM 111.5 56.7 2.0 144.0 | H 0.158 | V 0.039 | pL 0.027 | vL 0.004 | ∇ 0.183\n",
      "U 126 | F 4128768 | FPS 0689 | D 279 | rR:μσmM 0.32 0.43 0.00 0.98 | F:μσmM 98.7 62.0 4.0 144.0 | H 0.204 | V 0.042 | pL -0.002 | vL 0.006 | ∇ 0.218\n",
      "U 127 | F 4161536 | FPS 0695 | D 327 | rR:μσmM 0.33 0.44 0.00 0.99 | F:μσmM 97.5 62.4 2.0 144.0 | H 0.177 | V 0.044 | pL -0.001 | vL 0.005 | ∇ 0.190\n",
      "U 128 | F 4194304 | FPS 0704 | D 373 | rR:μσmM 0.26 0.41 0.00 0.98 | F:μσmM 106.9 58.9 3.0 144.0 | H 0.160 | V 0.035 | pL 0.009 | vL 0.004 | ∇ 0.162\n",
      "U 129 | F 4227072 | FPS 0719 | D 419 | rR:μσmM 0.32 0.44 0.00 0.99 | F:μσmM 99.0 62.1 2.0 144.0 | H 0.181 | V 0.043 | pL 0.004 | vL 0.005 | ∇ 0.175\n",
      "U 130 | F 4259840 | FPS 0725 | D 464 | rR:μσmM 0.29 0.42 0.00 0.99 | F:μσmM 103.1 59.7 2.0 144.0 | H 0.151 | V 0.036 | pL -0.009 | vL 0.005 | ∇ 0.154\n",
      "Status saved\n",
      "U 131 | F 4292608 | FPS 0746 | D 508 | rR:μσmM 0.30 0.43 0.00 0.99 | F:μσmM 101.7 61.0 2.0 144.0 | H 0.159 | V 0.034 | pL -0.006 | vL 0.005 | ∇ 0.147\n",
      "U 132 | F 4325376 | FPS 0744 | D 552 | rR:μσmM 0.32 0.44 0.00 0.99 | F:μσmM 98.4 62.2 2.0 144.0 | H 0.166 | V 0.037 | pL -0.014 | vL 0.006 | ∇ 0.160\n",
      "U 133 | F 4358144 | FPS 0752 | D 596 | rR:μσmM 0.31 0.43 0.00 0.99 | F:μσmM 100.5 62.0 2.0 144.0 | H 0.145 | V 0.038 | pL 0.004 | vL 0.004 | ∇ 0.144\n",
      "U 134 | F 4390912 | FPS 0759 | D 639 | rR:μσmM 0.32 0.44 0.00 0.98 | F:μσmM 98.2 62.3 3.0 144.0 | H 0.141 | V 0.038 | pL -0.003 | vL 0.004 | ∇ 0.139\n",
      "U 135 | F 4423680 | FPS 0763 | D 682 | rR:μσmM 0.29 0.43 0.00 0.99 | F:μσmM 102.8 61.0 2.0 144.0 | H 0.132 | V 0.037 | pL 0.008 | vL 0.004 | ∇ 0.124\n",
      "U 136 | F 4456448 | FPS 0687 | D 729 | rR:μσmM 0.30 0.43 0.00 0.98 | F:μσmM 101.4 61.0 3.0 144.0 | H 0.141 | V 0.045 | pL 0.015 | vL 0.004 | ∇ 0.138\n",
      "U 137 | F 4489216 | FPS 0772 | D 772 | rR:μσmM 0.33 0.44 0.00 0.99 | F:μσmM 96.4 63.0 2.0 144.0 | H 0.133 | V 0.040 | pL -0.004 | vL 0.005 | ∇ 0.131\n",
      "U 138 | F 4521984 | FPS 0757 | D 815 | rR:μσmM 0.33 0.44 0.00 0.99 | F:μσmM 96.6 62.8 2.0 144.0 | H 0.132 | V 0.041 | pL -0.002 | vL 0.005 | ∇ 0.126\n",
      "U 139 | F 4554752 | FPS 0733 | D 860 | rR:μσmM 0.27 0.42 0.00 0.99 | F:μσmM 105.7 59.5 2.0 144.0 | H 0.136 | V 0.032 | pL 0.001 | vL 0.004 | ∇ 0.130\n",
      "U 140 | F 4587520 | FPS 0722 | D 905 | rR:μσmM 0.31 0.43 0.00 0.98 | F:μσmM 99.6 61.4 3.0 144.0 | H 0.149 | V 0.044 | pL 0.003 | vL 0.006 | ∇ 0.128\n",
      "Status saved\n",
      "U 141 | F 4620288 | FPS 0718 | D 951 | rR:μσmM 0.32 0.44 0.00 0.99 | F:μσmM 98.3 62.1 2.0 144.0 | H 0.163 | V 0.047 | pL 0.008 | vL 0.005 | ∇ 0.125\n",
      "U 142 | F 4653056 | FPS 0721 | D 996 | rR:μσmM 0.31 0.43 0.00 0.98 | F:μσmM 100.4 61.2 3.0 144.0 | H 0.134 | V 0.036 | pL -0.010 | vL 0.006 | ∇ 0.122\n",
      "U 143 | F 4685824 | FPS 0721 | D 1042 | rR:μσmM 0.32 0.44 0.00 0.99 | F:μσmM 98.4 62.3 2.0 144.0 | H 0.125 | V 0.043 | pL 0.005 | vL 0.004 | ∇ 0.116\n",
      "U 144 | F 4718592 | FPS 0707 | D 1088 | rR:μσmM 0.31 0.43 0.00 0.99 | F:μσmM 100.0 61.8 2.0 144.0 | H 0.117 | V 0.040 | pL 0.004 | vL 0.005 | ∇ 0.111\n",
      "U 145 | F 4751360 | FPS 0729 | D 1133 | rR:μσmM 0.32 0.44 0.00 0.99 | F:μσmM 98.0 62.5 2.0 144.0 | H 0.154 | V 0.031 | pL -0.017 | vL 0.005 | ∇ 0.113\n",
      "U 146 | F 4784128 | FPS 0730 | D 1178 | rR:μσmM 0.32 0.43 0.00 0.99 | F:μσmM 98.5 61.7 2.0 144.0 | H 0.175 | V 0.045 | pL 0.001 | vL 0.006 | ∇ 0.123\n",
      "U 147 | F 4816896 | FPS 0744 | D 1222 | rR:μσmM 0.33 0.44 0.00 0.99 | F:μσmM 96.9 63.1 2.0 144.0 | H 0.154 | V 0.038 | pL -0.003 | vL 0.006 | ∇ 0.113\n",
      "U 148 | F 4849664 | FPS 0727 | D 1267 | rR:μσmM 0.35 0.45 0.00 0.99 | F:μσmM 93.4 64.0 2.0 144.0 | H 0.194 | V 0.048 | pL 0.006 | vL 0.006 | ∇ 0.120\n",
      "U 149 | F 4882432 | FPS 0715 | D 1313 | rR:μσmM 0.31 0.44 0.00 0.98 | F:μσmM 99.6 62.5 3.0 144.0 | H 0.175 | V 0.037 | pL 0.005 | vL 0.004 | ∇ 0.115\n",
      "U 150 | F 4915200 | FPS 0701 | D 1360 | rR:μσmM 0.31 0.44 0.00 0.98 | F:μσmM 99.8 62.1 3.0 144.0 | H 0.166 | V 0.040 | pL 0.007 | vL 0.005 | ∇ 0.116\n",
      "Status saved\n",
      "U 151 | F 4947968 | FPS 0691 | D 1407 | rR:μσmM 0.29 0.43 0.00 0.99 | F:μσmM 102.1 61.6 2.0 144.0 | H 0.199 | V 0.036 | pL 0.010 | vL 0.004 | ∇ 0.118\n",
      "U 152 | F 4980736 | FPS 0693 | D 1454 | rR:μσmM 0.35 0.45 0.00 0.99 | F:μσmM 94.8 63.6 2.0 144.0 | H 0.209 | V 0.045 | pL -0.000 | vL 0.006 | ∇ 0.119\n",
      "U 153 | F 5013504 | FPS 0708 | D 1501 | rR:μσmM 0.34 0.44 0.00 0.99 | F:μσmM 96.2 63.4 2.0 144.0 | H 0.217 | V 0.046 | pL 0.009 | vL 0.004 | ∇ 0.131\n",
      "U 154 | F 5046272 | FPS 0700 | D 1548 | rR:μσmM 0.34 0.45 0.00 0.98 | F:μσmM 96.0 63.7 3.0 144.0 | H 0.208 | V 0.039 | pL 0.001 | vL 0.005 | ∇ 0.131\n",
      "U 155 | F 5079040 | FPS 0754 | D 1591 | rR:μσmM 0.38 0.45 0.00 0.98 | F:μσmM 90.2 64.7 3.0 144.0 | H 0.204 | V 0.053 | pL 0.004 | vL 0.007 | ∇ 0.130\n",
      "U 156 | F 5111808 | FPS 0707 | D 1637 | rR:μσmM 0.37 0.45 0.00 0.99 | F:μσmM 91.3 64.5 2.0 144.0 | H 0.190 | V 0.047 | pL 0.001 | vL 0.005 | ∇ 0.118\n",
      "U 157 | F 5144576 | FPS 0759 | D 1681 | rR:μσmM 0.33 0.45 0.00 0.98 | F:μσmM 96.7 63.8 3.0 144.0 | H 0.162 | V 0.041 | pL 0.008 | vL 0.004 | ∇ 0.109\n",
      "U 158 | F 5177344 | FPS 0721 | D 1726 | rR:μσmM 0.36 0.45 0.00 0.98 | F:μσmM 92.7 64.7 3.0 144.0 | H 0.139 | V 0.039 | pL -0.004 | vL 0.005 | ∇ 0.113\n",
      "U 159 | F 5210112 | FPS 0730 | D 1771 | rR:μσmM 0.34 0.45 0.00 0.99 | F:μσmM 96.2 63.6 2.0 144.0 | H 0.199 | V 0.042 | pL 0.006 | vL 0.005 | ∇ 0.126\n",
      "U 160 | F 5242880 | FPS 0736 | D 1815 | rR:μσmM 0.30 0.44 0.00 0.98 | F:μσmM 101.1 62.5 3.0 144.0 | H 0.171 | V 0.032 | pL 0.007 | vL 0.004 | ∇ 0.115\n",
      "Status saved\n",
      "U 161 | F 5275648 | FPS 0715 | D 1861 | rR:μσmM 0.32 0.44 0.00 0.99 | F:μσmM 97.8 63.5 2.0 144.0 | H 0.165 | V 0.035 | pL 0.002 | vL 0.005 | ∇ 0.101\n",
      "U 162 | F 5308416 | FPS 0734 | D 1906 | rR:μσmM 0.32 0.44 0.00 0.99 | F:μσmM 98.7 63.3 2.0 144.0 | H 0.144 | V 0.031 | pL 0.000 | vL 0.003 | ∇ 0.108\n",
      "U 163 | F 5341184 | FPS 0676 | D 1954 | rR:μσmM 0.29 0.44 0.00 0.99 | F:μσmM 102.0 62.2 2.0 144.0 | H 0.137 | V 0.026 | pL -0.003 | vL 0.003 | ∇ 0.092\n",
      "U 164 | F 5373952 | FPS 0697 | D 2001 | rR:μσmM 0.36 0.46 0.00 0.98 | F:μσmM 92.9 65.2 3.0 144.0 | H 0.135 | V 0.031 | pL -0.010 | vL 0.004 | ∇ 0.094\n",
      "U 165 | F 5406720 | FPS 0716 | D 2047 | rR:μσmM 0.33 0.45 0.00 0.99 | F:μσmM 97.4 64.0 2.0 144.0 | H 0.152 | V 0.028 | pL -0.005 | vL 0.003 | ∇ 0.107\n",
      "U 166 | F 5439488 | FPS 0696 | D 2094 | rR:μσmM 0.31 0.44 0.00 0.98 | F:μσmM 99.7 62.7 3.0 144.0 | H 0.126 | V 0.029 | pL -0.006 | vL 0.003 | ∇ 0.092\n",
      "U 167 | F 5472256 | FPS 0728 | D 2139 | rR:μσmM 0.29 0.43 0.00 0.99 | F:μσmM 102.8 61.6 2.0 144.0 | H 0.110 | V 0.027 | pL -0.004 | vL 0.004 | ∇ 0.093\n",
      "U 168 | F 5505024 | FPS 0721 | D 2185 | rR:μσmM 0.36 0.45 0.00 0.98 | F:μσmM 92.8 64.2 3.0 144.0 | H 0.127 | V 0.045 | pL 0.000 | vL 0.005 | ∇ 0.107\n",
      "U 169 | F 5537792 | FPS 0718 | D 2230 | rR:μσmM 0.33 0.44 0.00 0.99 | F:μσmM 97.0 63.4 2.0 144.0 | H 0.156 | V 0.039 | pL 0.005 | vL 0.005 | ∇ 0.106\n",
      "U 170 | F 5570560 | FPS 0748 | D 2274 | rR:μσmM 0.36 0.46 0.00 0.99 | F:μσmM 92.7 65.1 2.0 144.0 | H 0.107 | V 0.040 | pL 0.004 | vL 0.004 | ∇ 0.092\n",
      "Status saved\n",
      "U 171 | F 5603328 | FPS 0760 | D 2317 | rR:μσmM 0.31 0.44 0.00 0.99 | F:μσmM 99.8 62.9 2.0 144.0 | H 0.104 | V 0.036 | pL 0.010 | vL 0.003 | ∇ 0.104\n",
      "U 172 | F 5636096 | FPS 0709 | D 2364 | rR:μσmM 0.31 0.44 0.00 0.99 | F:μσmM 99.9 62.9 2.0 144.0 | H 0.115 | V 0.029 | pL -0.000 | vL 0.003 | ∇ 0.097\n",
      "U 173 | F 5668864 | FPS 0725 | D 2409 | rR:μσmM 0.29 0.43 0.00 0.98 | F:μσmM 103.2 61.6 3.0 144.0 | H 0.123 | V 0.025 | pL -0.005 | vL 0.003 | ∇ 0.112\n",
      "U 174 | F 5701632 | FPS 0767 | D 2451 | rR:μσmM 0.37 0.46 0.00 0.98 | F:μσmM 90.8 65.1 3.0 144.0 | H 0.107 | V 0.044 | pL 0.001 | vL 0.004 | ∇ 0.095\n",
      "U 175 | F 5734400 | FPS 0765 | D 2494 | rR:μσmM 0.32 0.44 0.00 0.99 | F:μσmM 98.7 63.4 2.0 144.0 | H 0.131 | V 0.031 | pL 0.001 | vL 0.003 | ∇ 0.097\n",
      "U 176 | F 5767168 | FPS 0765 | D 2537 | rR:μσmM 0.35 0.45 0.00 0.99 | F:μσmM 93.5 64.7 2.0 144.0 | H 0.112 | V 0.040 | pL 0.005 | vL 0.004 | ∇ 0.092\n",
      "U 177 | F 5799936 | FPS 0780 | D 2579 | rR:μσmM 0.35 0.45 0.00 0.98 | F:μσmM 94.1 64.3 3.0 144.0 | H 0.121 | V 0.039 | pL 0.000 | vL 0.004 | ∇ 0.085\n",
      "U 178 | F 5832704 | FPS 0748 | D 2623 | rR:μσmM 0.35 0.45 0.00 0.99 | F:μσmM 94.8 64.5 2.0 144.0 | H 0.117 | V 0.030 | pL -0.011 | vL 0.004 | ∇ 0.089\n",
      "U 179 | F 5865472 | FPS 0755 | D 2666 | rR:μσmM 0.30 0.44 0.00 0.99 | F:μσmM 100.8 62.2 2.0 144.0 | H 0.109 | V 0.026 | pL -0.011 | vL 0.004 | ∇ 0.080\n",
      "U 180 | F 5898240 | FPS 0700 | D 2713 | rR:μσmM 0.36 0.46 0.00 0.99 | F:μσmM 92.3 65.5 2.0 144.0 | H 0.112 | V 0.036 | pL 0.001 | vL 0.004 | ∇ 0.081\n",
      "Status saved\n",
      "U 181 | F 5931008 | FPS 0754 | D 2757 | rR:μσmM 0.35 0.45 0.00 0.98 | F:μσmM 93.5 65.0 3.0 144.0 | H 0.129 | V 0.032 | pL -0.007 | vL 0.004 | ∇ 0.088\n",
      "U 182 | F 5963776 | FPS 0789 | D 2798 | rR:μσmM 0.35 0.45 0.00 0.99 | F:μσmM 94.2 64.5 2.0 144.0 | H 0.156 | V 0.034 | pL -0.006 | vL 0.005 | ∇ 0.136\n",
      "U 183 | F 5996544 | FPS 0759 | D 2841 | rR:μσmM 0.33 0.45 0.00 0.98 | F:μσmM 96.3 63.8 3.0 144.0 | H 0.132 | V 0.037 | pL -0.000 | vL 0.005 | ∇ 0.094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mj/7zvl797s38qgzlm091ctt_gw0000gn/T/ipykernel_16218/28006821.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Collect experiences for cascade policies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mexps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_experiences_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Update model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/tfm-experiments/torch-ac/torch_ac/algos/base_pc.py\u001b[0m in \u001b[0;36mcollect_experiences_cascade\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;31m#obs, reward, done, _ = self.env.step(actions[0].cpu().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;31m# Update experiences values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/cursos/Data Science/master_viu/work/tfm-experiments/torch-ac/torch_ac/utils/penv.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate 2nd environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_clip_impsampling_reshaped', 'seed': 2, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 6000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14454d320>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-WallGapS6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 947.0 | FPS 3614 | D 0.0 | R:μσmM 0.94 0.02 0.88 0.99 | F:μσmM 9.3 3.3 2.0 20.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate 1st environment and test CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_clip_impsampling_reshaped', 'seed': 3, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 6000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14454d320>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 3\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-DoorKey-6x6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 20899.0 | FPS 4311 | D 4.0 | R:μσmM 0.51 0.34 0.00 0.95 | F:μσmM 184.5 120.6 19.0 360.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 3rd environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "#model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "\n",
    "add_frames = 5e5\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'early_stop':False,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':frames, # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "'lr':2.5e-4,#0.0001, # for Adam\n",
    "#'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "#'optimizer_type':'rmsprop',\n",
    "'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':False\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-SimpleCrossingS9N2-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped', 'seed': 1, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 9000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14296e680>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05]}\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "model_name = args.model or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "\n",
    "#txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden models loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "if \"optimizer_state\" in status:\n",
    "    algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U 61 | F 1998848 | FPS 0762 | D 42 | rR:μσmM 0.59 0.39 0.00 0.96 | F:μσmM 137.8 126.7 14.0 324.0 | H 0.858 | V -13.677 | pL 28.726 | vL 1118.038 | ∇ 62.790\n",
      "U 62 | F 2031616 | FPS 0752 | D 86 | rR:μσmM 0.65 0.35 0.00 0.96 | F:μσmM 120.3 114.1 15.0 324.0 | H 1.017 | V -14.820 | pL 24.913 | vL 932.368 | ∇ 46.410\n",
      "U 63 | F 2064384 | FPS 0758 | D 129 | rR:μσmM 0.68 0.32 0.00 0.96 | F:μσmM 111.4 105.2 14.0 324.0 | H 1.081 | V -15.295 | pL 23.323 | vL 854.402 | ∇ 40.493\n",
      "U 64 | F 2097152 | FPS 0791 | D 171 | rR:μσmM 0.71 0.29 0.00 0.96 | F:μσmM 99.4 95.7 14.0 324.0 | H 1.156 | V -15.511 | pL 21.258 | vL 781.626 | ∇ 37.196\n",
      "U 65 | F 2129920 | FPS 0791 | D 212 | rR:μσmM 0.72 0.29 0.00 0.96 | F:μσmM 97.4 94.7 14.0 324.0 | H 1.188 | V -16.531 | pL 20.457 | vL 729.243 | ∇ 35.055\n",
      "U 66 | F 2162688 | FPS 0728 | D 257 | rR:μσmM 0.74 0.25 0.00 0.96 | F:μσmM 91.4 85.8 13.0 324.0 | H 1.241 | V -16.742 | pL 19.262 | vL 673.609 | ∇ 37.096\n",
      "U 67 | F 2195456 | FPS 0774 | D 300 | rR:μσmM 0.76 0.25 0.00 0.96 | F:μσmM 82.8 81.6 15.0 324.0 | H 1.225 | V -17.037 | pL 17.501 | vL 608.815 | ∇ 35.975\n",
      "U 68 | F 2228224 | FPS 0769 | D 342 | rR:μσmM 0.77 0.22 0.00 0.96 | F:μσmM 82.4 74.8 14.0 324.0 | H 1.283 | V -17.738 | pL 16.769 | vL 571.613 | ∇ 35.392\n",
      "U 69 | F 2260992 | FPS 0793 | D 383 | rR:μσmM 0.77 0.23 0.00 0.96 | F:μσmM 81.1 77.6 14.0 324.0 | H 1.315 | V -17.827 | pL 16.503 | vL 565.214 | ∇ 36.379\n",
      "U 70 | F 2293760 | FPS 0795 | D 425 | rR:μσmM 0.77 0.23 0.00 0.96 | F:μσmM 82.3 78.3 14.0 324.0 | H 1.327 | V -18.261 | pL 16.694 | vL 572.774 | ∇ 35.229\n",
      "Status saved\n",
      "U 71 | F 2326528 | FPS 0803 | D 466 | rR:μσmM 0.78 0.22 0.00 0.96 | F:μσmM 76.7 73.9 14.0 324.0 | H 1.289 | V -18.413 | pL 14.827 | vL 506.102 | ∇ 38.105\n",
      "U 72 | F 2359296 | FPS 0779 | D 508 | rR:μσmM 0.82 0.17 0.00 0.96 | F:μσmM 65.0 58.1 13.0 324.0 | H 1.247 | V -18.321 | pL 12.711 | vL 427.683 | ∇ 35.921\n",
      "U 73 | F 2392064 | FPS 0791 | D 549 | rR:μσmM 0.80 0.20 0.00 0.96 | F:μσmM 72.6 69.5 13.0 324.0 | H 1.304 | V -18.971 | pL 13.879 | vL 475.705 | ∇ 37.716\n",
      "U 74 | F 2424832 | FPS 0811 | D 589 | rR:μσmM 0.80 0.19 0.00 0.96 | F:μσmM 69.8 64.7 13.0 324.0 | H 1.277 | V -18.988 | pL 13.195 | vL 449.854 | ∇ 37.305\n",
      "U 75 | F 2457600 | FPS 0796 | D 631 | rR:μσmM 0.82 0.19 0.00 0.96 | F:μσmM 63.2 65.0 13.0 324.0 | H 1.254 | V -18.920 | pL 12.726 | vL 452.270 | ∇ 40.750\n",
      "U 76 | F 2490368 | FPS 0751 | D 674 | rR:μσmM 0.84 0.16 0.00 0.96 | F:μσmM 56.8 55.3 13.0 324.0 | H 1.222 | V -19.110 | pL 10.203 | vL 368.282 | ∇ 37.076\n",
      "U 77 | F 2523136 | FPS 0815 | D 714 | rR:μσmM 0.83 0.18 0.00 0.96 | F:μσmM 58.8 60.0 13.0 324.0 | H 1.245 | V -19.287 | pL 10.458 | vL 379.486 | ∇ 42.254\n",
      "U 78 | F 2555904 | FPS 0803 | D 755 | rR:μσmM 0.85 0.15 0.00 0.96 | F:μσmM 52.8 53.6 13.0 324.0 | H 1.176 | V -19.306 | pL 9.026 | vL 351.936 | ∇ 38.897\n",
      "U 79 | F 2588672 | FPS 0796 | D 796 | rR:μσmM 0.87 0.14 0.00 0.96 | F:μσmM 47.3 47.6 13.0 324.0 | H 1.090 | V -19.294 | pL 6.957 | vL 293.469 | ∇ 37.951\n",
      "U 80 | F 2621440 | FPS 0766 | D 839 | rR:μσmM 0.88 0.13 0.00 0.96 | F:μσmM 44.3 45.2 13.0 324.0 | H 1.029 | V -19.313 | pL 6.361 | vL 288.109 | ∇ 40.299\n",
      "Status saved\n",
      "U 81 | F 2654208 | FPS 0772 | D 882 | rR:μσmM 0.89 0.11 0.00 0.96 | F:μσmM 39.8 39.9 13.0 324.0 | H 0.951 | V -19.004 | pL 4.220 | vL 238.561 | ∇ 38.662\n",
      "U 82 | F 2686976 | FPS 0734 | D 926 | rR:μσmM 0.90 0.09 0.00 0.96 | F:μσmM 37.5 33.8 13.0 324.0 | H 0.894 | V -18.934 | pL 3.117 | vL 204.620 | ∇ 37.086\n",
      "U 83 | F 2719744 | FPS 0805 | D 967 | rR:μσmM 0.91 0.07 0.39 0.96 | F:μσmM 31.6 24.0 13.0 221.0 | H 0.791 | V -17.986 | pL 0.088 | vL 137.676 | ∇ 33.311\n",
      "U 84 | F 2752512 | FPS 0800 | D 1008 | rR:μσmM 0.92 0.07 0.00 0.96 | F:μσmM 29.9 26.2 13.0 324.0 | H 0.733 | V -17.774 | pL 0.084 | vL 158.071 | ∇ 35.285\n",
      "U 85 | F 2785280 | FPS 0817 | D 1048 | rR:μσmM 0.93 0.07 0.15 0.96 | F:μσmM 26.9 23.7 13.0 306.0 | H 0.626 | V -17.042 | pL -1.349 | vL 132.184 | ∇ 33.158\n",
      "U 86 | F 2818048 | FPS 0811 | D 1088 | rR:μσmM 0.93 0.05 0.41 0.96 | F:μσmM 25.7 18.4 13.0 211.0 | H 0.554 | V -16.471 | pL -2.007 | vL 109.101 | ∇ 34.729\n",
      "U 87 | F 2850816 | FPS 0802 | D 1129 | rR:μσmM 0.93 0.04 0.52 0.96 | F:μσmM 23.8 15.4 13.0 174.0 | H 0.486 | V -16.077 | pL -3.073 | vL 94.512 | ∇ 28.328\n",
      "U 88 | F 2883584 | FPS 0769 | D 1172 | rR:μσmM 0.94 0.04 0.29 0.96 | F:μσmM 21.6 14.0 13.0 254.0 | H 0.363 | V -15.119 | pL -3.532 | vL 92.637 | ∇ 27.280\n",
      "U 89 | F 2916352 | FPS 0803 | D 1213 | rR:μσmM 0.94 0.03 0.49 0.96 | F:μσmM 21.2 11.9 13.0 184.0 | H 0.336 | V -14.862 | pL -3.906 | vL 83.080 | ∇ 25.364\n",
      "Number of frames:  2949120\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate 3rd environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-SimpleCrossingS9N2-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped', 'seed': 2, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 9000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14296e680>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-SimpleCrossingS9N2-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 3572.0 | FPS 3408 | D 1.0 | R:μσmM 0.90 0.07 0.60 0.96 | F:μσmM 34.5 26.7 13.0 145.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate 1st environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped', 'seed': 3, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 9000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14296e680>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 3\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-DoorKey-6x6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 27381.0 | FPS 3574 | D 7.0 | R:μσmM 0.30 0.31 0.00 0.93 | F:μσmM 261.9 106.8 27.0 360.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate 2nd environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped', 'seed': 3, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 9000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14296e680>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 3\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-WallGapS6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 1175.0 | FPS 3473 | D 0.0 | R:μσmM 0.93 0.04 0.75 0.99 | F:μσmM 11.7 6.8 2.0 40.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfm-experiments",
   "language": "python",
   "name": "tfm-experiments"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "5d2efec84aee61a766032e9dfbe418d90107ced57033c9077d1ba6267f248fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
