{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNF5-qJ7B0Q3"
   },
   "source": [
    "# MiniGrid settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdM2FnEJB0Q8"
   },
   "source": [
    "## Basic Jupyter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647123362972,
     "user": {
      "displayName": "IÃ±igo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "aycUmr6OB0Q8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef0DdE0b4pLd"
   },
   "source": [
    "## Initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLM4YYcL5rBt"
   },
   "source": [
    "Import libraries and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "YgenDMtf4pLe"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "# import stable_baselines3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint \n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28U_WEp25rBu"
   },
   "source": [
    "Minigrid helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "d7eCH8Kf4pLf"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "        \n",
    "def show_animation(experiment):\n",
    "    giflist = glob.glob('animation/*.gif')\n",
    "    if len(giflist) > 0:\n",
    "        matching = [s for s in giflist if experiment in s]\n",
    "        gif_path = matching[0]\n",
    "        b64 = base64.b64encode(open(gif_path,'rb').read()).decode('ascii')\n",
    "        display.display(HTML(f'<img src=\"data:image/gif;base64,{b64}\" height=\"400\" />'))\n",
    "    else:\n",
    "        print(\"Could not find animation\")\n",
    "\n",
    "## Define rendering wrappers\n",
    "\n",
    "# Define wrapper for CNN Policy\n",
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))\n",
    "\n",
    "def gen_wrapped_env_cnn(env_name):\n",
    "    return wrap_env(ImgObsWrapper(RGBImgPartialObsWrapper(gym.make(env_name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umY09KJP5rCI"
   },
   "source": [
    "# Policy Consolidation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPq1XkeL5rCI"
   },
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import base64\n",
    "import datetime\n",
    "import torch\n",
    "import torch_ac\n",
    "import tensorboardX\n",
    "import sys\n",
    "import utils\n",
    "from model import ACModel\n",
    "from torch_ac.utils import DictList, ParallelEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_envs(env_id, procs, seed=None):\n",
    "    envs = []\n",
    "    for i in range(procs):\n",
    "        if seed:\n",
    "            e = utils.make_env(env_id, seed + 10000 * i)\n",
    "        else:\n",
    "            e = utils.make_env(env_id)\n",
    "        envs.append(e)\n",
    "    env = ParallelEnv(envs)\n",
    "    return env\n",
    "\n",
    "# functions to calculate decays\n",
    "def constfn(val):\n",
    "    def f(_):\n",
    "        return val\n",
    "    return f\n",
    "\n",
    "def constfn_arr(val,length):\n",
    "    return [val for i in range(length)]\n",
    "\n",
    "def constfn_arr2(val,length):\n",
    "    def f(_):\n",
    "        return [val for i in range(length)]\n",
    "    return f\n",
    "\n",
    "def decayfn_arr_2(start, decay, length):\n",
    "    def f(start):\n",
    "        return [start*decay**i for i in range(length)]\n",
    "    return f\n",
    "\n",
    "def decayfn_arr(start, decay, length):\n",
    "    return [start*decay**i for i in range(length)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#model = 'MiniGrid-WallGapS6-v0_PPOPC_RMSProp_frames_1M_proc_16_cascade_2_nodecay_next_MiniGrid-DoorKey-6x6-v0_mid'\n",
    "#model = 'test_cascade_8_rmsprop_steps_1e6_seed_1_doorkey_wallgap_lavagap'\n",
    "model = 'redbluedoor-lavagap-doorkey-wallgap'\n",
    "\n",
    "\n",
    "processes = 16\n",
    "frames = 1e6\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'early_stop':False,\n",
    "'seed':123456,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':int(frames), # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':2.5e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'equal',\n",
    "'reshape_reward':False,\n",
    "'date':date.today()\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-RedBlueDoors-6x6-v0', 'model': 'redbluedoor-lavagap-doorkey-wallgap', 'early_stop': False, 'seed': 123456, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 1000000, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'equal', 'reshape_reward': False, 'date': datetime.date(2023, 11, 10), 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x144cb6170>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05]}\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set run dir\n",
    "\n",
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "#model_name = args.model or default_model_name\n",
    "model_name = '{}_{}_{}_{}_{}_cascade_{}_reshape_{}'.format(args.date, args.model, args.algo, args.seed, args.frames, args.cascade_depth, args.reshape_reward) or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "\n",
    "#txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environments, model, algo and prepare training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden models loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "algo.renew_optimizer()\n",
    "# if \"optimizer_state\" in status:\n",
    "#     algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "# txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 20\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Evaluate decay lr and cliprange    \n",
    "    # frac = 1.0 - (update - 1.0) / nupdates\n",
    "    # algo.lrs = lrs(frac)\n",
    "    # algo.clipranges = clip_epss(frac)\n",
    "    # algo.kl_betas = None # Clipped-PPO, not full KL\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()        \n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:Î¼ÏmM {:.2f} {:.2f} {:.2f} {:.2f} | F:Î¼ÏmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | â {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_ac.utils.penv import ParallelEnv\n",
    "\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 123456\n",
    "# Deterministic actions for evaluation, no exploit, no need of randomness for exploration\n",
    "args.argmax = True \n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, Run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [123456]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:Î¼ÏmM {:.2f} {:.2f} {:.2f} {:.2f} | F:Î¼ÏmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array2gif\n",
    "from pathlib import Path\n",
    "import numpy\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'env':args.env,\n",
    "'model':args.model,\n",
    "'seed':15,\n",
    "'shift':0,\n",
    "'argmax':True,\n",
    "'pause':0.1,\n",
    "'gif':args.model,\n",
    "'episodes':5\n",
    "}\n",
    "\n",
    "args = DictList(args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment, agent and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Load environment\n",
    "\n",
    "env = utils.make_env(args.env, args.seed)\n",
    "for _ in range(args.shift):\n",
    "    env.reset()\n",
    "print(\"Environment loaded\\n\")\n",
    "\n",
    "# Load agent\n",
    "\n",
    "model_dir = utils.get_model_dir(args.model)\n",
    "agent = utils.Agent(env.observation_space, env.action_space, model_dir, device, args.argmax)\n",
    "\n",
    "print(\"Agent loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run the agent\n",
    "\n",
    "if args.gif:\n",
    "   from array2gif import write_gif\n",
    "   frames = []\n",
    "\n",
    "# Create a window to view the environment\n",
    "env.render('human')\n",
    "\n",
    "for episode in range(args.episodes):\n",
    "    obs = env.reset()\n",
    "    done2 = False\n",
    "    while True:\n",
    "        env.render('human')\n",
    "        if args.gif:\n",
    "            frames.append(numpy.moveaxis(env.render(\"rgb_array\"), 2, 0))\n",
    "            \n",
    "\n",
    "        action = agent.get_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        agent.analyze_feedback(reward, done)\n",
    "        \n",
    "        if done or env.window.closed:\n",
    "            if episode == 4:\n",
    "                done2 = True\n",
    "            break\n",
    "    if done2 == True:\n",
    "        env.close()\n",
    "        break\n",
    "    #if env.window.closed:\n",
    "    #    break\n",
    "print('doneeee')\n",
    "if args.gif:\n",
    "    print(\"Saving gif... \", end=\"\")\n",
    "    utils.create_folders_if_necessary(\"./animation\")\n",
    "    #Path(\"./animation\").mkdir(parents=True, exist_ok=True)\n",
    "    write_gif(numpy.array(frames), \"./animation/\"+args.gif+\".gif\")\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_animation(args.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = wrap_env(env)\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(observation)\n",
    "    observation, reward, done, info = test_env.step(action)\n",
    "    episode_reward += reward\n",
    "    episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 2nd environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "\n",
    "add_frames = 1e6\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'early_stop':False,\n",
    "'seed':123456,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':int(frames), # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':2.5e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':False\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previous loggers and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "# default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "# model_name = args.model or default_model_name\n",
    "# model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# # Load loggers and Tensorboard writer\n",
    "\n",
    "# txt_logger = utils.get_txt_logger(model_dir)\n",
    "# csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "# tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# # Log command and all script arguments\n",
    "\n",
    "# #txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "# txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# # Set seed for all randomness sources\n",
    "\n",
    "# utils.seed(args.seed)\n",
    "\n",
    "# # Set device\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing environments, model and training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "algo.renew_optimizer()\n",
    "# if \"optimizer_state\" in status:\n",
    "#     algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "# txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:Î¼ÏmM {:.2f} {:.2f} {:.2f} {:.2f} | F:Î¼ÏmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | â {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 3rd environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "#model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "\n",
    "add_frames = 1e6\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'early_stop':False,\n",
    "'seed':123456,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':int(frames), # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':2.5e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':False\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "# default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "# model_name = args.model or default_model_name\n",
    "# model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# # Load loggers and Tensorboard writer\n",
    "\n",
    "# txt_logger = utils.get_txt_logger(model_dir)\n",
    "# csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "# tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# # Log command and all script arguments\n",
    "\n",
    "# #txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "# txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# # Set seed for all randomness sources\n",
    "\n",
    "# utils.seed(args.seed)\n",
    "\n",
    "# # Set device\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "algo.renew_optimizer()\n",
    "# if \"optimizer_state\" in status:\n",
    "#     algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "# txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:Î¼ÏmM {:.2f} {:.2f} {:.2f} {:.2f} | F:Î¼ÏmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | â {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 4th environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "#model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "\n",
    "add_frames = 1e6\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'early_stop':False,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':int(frames), # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':2.5e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':False\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "# default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "# model_name = args.model or default_model_name\n",
    "# model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# # Load loggers and Tensorboard writer\n",
    "\n",
    "# txt_logger = utils.get_txt_logger(model_dir)\n",
    "# csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "# tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# # Log command and all script arguments\n",
    "\n",
    "# #txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "# txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# # Set seed for all randomness sources\n",
    "\n",
    "# utils.seed(args.seed)\n",
    "\n",
    "# # Set device\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "algo.renew_optimizer()\n",
    "# if \"optimizer_state\" in status:\n",
    "#     algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "# txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:Î¼ÏmM {:.2f} {:.2f} {:.2f} {:.2f} | F:Î¼ÏmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | â {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate 1st environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 123456\n",
    "args.argmax = True # Deterministic for evaluation\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [123456]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:Î¼ÏmM {:.2f} {:.2f} {:.2f} {:.2f} | F:Î¼ÏmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate  2nd environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 123456\n",
    "args.argmax = True # Deterministic for evaluation\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [123456]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:Î¼ÏmM {:.2f} {:.2f} {:.2f} {:.2f} | F:Î¼ÏmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate  3rd environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 123456\n",
    "args.argmax = True # Deterministic for evaluation\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [123456]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:Î¼ÏmM {:.2f} {:.2f} {:.2f} {:.2f} | F:Î¼ÏmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate  4th environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 123456\n",
    "args.argmax = True # Deterministic for evaluation\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [123456]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:Î¼ÏmM {:.2f} {:.2f} {:.2f} {:.2f} | F:Î¼ÏmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfm-experiments",
   "language": "python",
   "name": "tfm-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "5d2efec84aee61a766032e9dfbe418d90107ced57033c9077d1ba6267f248fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
