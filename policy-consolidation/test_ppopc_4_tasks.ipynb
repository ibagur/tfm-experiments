{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umY09KJP5rCI"
   },
   "source": [
    "# Policy Consolidation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppo', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'wallgap-doorkey-redblue-crossing', 'early_stop': False, 'seed': 123456, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 500000, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 128, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 4, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'equal', 'reshape_reward': True, 'date': datetime.date(2023, 11, 11), 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x7f16cc790b90>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125]}\n",
      "\n",
      "Device: cpu\n",
      "\n",
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden models loaded\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PPOAlgo' object has no attribute 'collect_experiences_cascade'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4066/2334180767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Incorrect algorithm name: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0malgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenew_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     \u001b[0;31m# if \"optimizer_state\" in status:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;31m#     algo.optimizer.load_state_dict(status[\"optimizer_state\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PPOAlgo' object has no attribute 'renew_optimizer'"
     ]
    }
   ],
   "source": [
    "# JUPYTER SETTINGS\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# IMPORT LIBRARIES\n",
    "\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import base64\n",
    "import datetime\n",
    "import torch\n",
    "import torch_ac\n",
    "import tensorboardX\n",
    "import sys\n",
    "import utils\n",
    "from model import ACModel\n",
    "from torch_ac.utils import DictList, ParallelEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import date\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "\n",
    "def make_envs(env_id, procs, seed=None):\n",
    "    envs = []\n",
    "    for i in range(procs):\n",
    "        if seed:\n",
    "            e = utils.make_env(env_id, seed + 10000 * i)\n",
    "        else:\n",
    "            e = utils.make_env(env_id)\n",
    "        envs.append(e)\n",
    "    env = ParallelEnv(envs)\n",
    "    return env\n",
    "\n",
    "# functions to calculate decays\n",
    "def constfn(val):\n",
    "    def f(_):\n",
    "        return val\n",
    "    return f\n",
    "\n",
    "def constfn_arr(val,length):\n",
    "    return [val for i in range(length)]\n",
    "\n",
    "def constfn_arr2(val,length):\n",
    "    def f(_):\n",
    "        return [val for i in range(length)]\n",
    "    return f\n",
    "\n",
    "def decayfn_arr_2(start, decay, length):\n",
    "    def f(start):\n",
    "        return [start*decay**i for i in range(length)]\n",
    "    return f\n",
    "\n",
    "def decayfn_arr(start, decay, length):\n",
    "    return [start*decay**i for i in range(length)]\n",
    "\n",
    "# List of tasks\n",
    "\n",
    "sequence = 2\n",
    "\n",
    "if sequence == 0:\n",
    "    ## Experiment 0\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-RedBlueDoors-6x6-v0'), \n",
    "        (1, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (2, 'MiniGrid-WallGapS6-v0'),\n",
    "        (3, 'MiniGrid-LavaGapS6-v0')      \n",
    "        ]\n",
    "elif sequence == 1:\n",
    "    ## Experiment 1\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (1, 'MiniGrid-RedBlueDoors-6x6-v0'), \n",
    "        (2, 'MiniGrid-WallGapS6-v0'),\n",
    "        (3, 'MiniGrid-LavaGapS6-v0')\n",
    "        ]\n",
    "elif sequence == 2:\n",
    "    ## Experiment 3\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-WallGapS6-v0'),\n",
    "        (1, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (2, 'MiniGrid-RedBlueDoors-6x6-v0'), \n",
    "        (3, 'MiniGrid-SimpleCrossingS9N1-v0')   \n",
    "        ]\n",
    "elif sequence == 3:\n",
    "    ## Experiment 4\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-WallGapS6-v0'),\n",
    "        (1, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (2, 'MiniGrid-SimpleCrossingS9N1-v0'),\n",
    "        (3, 'MiniGrid-UnlockPickup-v0'), \n",
    "        ]\n",
    "elif sequence == 4:\n",
    "    ## Experiment 4\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-UnlockPickup-v0'),\n",
    "        (1, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (2, 'MiniGrid-WallGapS6-v0'),\n",
    "        (3, 'MiniGrid-SimpleCrossingS9N1-v0'), \n",
    "        ]\n",
    "elif sequence == 5:\n",
    "    ## Experiment 3\n",
    "    taskcla = [(0,7), (1,7), (2,7), (3,7)]\n",
    "    tasks_sequence = [\n",
    "        (0, 'MiniGrid-LavaGapS5-v0'),\n",
    "        (1, 'MiniGrid-DoorKey-6x6-v0'),\n",
    "        (2, 'MiniGrid-RedBlueDoors-6x6-v0'), \n",
    "        (3, 'MiniGrid-WallGapS6-v0')   \n",
    "        ]\n",
    "\n",
    "seed_list = [123456, 789012, 345678]\n",
    "\n",
    "model = \"wallgap-doorkey-redblue-crossing\"\n",
    "frames_per_proc = 512\n",
    "processes = 16\n",
    "\n",
    "for seed in seed_list:\n",
    "    \n",
    "    # START TRAINING 1st ENVIRONMENT ----------------------\n",
    "\n",
    "    # LOAD PARAMETERS\n",
    "    index = 0\n",
    "    env_id = tasks_sequence[index][1]\n",
    "\n",
    "    frames = 5e5\n",
    "    add_frames = 5e5\n",
    "\n",
    "    ## Hyper-parameters\n",
    "    args = {\n",
    "    # General parameters\n",
    "    'algo':\"ppopc\",\n",
    "    'env':env_id,\n",
    "    'model':model,\n",
    "    'early_stop':False,\n",
    "    'seed':seed,\n",
    "    'log_interval':1,\n",
    "    'save_interval':10,\n",
    "    'procs':processes,\n",
    "    'frames':int(frames), # default 1e7\n",
    "    # Parameters for main algorithm\n",
    "    'epochs':4,\n",
    "    'batch_size':256,\n",
    "    'frames_per_proc':frames_per_proc, # 128 for PPO and 5 per A2C\n",
    "    'discount':0.99,\n",
    "    #'lr':2.5e-4,#0.0001, # for Adam\n",
    "    'lr':0.0007, # for RMSProp\n",
    "    #'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "    'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "    'entropy_coef': 0.01,\n",
    "    'value_loss_coef':0.5,\n",
    "    'max_grad_norm':0.5,\n",
    "    'optim_eps':1e-8,\n",
    "    'optim_alpha':0.99,\n",
    "    'clip_eps':0.2,\n",
    "    'recurrence':1, # if > 1, a LSTM is added\n",
    "    'text':False, # add a GRU for text input\n",
    "    # Model Parameters\n",
    "    'cascade_depth':4,\n",
    "    'flow_factor':1.0,\n",
    "    'mesh_factor':4.0,\n",
    "    'lr_decay':True,\n",
    "    'imp_sampling':'clipped',\n",
    "    'imp_clips': [-5,5],\n",
    "    'dynamic_neglogpacs':False,\n",
    "    'optimizer_type':'rmsprop',\n",
    "    #'optimizer_type':'adam',\n",
    "    'scheduler_flag':False,\n",
    "    'var_init':'equal',\n",
    "    'reshape_reward':True,\n",
    "    'date':date.today()\n",
    "    }\n",
    "\n",
    "    #args = utils.dotdict(args)\n",
    "    args = DictList(args)\n",
    "\n",
    "    args.mem = args.recurrence > 1\n",
    "\n",
    "    # Create decay learning rates and clip ranges\n",
    "    if isinstance(args.lr, float):\n",
    "        if args.lr_decay:\n",
    "            args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "            args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "        else:\n",
    "            args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "            args.lrs_fn = None\n",
    "    else: assert callable(args.lr)\n",
    "    if isinstance(args.clip_eps, float):\n",
    "        if args.lr_decay:\n",
    "            args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "        else:\n",
    "            args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "    else: assert callable(args.clip_eps)\n",
    "\n",
    "\n",
    "    # INITIAL SETTINGS\n",
    "\n",
    "    # Set run dir\n",
    "\n",
    "    date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "    default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "    #model_name = args.model or default_model_name\n",
    "    model_name = '{}_{}_{}_{}_{}_frames_{}_cascade_{}_reshape_{}'.format(args.date, args.model, args.algo, args.seed, args.frames, args.frames_per_proc, args.cascade_depth, args.reshape_reward) or default_model_name\n",
    "    model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "    # Load loggers and Tensorboard writer\n",
    "\n",
    "    txt_logger = utils.get_txt_logger(model_dir)\n",
    "    csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "    tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "    # Log command and all script arguments\n",
    "\n",
    "    #txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "    txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "    # Set seed for all randomness sources\n",
    "\n",
    "    utils.seed(args.seed)\n",
    "\n",
    "    # Set device\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    txt_logger.info(f\"Device: {device}\\n\")\n",
    "\n",
    "\n",
    "    # LOAD ENVIRONMENTS AND INITIALIZE MODELS\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "    txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "    # Load training status\n",
    "\n",
    "    try:\n",
    "        status = utils.get_status(model_dir)\n",
    "    except OSError:\n",
    "        status = {\"num_frames\": 0, \"update\": 0}\n",
    "    txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "    # Load observations preprocessor\n",
    "\n",
    "    obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "    if \"vocab\" in status:\n",
    "        preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "    txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "    # Reshape reward function\n",
    "    if args.reshape_reward:\n",
    "        def reshape_reward(obs, action, reward, done):\n",
    "            if not done:\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 1\n",
    "            return reward\n",
    "    else:\n",
    "        reshape_reward = None\n",
    "\n",
    "    # Load model\n",
    "\n",
    "    acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    if \"model_state\" in status:\n",
    "        acmodel.load_state_dict(status[\"model_state\"])\n",
    "    acmodel.to(device)\n",
    "    txt_logger.info(\"Model loaded\\n\")\n",
    "    txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "    # Create hidden policies\n",
    "\n",
    "    acmodels = [acmodel]\n",
    "\n",
    "    for k in range(1, args.cascade_depth):\n",
    "        hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "        hidden_acmodel.to(device)\n",
    "        # Initialise all hidden policy networks to same as visible policy\n",
    "        if args.var_init == \"equal\":\n",
    "            hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "        elif args.var_init == \"saved\":\n",
    "            hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "        acmodels.append(hidden_acmodel)\n",
    "    txt_logger.info(\"Hidden models loaded\\n\")\n",
    "\n",
    "\n",
    "    # Load algo\n",
    "\n",
    "    if args.algo == \"a2c\":\n",
    "        algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "    elif args.algo == \"ppo\":\n",
    "        algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "    elif args.algo == \"ppopc\":\n",
    "        algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "    else:\n",
    "        raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "    algo.renew_optimizer()\n",
    "    # if \"optimizer_state\" in status:\n",
    "    #     algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "    # txt_logger.info(\"Optimizer loaded\\n\")\n",
    "\n",
    "    # TRAINING LOOP\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    num_frames = status[\"num_frames\"]\n",
    "    update = status[\"update\"]\n",
    "    start_time = time.time()\n",
    "    #nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "    # Moving average parameters\n",
    "    threshold = 0.90\n",
    "    window = 20\n",
    "    rreturn_total = 0\n",
    "    i = 0\n",
    "\n",
    "    while num_frames < args.frames:\n",
    "\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        # Evaluate decay lr and cliprange    \n",
    "        # frac = 1.0 - (update - 1.0) / nupdates\n",
    "        # algo.lrs = lrs(frac)\n",
    "        # algo.clipranges = clip_epss(frac)\n",
    "        # algo.kl_betas = None # Clipped-PPO, not full KL\n",
    "\n",
    "        # Collect experiences for cascade policies\n",
    "        exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "        # Update model parameters\n",
    "        logs2 = algo.update_parameters_cascade(exps)\n",
    "        logs = {**logs1, **logs2}\n",
    "        update_end_time = time.time()\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "        update += 1\n",
    "\n",
    "        # Print logs\n",
    "\n",
    "        if update % args.log_interval == 0:\n",
    "            fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "            duration = int(time.time() - start_time)\n",
    "            return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "            rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "            num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "            # Moving average to break loop if mean reward threshold reached\n",
    "            #rreturn_total +=rreturn_per_episode['mean']\n",
    "            if args.early_stop:\n",
    "                rreturn_total +=return_per_episode['mean']\n",
    "                i+=1\n",
    "                if i >= window:\n",
    "                    rreturn_mavg = rreturn_total / i\n",
    "                    if rreturn_mavg >= threshold:\n",
    "                        break_flag = True \n",
    "                        break\n",
    "                    else:\n",
    "                        i = 0\n",
    "                        rreturn_total = 0\n",
    "\n",
    "            header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "            data = [update, num_frames, fps, duration]\n",
    "            # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "            # data += rreturn_per_episode.values()\n",
    "            header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()        \n",
    "            header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "            data += num_frames_per_episode.values()\n",
    "            header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "            data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "            txt_logger.info(\n",
    "                \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "                .format(*data))\n",
    "\n",
    "            header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "\n",
    "            if status[\"num_frames\"] == 0:\n",
    "                csv_logger.writerow(header)\n",
    "            csv_logger.writerow(data)\n",
    "            csv_file.flush()\n",
    "\n",
    "            for field, value in zip(header, data):\n",
    "                tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "        # Save status\n",
    "\n",
    "        if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "            status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                      \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "            if hasattr(preprocess_obss, \"vocab\"):\n",
    "                status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "            if args.cascade_depth > 1:\n",
    "                status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "            utils.save_status(status, model_dir)\n",
    "            txt_logger.info(\"Status saved\")\n",
    "\n",
    "    print(\"Number of frames: \", num_frames)\n",
    "\n",
    "\n",
    "\n",
    "    # CONTINUE TRAINING 2nd ENVIRONMENT ----------------------\n",
    "\n",
    "    index = 1\n",
    "    env_id = tasks_sequence[index][1]\n",
    "    frames = args.frames + add_frames\n",
    "\n",
    "    ## Hyper-parameters\n",
    "    args.env = env_id\n",
    "    args.frames = int(frames)\n",
    "    args.var_init:'saved'\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "    txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "    # Load training status\n",
    "\n",
    "    try:\n",
    "        status = utils.get_status(model_dir)\n",
    "    except OSError:\n",
    "        status = {\"num_frames\": 0, \"update\": 0}\n",
    "    txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "    # Load observations preprocessor\n",
    "\n",
    "    obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "    if \"vocab\" in status:\n",
    "        preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "    txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "    # Reshape reward function\n",
    "    if args.reshape_reward:\n",
    "        def reshape_reward(obs, action, reward, done):\n",
    "            if not done:\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 1\n",
    "            return reward\n",
    "    else:\n",
    "        reshape_reward = None\n",
    "\n",
    "    # Load model\n",
    "\n",
    "    acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    if \"model_state\" in status:\n",
    "        acmodel.load_state_dict(status[\"model_state\"])\n",
    "    acmodel.to(device)\n",
    "    txt_logger.info(\"Model loaded\\n\")\n",
    "    txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "    # Create hidden policies\n",
    "\n",
    "    acmodels = [acmodel]\n",
    "\n",
    "    for k in range(1, args.cascade_depth):\n",
    "        hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "        hidden_acmodel.to(device)\n",
    "        # Initialise all hidden policy networks to same as visible policy\n",
    "        if args.var_init == \"equal\":\n",
    "            print(\"Hidden policies initialized as visible policy\")\n",
    "            hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "        elif args.var_init == \"saved\":\n",
    "            print(\"Hidden policies initialized with previous saved states\")\n",
    "            hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "        acmodels.append(hidden_acmodel)\n",
    "    txt_logger.info(\"Hidden models loaded\\n\")\n",
    "\n",
    "    # Load algo\n",
    "\n",
    "    if args.algo == \"a2c\":\n",
    "        algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "    elif args.algo == \"ppo\":\n",
    "        algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "    elif args.algo == \"ppopc\":\n",
    "        algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "    else:\n",
    "        raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "    algo.renew_optimizer()\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    num_frames = status[\"num_frames\"]\n",
    "    update = status[\"update\"]\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    # Moving average parameters\n",
    "    threshold = 0.90\n",
    "    window = 10\n",
    "    rreturn_total = 0\n",
    "    i = 0\n",
    "\n",
    "    while num_frames < args.frames:\n",
    "\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        # Collect experiences for cascade policies\n",
    "        exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "        # Update model parameters\n",
    "        logs2 = algo.update_parameters_cascade(exps)\n",
    "        logs = {**logs1, **logs2}\n",
    "        update_end_time = time.time()\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "        update += 1\n",
    "\n",
    "        # Print logs\n",
    "\n",
    "        if update % args.log_interval == 0:\n",
    "            fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "            duration = int(time.time() - start_time)\n",
    "            return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "            rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "            num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "            if args.early_stop:\n",
    "                rreturn_total +=return_per_episode['mean']\n",
    "                i+=1\n",
    "                if i >= window:\n",
    "                    rreturn_mavg = rreturn_total / i\n",
    "                    if rreturn_mavg >= threshold:\n",
    "                        break_flag = True \n",
    "                        break\n",
    "                    else:\n",
    "                        i = 0\n",
    "                        rreturn_total = 0\n",
    "\n",
    "            header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "            data = [update, num_frames, fps, duration]\n",
    "            header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "            header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "            data += num_frames_per_episode.values()\n",
    "            header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "            data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "            txt_logger.info(\n",
    "                \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "                .format(*data))\n",
    "\n",
    "            header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "\n",
    "            if status[\"num_frames\"] == 0:\n",
    "                csv_logger.writerow(header)\n",
    "            csv_logger.writerow(data)\n",
    "            csv_file.flush()\n",
    "\n",
    "            for field, value in zip(header, data):\n",
    "                tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "        # Save status\n",
    "\n",
    "        if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "            status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                      \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "            if hasattr(preprocess_obss, \"vocab\"):\n",
    "                status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "            if args.cascade_depth > 1:\n",
    "                status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "            utils.save_status(status, model_dir)\n",
    "            txt_logger.info(\"Status saved\")\n",
    "\n",
    "    print(\"Number of frames: \", num_frames)\n",
    "\n",
    "\n",
    "    # CONTINUE TRAINING 3rd ENVIRONMENT ----------------------\n",
    "\n",
    "    index = 2\n",
    "    env_id = tasks_sequence[index][1]\n",
    "    frames = args.frames + add_frames\n",
    "\n",
    "    ## Hyper-parameters\n",
    "    args.env = env_id\n",
    "    args.frames = int(frames)\n",
    "\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "    txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "    # Load training status\n",
    "\n",
    "    try:\n",
    "        status = utils.get_status(model_dir)\n",
    "    except OSError:\n",
    "        status = {\"num_frames\": 0, \"update\": 0}\n",
    "    txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "    # Load observations preprocessor\n",
    "\n",
    "    obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "    if \"vocab\" in status:\n",
    "        preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "    txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "    # Reshape reward function\n",
    "    if args.reshape_reward:\n",
    "        def reshape_reward(obs, action, reward, done):\n",
    "            if not done:\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 1\n",
    "            return reward\n",
    "    else:\n",
    "        reshape_reward = None\n",
    "\n",
    "    # Load model\n",
    "\n",
    "    acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    if \"model_state\" in status:\n",
    "        acmodel.load_state_dict(status[\"model_state\"])\n",
    "    acmodel.to(device)\n",
    "    txt_logger.info(\"Model loaded\\n\")\n",
    "    txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "    # Create hidden policies\n",
    "\n",
    "    acmodels = [acmodel]\n",
    "\n",
    "    for k in range(1, args.cascade_depth):\n",
    "        hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "        hidden_acmodel.to(device)\n",
    "        # Initialise all hidden policy networks to same as visible policy\n",
    "        if args.var_init == \"equal\":\n",
    "            print(\"Hidden policies initialized as visible policy\")\n",
    "            hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "        elif args.var_init == \"saved\":\n",
    "            print(\"Hidden policies initialized with previous saved states\")\n",
    "            hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "        acmodels.append(hidden_acmodel)\n",
    "    txt_logger.info(\"Hidden models loaded\\n\")\n",
    "\n",
    "    # Load algo\n",
    "\n",
    "    if args.algo == \"a2c\":\n",
    "        algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "    elif args.algo == \"ppo\":\n",
    "        algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "    elif args.algo == \"ppopc\":\n",
    "        algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "    else:\n",
    "        raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "    algo.renew_optimizer()\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    num_frames = status[\"num_frames\"]\n",
    "    update = status[\"update\"]\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    # Moving average parameters\n",
    "    threshold = 0.90\n",
    "    window = 10\n",
    "    rreturn_total = 0\n",
    "    i = 0\n",
    "\n",
    "    while num_frames < args.frames:\n",
    "\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        # Collect experiences for cascade policies\n",
    "        exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "        # Update model parameters\n",
    "        logs2 = algo.update_parameters_cascade(exps)\n",
    "        logs = {**logs1, **logs2}\n",
    "        update_end_time = time.time()\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "        update += 1\n",
    "\n",
    "        # Print logs\n",
    "\n",
    "        if update % args.log_interval == 0:\n",
    "            fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "            duration = int(time.time() - start_time)\n",
    "            return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "            rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "            num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "            if args.early_stop:\n",
    "                rreturn_total +=return_per_episode['mean']\n",
    "                i+=1\n",
    "                if i >= window:\n",
    "                    rreturn_mavg = rreturn_total / i\n",
    "                    if rreturn_mavg >= threshold:\n",
    "                        break_flag = True \n",
    "                        break\n",
    "                    else:\n",
    "                        i = 0\n",
    "                        rreturn_total = 0\n",
    "\n",
    "            header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "            data = [update, num_frames, fps, duration]\n",
    "            header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "            header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "            data += num_frames_per_episode.values()\n",
    "            header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "            data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "            txt_logger.info(\n",
    "                \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "                .format(*data))\n",
    "\n",
    "            header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "\n",
    "            if status[\"num_frames\"] == 0:\n",
    "                csv_logger.writerow(header)\n",
    "            csv_logger.writerow(data)\n",
    "            csv_file.flush()\n",
    "\n",
    "            for field, value in zip(header, data):\n",
    "                tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "        # Save status\n",
    "\n",
    "        if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "            status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                      \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "            if hasattr(preprocess_obss, \"vocab\"):\n",
    "                status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "            if args.cascade_depth > 1:\n",
    "                status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "            utils.save_status(status, model_dir)\n",
    "            txt_logger.info(\"Status saved\")\n",
    "\n",
    "    print(\"Number of frames: \", num_frames)\n",
    "\n",
    "    # CONTINUE TRAINING 4th ENVIRONMENT ----------------------\n",
    "\n",
    "    index = 3\n",
    "    env_id = tasks_sequence[index][1]\n",
    "    frames = args.frames + add_frames\n",
    "\n",
    "    ## Hyper-parameters\n",
    "    args.env = env_id\n",
    "    args.frames = int(frames)\n",
    "\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "    txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "    # Load training status\n",
    "\n",
    "    try:\n",
    "        status = utils.get_status(model_dir)\n",
    "    except OSError:\n",
    "        status = {\"num_frames\": 0, \"update\": 0}\n",
    "    txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "    # Load observations preprocessor\n",
    "\n",
    "    obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "    if \"vocab\" in status:\n",
    "        preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "    txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "    # Reshape reward function\n",
    "    if args.reshape_reward:\n",
    "        def reshape_reward(obs, action, reward, done):\n",
    "            if not done:\n",
    "                reward = -1\n",
    "            else:\n",
    "                reward = 1\n",
    "            return reward\n",
    "    else:\n",
    "        reshape_reward = None\n",
    "\n",
    "    # Load model\n",
    "\n",
    "    acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    if \"model_state\" in status:\n",
    "        acmodel.load_state_dict(status[\"model_state\"])\n",
    "    acmodel.to(device)\n",
    "    txt_logger.info(\"Model loaded\\n\")\n",
    "    txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "    # Create hidden policies\n",
    "\n",
    "    acmodels = [acmodel]\n",
    "\n",
    "    for k in range(1, args.cascade_depth):\n",
    "        hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "        hidden_acmodel.to(device)\n",
    "        # Initialise all hidden policy networks to same as visible policy\n",
    "        if args.var_init == \"equal\":\n",
    "            print(\"Hidden policies initialized as visible policy\")\n",
    "            hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "        elif args.var_init == \"saved\":\n",
    "            print(\"Hidden policies initialized with previous saved states\")\n",
    "            hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "        acmodels.append(hidden_acmodel)\n",
    "    txt_logger.info(\"Hidden models loaded\\n\")\n",
    "\n",
    "    # Load algo\n",
    "\n",
    "    if args.algo == \"a2c\":\n",
    "        algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "    elif args.algo == \"ppo\":\n",
    "        algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "    elif args.algo == \"ppopc\":\n",
    "        algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                                args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                                args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "    else:\n",
    "        raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "    algo.renew_optimizer()\n",
    "\n",
    "    # Train model\n",
    "\n",
    "    num_frames = status[\"num_frames\"]\n",
    "    update = status[\"update\"]\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    # Moving average parameters\n",
    "    threshold = 0.90\n",
    "    window = 10\n",
    "    rreturn_total = 0\n",
    "    i = 0\n",
    "\n",
    "    while num_frames < args.frames:\n",
    "\n",
    "        update_start_time = time.time()\n",
    "\n",
    "        # Collect experiences for cascade policies\n",
    "        exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "        # Update model parameters\n",
    "        logs2 = algo.update_parameters_cascade(exps)\n",
    "        logs = {**logs1, **logs2}\n",
    "        update_end_time = time.time()\n",
    "\n",
    "        num_frames += logs[\"num_frames\"]\n",
    "        update += 1\n",
    "\n",
    "        # Print logs\n",
    "\n",
    "        if update % args.log_interval == 0:\n",
    "            fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "            duration = int(time.time() - start_time)\n",
    "            return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "            rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "            num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "            if args.early_stop:\n",
    "                rreturn_total +=return_per_episode['mean']\n",
    "                i+=1\n",
    "                if i >= window:\n",
    "                    rreturn_mavg = rreturn_total / i\n",
    "                    if rreturn_mavg >= threshold:\n",
    "                        break_flag = True \n",
    "                        break\n",
    "                    else:\n",
    "                        i = 0\n",
    "                        rreturn_total = 0\n",
    "\n",
    "            header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "            data = [update, num_frames, fps, duration]\n",
    "            header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "            header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "            data += num_frames_per_episode.values()\n",
    "            header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "            data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "            txt_logger.info(\n",
    "                \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "                .format(*data))\n",
    "\n",
    "            header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "            data += return_per_episode.values()\n",
    "\n",
    "            if status[\"num_frames\"] == 0:\n",
    "                csv_logger.writerow(header)\n",
    "            csv_logger.writerow(data)\n",
    "            csv_file.flush()\n",
    "\n",
    "            for field, value in zip(header, data):\n",
    "                tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "        # Save status\n",
    "\n",
    "        if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "            status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                      \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "            if hasattr(preprocess_obss, \"vocab\"):\n",
    "                status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "            if args.cascade_depth > 1:\n",
    "                status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "            utils.save_status(status, model_dir)\n",
    "            txt_logger.info(\"Status saved\")\n",
    "\n",
    "    print(\"Number of frames: \", num_frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden policies initialized as visible policy\n",
      "Hidden policies initialized as visible policy\n",
      "Hidden policies initialized as visible policy\n",
      "Hidden models loaded\n",
      "\n",
      "U 181 | F 1482752 | FPS 1849 | D 4 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V -0.805 | pL -2.521 | vL 6.025 | ∇ 0.035\n",
      "U 182 | F 1490944 | FPS 2202 | D 8 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V -0.055 | pL -0.802 | vL 1.129 | ∇ 0.009\n",
      "U 183 | F 1499136 | FPS 2259 | D 11 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.204 | pL -0.301 | vL 0.146 | ∇ 0.010\n",
      "U 184 | F 1507328 | FPS 2122 | D 15 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.234 | pL -0.053 | vL 0.001 | ∇ 0.173\n",
      "U 185 | F 1515520 | FPS 1899 | D 19 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.246 | pL 0.003 | vL 0.000 | ∇ 0.104\n",
      "U 186 | F 1523712 | FPS 1859 | D 24 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.249 | pL 0.031 | vL 0.000 | ∇ 0.154\n",
      "U 187 | F 1531904 | FPS 1896 | D 28 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.246 | pL 0.021 | vL 0.000 | ∇ 0.157\n",
      "U 188 | F 1540096 | FPS 1706 | D 33 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.242 | pL 0.021 | vL 0.000 | ∇ 0.161\n",
      "U 189 | F 1548288 | FPS 1904 | D 37 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.257 | pL 0.019 | vL 0.001 | ∇ 0.179\n",
      "U 190 | F 1556480 | FPS 1860 | D 42 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.255 | pL 0.015 | vL 0.000 | ∇ 0.162\n",
      "Status saved\n",
      "U 191 | F 1564672 | FPS 1883 | D 46 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.236 | pL 0.025 | vL 0.000 | ∇ 0.157\n",
      "U 192 | F 1572864 | FPS 1912 | D 50 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.251 | pL 0.026 | vL 0.000 | ∇ 0.155\n",
      "U 193 | F 1581056 | FPS 1854 | D 55 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.270 | pL 0.026 | vL 0.000 | ∇ 0.157\n",
      "U 194 | F 1589248 | FPS 1876 | D 59 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.254 | pL 0.022 | vL 0.000 | ∇ 0.154\n",
      "U 195 | F 1597440 | FPS 1866 | D 64 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.263 | pL 0.022 | vL 0.000 | ∇ 0.150\n",
      "U 196 | F 1605632 | FPS 1886 | D 68 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.242 | pL 0.020 | vL 0.000 | ∇ 0.164\n",
      "U 197 | F 1613824 | FPS 1859 | D 72 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.245 | pL 0.020 | vL 0.000 | ∇ 0.163\n",
      "U 198 | F 1622016 | FPS 1891 | D 77 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.232 | pL 0.020 | vL 0.000 | ∇ 0.153\n",
      "U 199 | F 1630208 | FPS 1868 | D 81 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.226 | pL 0.026 | vL 0.000 | ∇ 0.161\n",
      "U 200 | F 1638400 | FPS 1879 | D 85 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.236 | pL 0.020 | vL 0.000 | ∇ 0.166\n",
      "Status saved\n",
      "U 201 | F 1646592 | FPS 1712 | D 90 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.255 | pL 0.024 | vL 0.000 | ∇ 0.169\n",
      "U 202 | F 1654784 | FPS 1862 | D 95 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.253 | pL 0.024 | vL 0.000 | ∇ 0.150\n",
      "U 203 | F 1662976 | FPS 1874 | D 99 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.242 | pL 0.024 | vL 0.000 | ∇ 0.157\n",
      "U 204 | F 1671168 | FPS 1861 | D 103 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.254 | pL 0.018 | vL 0.001 | ∇ 0.172\n",
      "U 205 | F 1679360 | FPS 1872 | D 108 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.251 | pL 0.019 | vL 0.001 | ∇ 0.170\n",
      "U 206 | F 1687552 | FPS 1677 | D 113 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.241 | pL 0.022 | vL 0.000 | ∇ 0.147\n",
      "U 207 | F 1695744 | FPS 1586 | D 118 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.249 | pL 0.027 | vL 0.000 | ∇ 0.159\n",
      "U 208 | F 1703936 | FPS 1708 | D 123 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.235 | pL 0.022 | vL 0.000 | ∇ 0.160\n",
      "U 209 | F 1712128 | FPS 1889 | D 127 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.246 | pL 0.024 | vL 0.000 | ∇ 0.147\n",
      "U 210 | F 1720320 | FPS 1879 | D 131 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.232 | pL 0.022 | vL 0.001 | ∇ 0.170\n",
      "Status saved\n",
      "U 211 | F 1728512 | FPS 1850 | D 136 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.262 | pL 0.013 | vL 0.001 | ∇ 0.180\n",
      "U 212 | F 1736704 | FPS 1874 | D 140 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.228 | pL 0.017 | vL 0.001 | ∇ 0.170\n",
      "U 213 | F 1744896 | FPS 1880 | D 145 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.241 | pL 0.018 | vL 0.001 | ∇ 0.169\n",
      "U 214 | F 1753088 | FPS 1712 | D 149 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.250 | pL 0.019 | vL 0.000 | ∇ 0.166\n",
      "U 215 | F 1761280 | FPS 1863 | D 154 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.234 | pL 0.020 | vL 0.000 | ∇ 0.146\n",
      "U 216 | F 1769472 | FPS 1854 | D 158 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.255 | pL 0.023 | vL 0.000 | ∇ 0.156\n",
      "U 217 | F 1777664 | FPS 1842 | D 163 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.241 | pL 0.017 | vL 0.000 | ∇ 0.165\n",
      "U 218 | F 1785856 | FPS 1846 | D 167 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.252 | pL 0.022 | vL 0.000 | ∇ 0.163\n",
      "U 219 | F 1794048 | FPS 1832 | D 172 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.249 | pL 0.022 | vL 0.000 | ∇ 0.165\n",
      "U 220 | F 1802240 | FPS 1868 | D 176 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.232 | pL 0.023 | vL 0.000 | ∇ 0.157\n",
      "Status saved\n",
      "U 221 | F 1810432 | FPS 1875 | D 180 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.236 | pL 0.025 | vL 0.001 | ∇ 0.171\n",
      "U 222 | F 1818624 | FPS 1873 | D 185 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.221 | pL 0.021 | vL 0.000 | ∇ 0.147\n",
      "U 223 | F 1826816 | FPS 1851 | D 189 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.236 | pL 0.023 | vL 0.000 | ∇ 0.159\n",
      "U 224 | F 1835008 | FPS 1888 | D 193 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.242 | pL 0.016 | vL 0.001 | ∇ 0.171\n",
      "U 225 | F 1843200 | FPS 1842 | D 198 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.229 | pL 0.020 | vL 0.000 | ∇ 0.170\n",
      "U 226 | F 1851392 | FPS 1895 | D 202 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.262 | pL 0.021 | vL 0.001 | ∇ 0.170\n",
      "U 227 | F 1859584 | FPS 1861 | D 207 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.265 | pL 0.023 | vL 0.000 | ∇ 0.136\n",
      "U 228 | F 1867776 | FPS 1690 | D 212 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.251 | pL 0.025 | vL 0.000 | ∇ 0.157\n",
      "U 229 | F 1875968 | FPS 1991 | D 216 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.254 | pL 0.022 | vL 0.000 | ∇ 0.167\n",
      "U 230 | F 1884160 | FPS 1869 | D 220 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.253 | pL 0.025 | vL 0.000 | ∇ 0.133\n",
      "Status saved\n",
      "U 231 | F 1892352 | FPS 1841 | D 224 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.251 | pL 0.023 | vL 0.000 | ∇ 0.157\n",
      "U 232 | F 1900544 | FPS 1864 | D 229 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.245 | pL 0.021 | vL 0.001 | ∇ 0.172\n",
      "U 233 | F 1908736 | FPS 1826 | D 233 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.253 | pL 0.018 | vL 0.000 | ∇ 0.163\n",
      "U 234 | F 1916928 | FPS 1693 | D 238 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.238 | pL 0.021 | vL 0.000 | ∇ 0.165\n",
      "U 235 | F 1925120 | FPS 1599 | D 243 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.244 | pL 0.022 | vL 0.000 | ∇ 0.157\n",
      "U 236 | F 1933312 | FPS 1838 | D 248 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.248 | pL 0.024 | vL 0.000 | ∇ 0.155\n",
      "U 237 | F 1941504 | FPS 1852 | D 252 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.265 | pL 0.025 | vL 0.000 | ∇ 0.156\n",
      "U 238 | F 1949696 | FPS 1853 | D 257 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.236 | pL 0.017 | vL 0.000 | ∇ 0.161\n",
      "U 239 | F 1957888 | FPS 1840 | D 261 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.252 | pL 0.019 | vL 0.000 | ∇ 0.159\n",
      "U 240 | F 1966080 | FPS 1870 | D 266 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.244 | pL 0.020 | vL 0.000 | ∇ 0.160\n",
      "Status saved\n",
      "U 241 | F 1974272 | FPS 1699 | D 270 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.233 | pL 0.022 | vL 0.000 | ∇ 0.166\n",
      "U 242 | F 1982464 | FPS 1869 | D 275 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.247 | pL 0.021 | vL 0.000 | ∇ 0.159\n",
      "U 243 | F 1990656 | FPS 1859 | D 279 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.236 | pL 0.020 | vL 0.000 | ∇ 0.165\n",
      "U 244 | F 1998848 | FPS 1857 | D 284 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.252 | pL 0.022 | vL 0.000 | ∇ 0.159\n",
      "U 245 | F 2007040 | FPS 1843 | D 288 | rR:μσmM 0.00 0.00 0.00 0.00 | F:μσmM 2.0 1.0 1.0 4.0 | H 0.000 | V 0.254 | pL 0.018 | vL 0.000 | ∇ 0.167\n",
      "Number of frames:  2007040\n"
     ]
    }
   ],
   "source": [
    "# CONTINUE TRAINING 4th ENVIRONMENT ----------------------\n",
    "\n",
    "index = 3\n",
    "#env_id = tasks_sequence[index][1]\n",
    "env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "frames = args.frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.frames = int(frames)\n",
    "\n",
    "\n",
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")\n",
    "\n",
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "algo.renew_optimizer()\n",
    "\n",
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        \n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PARAMETERS\n",
    "\n",
    "from torch_ac.utils.penv import ParallelEnv\n",
    "\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 1\n",
    "# Deterministic actions for evaluation, no exploit, no need of randomness for exploration\n",
    "args.argmax = True \n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "\n",
    "# SET ENVIROMENTS, AGENT AND LOGS. RUN EVALUATION\n",
    "\n",
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [1]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    #model_dir = utils.get_model_dir(args.model)\n",
    "    model_dir = utils.get_model_dir(model_name)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "#print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "#      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.mean(num_frames_tot, axis=0)[0], np.mean(fps_tot, axis=0)[0], np.mean(duration_tot, axis=0)[0], *np.mean(return_per_episode_tot, axis=0), *np.mean(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array2gif\n",
    "from pathlib import Path\n",
    "import numpy\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'env':args.env,\n",
    "'model':args.model,\n",
    "'seed':15,\n",
    "'shift':0,\n",
    "'argmax':True,\n",
    "'pause':0.1,\n",
    "'gif':args.model,\n",
    "'episodes':5\n",
    "}\n",
    "\n",
    "args = DictList(args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment, agent and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Load environment\n",
    "\n",
    "env = utils.make_env(args.env, args.seed)\n",
    "for _ in range(args.shift):\n",
    "    env.reset()\n",
    "print(\"Environment loaded\\n\")\n",
    "\n",
    "# Load agent\n",
    "\n",
    "model_dir = utils.get_model_dir(args.model)\n",
    "agent = utils.Agent(env.observation_space, env.action_space, model_dir, device, args.argmax)\n",
    "\n",
    "print(\"Agent loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run the agent\n",
    "\n",
    "if args.gif:\n",
    "   from array2gif import write_gif\n",
    "   frames = []\n",
    "\n",
    "# Create a window to view the environment\n",
    "env.render('human')\n",
    "\n",
    "for episode in range(args.episodes):\n",
    "    obs = env.reset()\n",
    "    done2 = False\n",
    "    while True:\n",
    "        env.render('human')\n",
    "        if args.gif:\n",
    "            frames.append(numpy.moveaxis(env.render(\"rgb_array\"), 2, 0))\n",
    "            \n",
    "\n",
    "        action = agent.get_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        agent.analyze_feedback(reward, done)\n",
    "        \n",
    "        if done or env.window.closed:\n",
    "            if episode == 4:\n",
    "                done2 = True\n",
    "            break\n",
    "    if done2 == True:\n",
    "        env.close()\n",
    "        break\n",
    "    #if env.window.closed:\n",
    "    #    break\n",
    "print('doneeee')\n",
    "if args.gif:\n",
    "    print(\"Saving gif... \", end=\"\")\n",
    "    utils.create_folders_if_necessary(\"./animation\")\n",
    "    #Path(\"./animation\").mkdir(parents=True, exist_ok=True)\n",
    "    write_gif(numpy.array(frames), \"./animation/\"+args.gif+\".gif\")\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_animation(args.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = wrap_env(env)\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(observation)\n",
    "    observation, reward, done, info = test_env.step(action)\n",
    "    episode_reward += reward\n",
    "    episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 2nd environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 3rd environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "#model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "\n",
    "add_frames = 1e6\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'early_stop':False,\n",
    "'seed':123456,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':int(frames), # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':2.5e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':False\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "# default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "# model_name = args.model or default_model_name\n",
    "# model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# # Load loggers and Tensorboard writer\n",
    "\n",
    "# txt_logger = utils.get_txt_logger(model_dir)\n",
    "# csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "# tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# # Log command and all script arguments\n",
    "\n",
    "# #txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "# txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# # Set seed for all randomness sources\n",
    "\n",
    "# utils.seed(args.seed)\n",
    "\n",
    "# # Set device\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "algo.renew_optimizer()\n",
    "# if \"optimizer_state\" in status:\n",
    "#     algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "# txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 4th environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "#model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "\n",
    "add_frames = 1e6\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'early_stop':False,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':int(frames), # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':2.5e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':False\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "# default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "# model_name = args.model or default_model_name\n",
    "# model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# # Load loggers and Tensorboard writer\n",
    "\n",
    "# txt_logger = utils.get_txt_logger(model_dir)\n",
    "# csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "# tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# # Log command and all script arguments\n",
    "\n",
    "# #txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "# txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# # Set seed for all randomness sources\n",
    "\n",
    "# utils.seed(args.seed)\n",
    "\n",
    "# # Set device\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "algo.renew_optimizer()\n",
    "# if \"optimizer_state\" in status:\n",
    "#     algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "# txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        if args.early_stop:\n",
    "            rreturn_total +=return_per_episode['mean']\n",
    "            i+=1\n",
    "            if i >= window:\n",
    "                rreturn_mavg = rreturn_total / i\n",
    "                if rreturn_mavg >= threshold:\n",
    "                    break_flag = True \n",
    "                    break\n",
    "                else:\n",
    "                    i = 0\n",
    "                    rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate 1st environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 123456\n",
    "args.argmax = True # Deterministic for evaluation\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [123456]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate  2nd environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 123456\n",
    "args.argmax = True # Deterministic for evaluation\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [123456]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate  3rd environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 123456\n",
    "args.argmax = True # Deterministic for evaluation\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [123456]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate  4th environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "\n",
    "#args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 123456\n",
    "args.argmax = True # Deterministic for evaluation\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [123456]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "5d2efec84aee61a766032e9dfbe418d90107ced57033c9077d1ba6267f248fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
