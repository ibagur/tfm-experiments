{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNF5-qJ7B0Q3"
   },
   "source": [
    "# MiniGrid settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdM2FnEJB0Q8"
   },
   "source": [
    "## Basic Jupyter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647123362972,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "aycUmr6OB0Q8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef0DdE0b4pLd"
   },
   "source": [
    "## Initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLM4YYcL5rBt"
   },
   "source": [
    "Import libraries and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YgenDMtf4pLe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigo/.local/share/virtualenvs/tfm-experiments-K5nk3NK1/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "# import stable_baselines3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint \n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28U_WEp25rBu"
   },
   "source": [
    "Minigrid helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d7eCH8Kf4pLf"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "        \n",
    "def show_animation(experiment):\n",
    "    giflist = glob.glob('animation/*.gif')\n",
    "    if len(giflist) > 0:\n",
    "        matching = [s for s in giflist if experiment in s]\n",
    "        gif_path = matching[0]\n",
    "        b64 = base64.b64encode(open(gif_path,'rb').read()).decode('ascii')\n",
    "        display.display(HTML(f'<img src=\"data:image/gif;base64,{b64}\" height=\"400\" />'))\n",
    "    else:\n",
    "        print(\"Could not find animation\")\n",
    "\n",
    "## Define rendering wrappers\n",
    "\n",
    "# Define wrapper for CNN Policy\n",
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))\n",
    "\n",
    "def gen_wrapped_env_cnn(env_name):\n",
    "    return wrap_env(ImgObsWrapper(RGBImgPartialObsWrapper(gym.make(env_name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KchGuXpd5rBv"
   },
   "source": [
    "Define the rendering wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Gdhk3Oep4pLf"
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Define wrapper for CNN Policy\n",
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))\n",
    "\n",
    "def gen_wrapped_env_cnn(env_name):\n",
    "    return wrap_env(ImgObsWrapper(RGBImgPartialObsWrapper(gym.make(env_name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render an environment image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1647083269049,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "B21JwGYn5rBv",
    "outputId": "54dfa526-2621-4f91-df2b-e4ff7a447aad"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import randint\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-16x16-v0'\n",
    "env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'BreakoutNoFrameskip-v4'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id ='MiniGrid-UnlockPickup-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "eval_env = gym.make(env_id)\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "#random_action = eval_env.action_space.sample()\n",
    "#new_obs, reward, done, info = eval_env.step(random_action)\n",
    "\n",
    "before_img = eval_env.render('rgb_array')\n",
    "\n",
    "plt.figure(figsize = (4.,4.))\n",
    "plt.imshow(before_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umY09KJP5rCI"
   },
   "source": [
    "# Policy Consolidation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPq1XkeL5rCI"
   },
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import base64\n",
    "import datetime\n",
    "import torch\n",
    "import torch_ac\n",
    "import tensorboardX\n",
    "import sys\n",
    "import utils\n",
    "from model import ACModel\n",
    "from torch_ac.utils import DictList, ParallelEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_envs(env_id, procs, seed=None):\n",
    "    envs = []\n",
    "    for i in range(procs):\n",
    "        if seed:\n",
    "            e = utils.make_env(env_id, seed + 10000 * i)\n",
    "        else:\n",
    "            e = utils.make_env(env_id)\n",
    "        envs.append(e)\n",
    "    env = ParallelEnv(envs)\n",
    "    return env\n",
    "\n",
    "# functions to calculate decays\n",
    "def constfn(val):\n",
    "    def f(_):\n",
    "        return val\n",
    "    return f\n",
    "\n",
    "def constfn_arr(val,length):\n",
    "    return [val for i in range(length)]\n",
    "\n",
    "def constfn_arr2(val,length):\n",
    "    def f(_):\n",
    "        return [val for i in range(length)]\n",
    "    return f\n",
    "\n",
    "def decayfn_arr_2(start, decay, length):\n",
    "    def f(start):\n",
    "        return [start*decay**i for i in range(length)]\n",
    "    return f\n",
    "\n",
    "def decayfn_arr(start, decay, length):\n",
    "    return [start*decay**i for i in range(length)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render Parallel environment snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "procs = 16\n",
    "max_tasks = 20\n",
    "seed_list = range(procs * max_tasks, (procs + 1) * max_tasks)\n",
    "\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N1-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N3-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS11N5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-LavaCrossingS9N2-v0'\n",
    "env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "seed = 1\n",
    "env = make_envs(env_id, procs, seed)\n",
    "obs = env.reset()\n",
    "\n",
    "im_list = []\n",
    "for e in env.envs:\n",
    "    #print(type(e.render('rgb_array')))\n",
    "    #e.reset()\n",
    "    im_list.append(e.render('rgb_array'))\n",
    "\n",
    "fig = plt.figure(figsize=(8., 8.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(4, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, im_list):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#model = 'MiniGrid-WallGapS6-v0_PPOPC_RMSProp_frames_1M_proc_16_cascade_2_nodecay_next_MiniGrid-DoorKey-6x6-v0_mid'\n",
    "model = 'test_cascade_8_frames_2048_wallgap_doorkey_clip_impsampling_reshaped'\n",
    "\n",
    "\n",
    "processes = 16\n",
    "frames = 3e6\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':frames, # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':3e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'equal',\n",
    "'reshape_reward':True\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'test_cascade_8_frames_2048_wallgap_doorkey_clip_impsampling_reshaped', 'seed': 1, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 3000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'equal', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x13d6f1b00>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05]}\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set run dir\n",
    "\n",
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "model_name = args.model or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "\n",
    "#txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environments, model, algo and prepare training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden models loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "if \"optimizer_state\" in status:\n",
    "    algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U 1 | F 032768 | FPS 0780 | D 41 | rR:μσmM 0.07 0.20 0.00 0.98 | F:μσmM 134.5 26.3 3.0 144.0 | H 1.899 | V -0.504 | pL 32.177 | vL 1218.999 | ∇ 7.627\n",
      "U 2 | F 065536 | FPS 0830 | D 81 | rR:μσmM 0.05 0.15 0.00 0.89 | F:μσmM 137.8 19.1 17.0 144.0 | H 1.908 | V -0.902 | pL 31.941 | vL 1196.543 | ∇ 7.345\n",
      "U 3 | F 098304 | FPS 0837 | D 120 | rR:μσmM 0.14 0.26 0.00 0.94 | F:μσmM 126.1 35.7 10.0 144.0 | H 1.898 | V -1.217 | pL 31.111 | vL 1152.782 | ∇ 7.829\n",
      "U 4 | F 131072 | FPS 0839 | D 159 | rR:μσmM 0.16 0.29 0.00 0.98 | F:μσmM 123.1 39.6 3.0 144.0 | H 1.880 | V -1.685 | pL 30.703 | vL 1130.470 | ∇ 7.690\n",
      "U 5 | F 163840 | FPS 0841 | D 198 | rR:μσmM 0.16 0.29 0.00 0.98 | F:μσmM 122.4 40.1 4.0 144.0 | H 1.879 | V -2.116 | pL 30.254 | vL 1107.839 | ∇ 7.702\n",
      "U 6 | F 196608 | FPS 0839 | D 237 | rR:μσmM 0.27 0.34 0.00 0.98 | F:μσmM 108.3 47.7 3.0 144.0 | H 1.872 | V -2.522 | pL 29.049 | vL 1046.966 | ∇ 8.253\n",
      "U 7 | F 229376 | FPS 0838 | D 276 | rR:μσmM 0.35 0.36 0.00 0.98 | F:μσmM 97.6 50.8 3.0 144.0 | H 1.848 | V -3.089 | pL 27.756 | vL 980.012 | ∇ 8.819\n",
      "U 8 | F 262144 | FPS 0842 | D 315 | rR:μσmM 0.44 0.36 0.00 0.98 | F:μσmM 83.9 51.6 3.0 144.0 | H 1.818 | V -3.557 | pL 26.050 | vL 893.903 | ∇ 9.375\n",
      "U 9 | F 294912 | FPS 0805 | D 356 | rR:μσmM 0.56 0.35 0.00 0.99 | F:μσmM 67.6 50.1 2.0 144.0 | H 1.757 | V -3.985 | pL 23.582 | vL 777.501 | ∇ 11.162\n",
      "U 10 | F 327680 | FPS 0831 | D 395 | rR:μσmM 0.66 0.30 0.00 0.99 | F:μσmM 52.3 44.4 2.0 144.0 | H 1.669 | V -4.344 | pL 20.377 | vL 641.927 | ∇ 11.183\n",
      "Status saved\n",
      "U 11 | F 360448 | FPS 0844 | D 434 | rR:μσmM 0.76 0.22 0.00 0.99 | F:μσmM 38.3 34.1 2.0 144.0 | H 1.547 | V -4.569 | pL 16.179 | vL 460.743 | ∇ 11.337\n",
      "U 12 | F 393216 | FPS 0849 | D 473 | rR:μσmM 0.85 0.12 0.00 0.99 | F:μσmM 24.1 19.6 2.0 144.0 | H 1.325 | V -4.596 | pL 9.096 | vL 198.303 | ∇ 9.096\n",
      "U 13 | F 425984 | FPS 0841 | D 512 | rR:μσmM 0.90 0.07 0.26 0.99 | F:μσmM 16.8 11.1 2.0 119.0 | H 1.100 | V -4.530 | pL 4.575 | vL 73.303 | ∇ 6.175\n",
      "U 14 | F 458752 | FPS 0836 | D 551 | rR:μσmM 0.92 0.04 0.56 0.99 | F:μσmM 13.2 7.1 2.0 71.0 | H 0.880 | V -4.316 | pL 2.086 | vL 26.019 | ∇ 4.759\n",
      "U 15 | F 491520 | FPS 0840 | D 590 | rR:μσmM 0.93 0.03 0.56 0.99 | F:μσmM 11.3 5.5 2.0 71.0 | H 0.673 | V -4.173 | pL 0.922 | vL 13.476 | ∇ 3.714\n",
      "U 16 | F 524288 | FPS 0832 | D 629 | rR:μσmM 0.94 0.03 0.76 0.99 | F:μσmM 10.0 4.0 2.0 39.0 | H 0.469 | V -3.917 | pL 0.101 | vL 5.094 | ∇ 2.812\n",
      "U 17 | F 557056 | FPS 0835 | D 669 | rR:μσmM 0.94 0.02 0.83 0.99 | F:μσmM 9.1 3.3 2.0 27.0 | H 0.330 | V -3.479 | pL -0.008 | vL 2.279 | ∇ 2.247\n",
      "U 18 | F 589824 | FPS 0837 | D 708 | rR:μσmM 0.94 0.02 0.86 0.99 | F:μσmM 8.8 2.9 2.0 22.0 | H 0.249 | V -3.341 | pL -0.193 | vL 1.251 | ∇ 1.854\n",
      "U 19 | F 622592 | FPS 0830 | D 747 | rR:μσmM 0.95 0.02 0.87 0.99 | F:μσmM 8.6 2.7 2.0 21.0 | H 0.184 | V -3.158 | pL -0.011 | vL 0.817 | ∇ 1.727\n",
      "Number of frames:  655360\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Evaluate decay lr and cliprange    \n",
    "    # frac = 1.0 - (update - 1.0) / nupdates\n",
    "    # algo.lrs = lrs(frac)\n",
    "    # algo.clipranges = clip_epss(frac)\n",
    "    # algo.kl_betas = None # Clipped-PPO, not full KL\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        rreturn_total +=return_per_episode['mean']\n",
    "        i+=1\n",
    "        if i >= window:\n",
    "            rreturn_mavg = rreturn_total / i\n",
    "            if rreturn_mavg >= threshold:\n",
    "                break_flag = True \n",
    "                break\n",
    "            else:\n",
    "                i = 0\n",
    "                rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()        \n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'test_cascade_8_frames_2048_wallgap_doorkey_clip_impsampling_reshaped', 'seed': 2, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 3000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'equal', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x13d6f1b00>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_ac.utils.penv import ParallelEnv\n",
    "\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, Run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-WallGapS6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 3162.0 | FPS 4162 | D 0.0 | R:μσmM 0.78 0.22 0.00 0.98 | F:μσmM 35.3 34.0 3.0 144.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array2gif\n",
    "from pathlib import Path\n",
    "import numpy\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'env':args.env,\n",
    "'model':args.model,\n",
    "'seed':15,\n",
    "'shift':0,\n",
    "'argmax':False,\n",
    "'pause':0.1,\n",
    "'gif':args.model,\n",
    "'episodes':5\n",
    "}\n",
    "\n",
    "args = DictList(args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment, agent and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Load environment\n",
    "\n",
    "env = utils.make_env(args.env, args.seed)\n",
    "for _ in range(args.shift):\n",
    "    env.reset()\n",
    "print(\"Environment loaded\\n\")\n",
    "\n",
    "# Load agent\n",
    "\n",
    "model_dir = utils.get_model_dir(args.model)\n",
    "agent = utils.Agent(env.observation_space, env.action_space, model_dir, device, args.argmax)\n",
    "\n",
    "print(\"Agent loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run the agent\n",
    "\n",
    "if args.gif:\n",
    "   from array2gif import write_gif\n",
    "   frames = []\n",
    "\n",
    "# Create a window to view the environment\n",
    "env.render('human')\n",
    "\n",
    "for episode in range(args.episodes):\n",
    "    obs = env.reset()\n",
    "    done2 = False\n",
    "    while True:\n",
    "        env.render('human')\n",
    "        if args.gif:\n",
    "            frames.append(numpy.moveaxis(env.render(\"rgb_array\"), 2, 0))\n",
    "            \n",
    "\n",
    "        action = agent.get_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        agent.analyze_feedback(reward, done)\n",
    "        \n",
    "        if done or env.window.closed:\n",
    "            if episode == 4:\n",
    "                done2 = True\n",
    "            break\n",
    "    if done2 == True:\n",
    "        env.close()\n",
    "        break\n",
    "    #if env.window.closed:\n",
    "    #    break\n",
    "print('doneeee')\n",
    "if args.gif:\n",
    "    print(\"Saving gif... \", end=\"\")\n",
    "    utils.create_folders_if_necessary(\"./animation\")\n",
    "    #Path(\"./animation\").mkdir(parents=True, exist_ok=True)\n",
    "    write_gif(numpy.array(frames), \"./animation/\"+args.gif+\".gif\")\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_animation(args.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = wrap_env(env)\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(observation)\n",
    "    observation, reward, done, info = test_env.step(action)\n",
    "    episode_reward += reward\n",
    "    episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 2nd environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "\n",
    "add_frames = 3000000\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':frames, # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':3e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':True\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previous loggers and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_wallgap_doorkey_clip_impsampling_reshaped', 'seed': 1, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 6000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x13fd934d0>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05]}\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "model_name = args.model or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "\n",
    "#txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing environments, model and training status (TEST for CF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "envs = make_envs(args.env, args.procs, args.seed)\n",
    "\n",
    "algo.env = envs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing environments, model and training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden models loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "if \"optimizer_state\" in status:\n",
    "    algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        rreturn_total +=return_per_episode['mean']\n",
    "        i+=1\n",
    "        if i >= window:\n",
    "            rreturn_mavg = rreturn_total / i\n",
    "            if rreturn_mavg >= threshold:\n",
    "                break_flag = True \n",
    "                break\n",
    "            else:\n",
    "                i = 0\n",
    "                rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate 2nd environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_wallgap_doorkey_clip_impsampling_reshaped', 'seed': 2, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 18000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14216d830>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': 10}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "args.env = env_id\n",
    "\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = 10\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-DoorKey-6x6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 1700.0 | FPS 3327 | D 0.0 | R:μσmM 0.96 0.01 0.90 0.98 | F:μσmM 16.7 5.1 9.0 38.0\n",
      "\n",
      "10 worst episodes:\n",
      "- episode 32: R=0.9049999713897705, F=38.0\n",
      "- episode 84: R=0.9275000095367432, F=29.0\n",
      "- episode 34: R=0.9300000071525574, F=28.0\n",
      "- episode 58: R=0.9300000071525574, F=28.0\n",
      "- episode 99: R=0.9300000071525574, F=28.0\n",
      "- episode 45: R=0.9325000047683716, F=27.0\n",
      "- episode 90: R=0.9325000047683716, F=27.0\n",
      "- episode 74: R=0.9350000023841858, F=26.0\n",
      "- episode 50: R=0.9424999952316284, F=23.0\n",
      "- episode 15: R=0.9449999928474426, F=22.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate 1st environment and test CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'test_cascade_8_frames_2048_wallgap_doorkey_clip_impsampling_reshaped', 'seed': 3, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 18000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14216d830>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "args.model = 'test_cascade_8_frames_2048_wallgap_doorkey_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 3\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-WallGapS6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 7787.0 | FPS 3905 | D 1.0 | R:μσmM 0.57 0.43 0.00 0.99 | F:μσmM 63.2 61.9 2.0 144.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 3rd environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "model = 'test_cascade_8_frames_2048_wallgap_doorkey_crossing_clip_impsampling_reshaped'\n",
    "\n",
    "add_frames = 3000000\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':frames, # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':3e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':True\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-SimpleCrossingS9N2-v0', 'model': 'test_cascade_8_frames_2048_wallgap_doorkey_crossing_clip_impsampling_reshaped', 'seed': 1, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 21000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x143290440>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05]}\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "model_name = args.model or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "\n",
    "#txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden models loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "if \"optimizer_state\" in status:\n",
    "    algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U 71 | F 2326528 | FPS 0753 | D 43 | rR:μσmM 0.25 0.36 0.00 0.96 | F:μσmM 246.6 114.9 16.0 324.0 | H 1.126 | V -16.671 | pL 30.861 | vL 1179.229 | ∇ 49.842\n",
      "U 72 | F 2359296 | FPS 0797 | D 84 | rR:μσmM 0.24 0.33 0.00 0.92 | F:μσmM 252.3 103.6 28.0 324.0 | H 1.267 | V -17.141 | pL 29.590 | vL 1112.662 | ∇ 51.609\n",
      "U 73 | F 2392064 | FPS 0804 | D 125 | rR:μσmM 0.36 0.37 0.00 0.94 | F:μσmM 214.5 116.2 20.0 324.0 | H 1.363 | V -18.734 | pL 27.555 | vL 1007.222 | ∇ 45.136\n",
      "U 74 | F 2424832 | FPS 0797 | D 166 | rR:μσmM 0.50 0.36 0.00 0.96 | F:μσmM 168.9 117.0 16.0 324.0 | H 1.398 | V -19.400 | pL 24.590 | vL 871.701 | ∇ 39.995\n",
      "U 75 | F 2457600 | FPS 0810 | D 207 | rR:μσmM 0.47 0.37 0.00 0.96 | F:μσmM 179.1 118.7 16.0 324.0 | H 1.383 | V -19.402 | pL 25.216 | vL 903.613 | ∇ 37.694\n",
      "U 76 | F 2490368 | FPS 0814 | D 247 | rR:μσmM 0.44 0.36 0.00 0.94 | F:μσmM 189.2 115.7 20.0 324.0 | H 1.439 | V -19.822 | pL 25.719 | vL 928.034 | ∇ 35.048\n",
      "U 77 | F 2523136 | FPS 0808 | D 287 | rR:μσmM 0.50 0.34 0.00 0.96 | F:μσmM 171.6 110.4 13.0 324.0 | H 1.493 | V -20.738 | pL 23.630 | vL 820.610 | ∇ 31.831\n",
      "U 78 | F 2555904 | FPS 0806 | D 328 | rR:μσmM 0.54 0.33 0.00 0.93 | F:μσmM 159.3 106.2 24.0 324.0 | H 1.512 | V -21.176 | pL 22.566 | vL 779.099 | ∇ 33.041\n",
      "U 79 | F 2588672 | FPS 0801 | D 369 | rR:μσmM 0.47 0.36 0.00 0.93 | F:μσmM 178.9 113.4 24.0 324.0 | H 1.528 | V -21.496 | pL 23.692 | vL 832.920 | ∇ 30.823\n",
      "U 80 | F 2621440 | FPS 0792 | D 410 | rR:μσmM 0.51 0.35 0.00 0.94 | F:μσmM 168.8 111.2 22.0 324.0 | H 1.557 | V -21.990 | pL 23.389 | vL 812.354 | ∇ 31.319\n",
      "Status saved\n",
      "U 81 | F 2654208 | FPS 0813 | D 451 | rR:μσmM 0.52 0.33 0.00 0.95 | F:μσmM 165.6 107.1 17.0 324.0 | H 1.541 | V -22.240 | pL 22.130 | vL 758.357 | ∇ 28.679\n",
      "U 82 | F 2686976 | FPS 0778 | D 493 | rR:μσmM 0.57 0.32 0.00 0.96 | F:μσmM 146.8 104.3 15.0 324.0 | H 1.515 | V -22.537 | pL 20.926 | vL 716.978 | ∇ 31.097\n",
      "U 83 | F 2719744 | FPS 0831 | D 532 | rR:μσmM 0.58 0.34 0.00 0.96 | F:μσmM 143.4 108.9 16.0 324.0 | H 1.537 | V -22.785 | pL 20.849 | vL 719.745 | ∇ 28.945\n",
      "U 84 | F 2752512 | FPS 0826 | D 572 | rR:μσmM 0.54 0.35 0.00 0.96 | F:μσmM 157.8 113.9 13.0 324.0 | H 1.570 | V -23.197 | pL 21.438 | vL 738.779 | ∇ 30.028\n",
      "U 85 | F 2785280 | FPS 0831 | D 611 | rR:μσmM 0.60 0.32 0.00 0.96 | F:μσmM 137.8 105.0 14.0 324.0 | H 1.555 | V -23.265 | pL 19.775 | vL 681.609 | ∇ 34.625\n",
      "U 86 | F 2818048 | FPS 0832 | D 651 | rR:μσmM 0.64 0.29 0.00 0.96 | F:μσmM 125.8 96.1 14.0 324.0 | H 1.547 | V -23.383 | pL 18.481 | vL 624.851 | ∇ 32.257\n",
      "U 87 | F 2850816 | FPS 0832 | D 690 | rR:μσmM 0.67 0.29 0.00 0.96 | F:μσmM 114.5 95.7 16.0 324.0 | H 1.523 | V -23.520 | pL 16.919 | vL 581.896 | ∇ 32.120\n",
      "U 88 | F 2883584 | FPS 0829 | D 730 | rR:μσmM 0.70 0.27 0.00 0.96 | F:μσmM 105.1 91.1 15.0 324.0 | H 1.475 | V -23.637 | pL 16.256 | vL 568.487 | ∇ 35.944\n",
      "U 89 | F 2916352 | FPS 0829 | D 769 | rR:μσmM 0.76 0.24 0.00 0.96 | F:μσmM 83.5 78.7 14.0 324.0 | H 1.423 | V -23.550 | pL 13.087 | vL 475.348 | ∇ 34.884\n",
      "U 90 | F 2949120 | FPS 0832 | D 809 | rR:μσmM 0.75 0.22 0.00 0.96 | F:μσmM 87.2 74.8 14.0 324.0 | H 1.409 | V -23.857 | pL 12.455 | vL 445.680 | ∇ 34.004\n",
      "Status saved\n",
      "U 91 | F 2981888 | FPS 0828 | D 848 | rR:μσmM 0.78 0.22 0.00 0.96 | F:μσmM 78.8 74.7 13.0 324.0 | H 1.386 | V -23.996 | pL 11.975 | vL 450.340 | ∇ 33.930\n",
      "U 92 | F 3014656 | FPS 0830 | D 888 | rR:μσmM 0.80 0.20 0.00 0.96 | F:μσmM 71.2 68.5 13.0 324.0 | H 1.329 | V -23.835 | pL 9.659 | vL 388.691 | ∇ 34.165\n",
      "U 93 | F 3047424 | FPS 0833 | D 927 | rR:μσmM 0.83 0.19 0.00 0.96 | F:μσmM 62.3 63.8 13.0 324.0 | H 1.257 | V -23.639 | pL 8.828 | vL 381.209 | ∇ 35.059\n",
      "U 94 | F 3080192 | FPS 0831 | D 966 | rR:μσmM 0.86 0.14 0.00 0.96 | F:μσmM 51.4 50.4 13.0 324.0 | H 1.146 | V -23.127 | pL 5.124 | vL 288.116 | ∇ 34.201\n",
      "U 95 | F 3112960 | FPS 0833 | D 1006 | rR:μσmM 0.88 0.12 0.00 0.96 | F:μσmM 42.6 40.5 13.0 324.0 | H 1.067 | V -22.643 | pL 2.556 | vL 245.193 | ∇ 31.026\n",
      "U 96 | F 3145728 | FPS 0832 | D 1045 | rR:μσmM 0.89 0.09 0.00 0.96 | F:μσmM 38.0 33.5 13.0 324.0 | H 0.963 | V -21.739 | pL 0.095 | vL 186.197 | ∇ 29.362\n",
      "U 97 | F 3178496 | FPS 0829 | D 1085 | rR:μσmM 0.91 0.08 0.19 0.96 | F:μσmM 33.8 28.3 13.0 293.0 | H 0.849 | V -21.275 | pL -1.615 | vL 171.260 | ∇ 30.170\n",
      "U 98 | F 3211264 | FPS 0829 | D 1124 | rR:μσmM 0.91 0.07 0.31 0.96 | F:μσmM 31.5 24.7 13.0 250.0 | H 0.801 | V -20.901 | pL -2.291 | vL 162.309 | ∇ 28.950\n",
      "U 99 | F 3244032 | FPS 0831 | D 1164 | rR:μσmM 0.92 0.07 0.23 0.96 | F:μσmM 30.0 24.2 13.0 278.0 | H 0.728 | V -20.340 | pL -2.690 | vL 153.105 | ∇ 30.559\n",
      "U 100 | F 3276800 | FPS 0828 | D 1203 | rR:μσmM 0.93 0.05 0.35 0.96 | F:μσmM 25.6 16.8 13.0 233.0 | H 0.595 | V -19.137 | pL -4.864 | vL 129.000 | ∇ 26.891\n",
      "Status saved\n",
      "U 101 | F 3309568 | FPS 0833 | D 1243 | rR:μσmM 0.94 0.04 0.37 0.96 | F:μσmM 23.0 14.6 13.0 226.0 | H 0.476 | V -18.269 | pL -5.679 | vL 126.116 | ∇ 22.582\n",
      "U 102 | F 3342336 | FPS 0833 | D 1282 | rR:μσmM 0.94 0.04 0.51 0.96 | F:μσmM 21.9 12.7 13.0 178.0 | H 0.388 | V -17.550 | pL -6.009 | vL 115.945 | ∇ 20.572\n",
      "U 103 | F 3375104 | FPS 0830 | D 1321 | rR:μσmM 0.94 0.04 0.00 0.96 | F:μσmM 20.5 13.2 13.0 324.0 | H 0.319 | V -16.999 | pL -5.976 | vL 128.706 | ∇ 19.026\n",
      "U 104 | F 3407872 | FPS 0832 | D 1361 | rR:μσmM 0.95 0.03 0.00 0.96 | F:μσmM 19.4 10.7 13.0 324.0 | H 0.245 | V -15.892 | pL -6.841 | vL 102.911 | ∇ 14.383\n",
      "U 105 | F 3440640 | FPS 0832 | D 1400 | rR:μσmM 0.95 0.03 0.41 0.96 | F:μσmM 18.9 9.5 13.0 211.0 | H 0.184 | V -15.111 | pL -6.002 | vL 100.171 | ∇ 16.284\n",
      "U 106 | F 3473408 | FPS 0830 | D 1440 | rR:μσmM 0.95 0.02 0.74 0.96 | F:μσmM 17.9 6.1 13.0 94.0 | H 0.138 | V -14.370 | pL -6.467 | vL 90.479 | ∇ 11.616\n",
      "U 107 | F 3506176 | FPS 0831 | D 1479 | rR:μσmM 0.95 0.02 0.78 0.96 | F:μσmM 17.5 5.5 13.0 78.0 | H 0.086 | V -13.532 | pL -6.011 | vL 79.772 | ∇ 9.744\n",
      "U 108 | F 3538944 | FPS 0834 | D 1518 | rR:μσmM 0.95 0.02 0.71 0.96 | F:μσmM 17.4 6.2 13.0 105.0 | H 0.067 | V -12.909 | pL -5.400 | vL 73.376 | ∇ 10.854\n",
      "U 109 | F 3571712 | FPS 0829 | D 1558 | rR:μσmM 0.95 0.02 0.49 0.96 | F:μσmM 17.2 6.5 13.0 184.0 | H 0.049 | V -12.370 | pL -4.928 | vL 67.700 | ∇ 9.679\n",
      "Number of frames:  3604480\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        rreturn_total +=return_per_episode['mean']\n",
    "        i+=1\n",
    "        if i >= window:\n",
    "            rreturn_mavg = rreturn_total / i\n",
    "            if rreturn_mavg >= threshold:\n",
    "                break_flag = True \n",
    "                break\n",
    "            else:\n",
    "                i = 0\n",
    "                rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate 3rd environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-SimpleCrossingS9N2-v0', 'model': 'test_cascade_8_frames_2048_wallgap_doorkey_crossing_clip_impsampling_reshaped', 'seed': 2, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 21000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x143290440>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "args.model = 'test_cascade_8_frames_2048_wallgap_doorkey_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-SimpleCrossingS9N2-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 2161.0 | FPS 3609 | D 0.0 | R:μσmM 0.94 0.03 0.79 0.96 | F:μσmM 21.6 11.7 13.0 75.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate 1st environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'test_cascade_8_frames_2048_wallgap_doorkey_crossing_clip_impsampling_reshaped', 'seed': 3, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 21000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x143290440>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "args.model = 'test_cascade_8_frames_2048_wallgap_doorkey_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 3\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-WallGapS6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 947.0 | FPS 3915 | D 0.0 | R:μσmM 0.94 0.03 0.79 0.99 | F:μσmM 9.2 4.7 2.0 33.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate 2nd environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_wallgap_doorkey_crossing_clip_impsampling_reshaped', 'seed': 3, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 21000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x143290440>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "args.model = 'test_cascade_8_frames_2048_wallgap_doorkey_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 3\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-DoorKey-6x6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 11775.0 | FPS 4215 | D 2.0 | R:μσmM 0.72 0.30 0.00 0.97 | F:μσmM 109.0 112.7 11.0 360.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfm-experiments",
   "language": "python",
   "name": "tfm-experiments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "5d2efec84aee61a766032e9dfbe418d90107ced57033c9077d1ba6267f248fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
