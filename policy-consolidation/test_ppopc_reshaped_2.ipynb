{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNF5-qJ7B0Q3"
   },
   "source": [
    "# MiniGrid settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdM2FnEJB0Q8"
   },
   "source": [
    "## Basic Jupyter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1647123362972,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "aycUmr6OB0Q8",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef0DdE0b4pLd"
   },
   "source": [
    "## Initial set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLM4YYcL5rBt"
   },
   "source": [
    "Import libraries and classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YgenDMtf4pLe",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inigo/.local/share/virtualenvs/tfm-experiments-K5nk3NK1/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import base64\n",
    "# import stable_baselines3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint \n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28U_WEp25rBu"
   },
   "source": [
    "Minigrid helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d7eCH8Kf4pLf",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "from IPython import display \n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")\n",
    "        \n",
    "def show_animation(experiment):\n",
    "    giflist = glob.glob('animation/*.gif')\n",
    "    if len(giflist) > 0:\n",
    "        matching = [s for s in giflist if experiment in s]\n",
    "        gif_path = matching[0]\n",
    "        b64 = base64.b64encode(open(gif_path,'rb').read()).decode('ascii')\n",
    "        display.display(HTML(f'<img src=\"data:image/gif;base64,{b64}\" height=\"400\" />'))\n",
    "    else:\n",
    "        print(\"Could not find animation\")\n",
    "\n",
    "## Define rendering wrappers\n",
    "\n",
    "# Define wrapper for CNN Policy\n",
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))\n",
    "\n",
    "def gen_wrapped_env_cnn(env_name):\n",
    "    return wrap_env(ImgObsWrapper(RGBImgPartialObsWrapper(gym.make(env_name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KchGuXpd5rBv"
   },
   "source": [
    "Define the rendering wrappers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Gdhk3Oep4pLf",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Define wrapper for CNN Policy\n",
    "def ImgRGBImgPartialObsWrapper(env):\n",
    "    return ImgObsWrapper(RGBImgPartialObsWrapper(env))\n",
    "\n",
    "# Monitor is a gym wrapper, which helps easy rendering of videos of the wrapped environment.\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env\n",
    "\n",
    "def gen_wrapped_env(env_name):\n",
    "    return wrap_env(FlatObsWrapper(gym.make(env_name)))\n",
    "\n",
    "def gen_wrapped_env_cnn(env_name):\n",
    "    return wrap_env(ImgObsWrapper(RGBImgPartialObsWrapper(gym.make(env_name))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render an environment image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1647083269049,
     "user": {
      "displayName": "Iñigo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "14378798962183195551"
     },
     "user_tz": -60
    },
    "id": "B21JwGYn5rBv",
    "outputId": "54dfa526-2621-4f91-df2b-e4ff7a447aad",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-16x16-v0'\n",
    "env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'BreakoutNoFrameskip-v4'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "#env_id ='MiniGrid-UnlockPickup-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "eval_env = gym.make(env_id)\n",
    "eval_env.seed(10000+randint(0, 10))\n",
    "eval_env.reset()\n",
    "#random_action = eval_env.action_space.sample()\n",
    "#new_obs, reward, done, info = eval_env.step(random_action)\n",
    "\n",
    "before_img = eval_env.render('rgb_array')\n",
    "\n",
    "plt.figure(figsize = (4.,4.))\n",
    "plt.imshow(before_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umY09KJP5rCI"
   },
   "source": [
    "# Policy Consolidation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPq1XkeL5rCI"
   },
   "source": [
    "## Define the environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import base64\n",
    "import datetime\n",
    "import torch\n",
    "import torch_ac\n",
    "import tensorboardX\n",
    "import sys\n",
    "import utils\n",
    "from model import ACModel\n",
    "from torch_ac.utils import DictList, ParallelEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import Monitor\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import FlatObsWrapper, ImgObsWrapper, RGBImgPartialObsWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def make_envs(env_id, procs, seed=None):\n",
    "    envs = []\n",
    "    for i in range(procs):\n",
    "        if seed:\n",
    "            e = utils.make_env(env_id, seed + 10000 * i)\n",
    "        else:\n",
    "            e = utils.make_env(env_id)\n",
    "        envs.append(e)\n",
    "    env = ParallelEnv(envs)\n",
    "    return env\n",
    "\n",
    "# functions to calculate decays\n",
    "def constfn(val):\n",
    "    def f(_):\n",
    "        return val\n",
    "    return f\n",
    "\n",
    "def constfn_arr(val,length):\n",
    "    return [val for i in range(length)]\n",
    "\n",
    "def constfn_arr2(val,length):\n",
    "    def f(_):\n",
    "        return [val for i in range(length)]\n",
    "    return f\n",
    "\n",
    "def decayfn_arr_2(start, decay, length):\n",
    "    def f(start):\n",
    "        return [start*decay**i for i in range(length)]\n",
    "    return f\n",
    "\n",
    "def decayfn_arr(start, decay, length):\n",
    "    return [start*decay**i for i in range(length)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render Parallel environment snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "procs = 16\n",
    "max_tasks = 20\n",
    "seed_list = range(procs * max_tasks, (procs + 1) * max_tasks)\n",
    "\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS6-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N1-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS9N3-v0'\n",
    "#env_id = 'MiniGrid-SimpleCrossingS11N5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-LavaCrossingS9N2-v0'\n",
    "env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "seed = 1\n",
    "env = make_envs(env_id, procs, seed)\n",
    "obs = env.reset()\n",
    "\n",
    "im_list = []\n",
    "for e in env.envs:\n",
    "    #print(type(e.render('rgb_array')))\n",
    "    #e.reset()\n",
    "    im_list.append(e.render('rgb_array'))\n",
    "\n",
    "fig = plt.figure(figsize=(8., 8.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(4, 4),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, im_list):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#model = 'MiniGrid-WallGapS6-v0_PPOPC_RMSProp_frames_1M_proc_16_cascade_2_nodecay_next_MiniGrid-DoorKey-6x6-v0_mid'\n",
    "model = 'test_cascade_8_frames_2048_doorkey_wallgap_clip_impsampling_reshaped'\n",
    "\n",
    "\n",
    "processes = 16\n",
    "frames = 3e6\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':frames, # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':3e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'equal',\n",
    "'reshape_reward':True\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_clip_impsampling_reshaped', 'seed': 1, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 3000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'equal', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x141fb6320>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05]}\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set run dir\n",
    "\n",
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "model_name = args.model or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "\n",
    "#txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load environments, model, algo and prepare training settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden models loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "if \"optimizer_state\" in status:\n",
    "    algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U 1 | F 032768 | FPS 0777 | D 42 | rR:μσmM 0.02 0.08 0.00 0.57 | F:μσmM 354.7 26.0 173.0 360.0 | H 1.922 | V -0.537 | pL 42.411 | vL 1935.689 | ∇ 7.424\n",
      "U 2 | F 065536 | FPS 0760 | D 85 | rR:μσmM 0.01 0.08 0.00 0.79 | F:μσmM 356.4 28.7 86.0 360.0 | H 1.916 | V -0.949 | pL 41.384 | vL 1861.245 | ∇ 7.539\n",
      "U 3 | F 098304 | FPS 0801 | D 126 | rR:μσmM 0.01 0.04 0.00 0.23 | F:μσmM 358.4 8.0 309.0 360.0 | H 1.912 | V -1.506 | pL 41.461 | vL 1866.256 | ∇ 8.376\n",
      "U 4 | F 131072 | FPS 0794 | D 167 | rR:μσmM 0.04 0.14 0.00 0.63 | F:μσmM 347.0 44.4 147.0 360.0 | H 1.911 | V -2.012 | pL 41.228 | vL 1849.421 | ∇ 8.839\n",
      "U 5 | F 163840 | FPS 0797 | D 208 | rR:μσmM 0.02 0.09 0.00 0.51 | F:μσmM 354.4 27.4 195.0 360.0 | H 1.916 | V -2.494 | pL 40.524 | vL 1802.136 | ∇ 9.255\n",
      "U 6 | F 196608 | FPS 0803 | D 249 | rR:μσmM 0.01 0.07 0.00 0.54 | F:μσmM 356.3 21.6 185.0 360.0 | H 1.923 | V -3.057 | pL 40.544 | vL 1793.753 | ∇ 8.200\n",
      "U 7 | F 229376 | FPS 0801 | D 290 | rR:μσmM 0.02 0.11 0.00 0.78 | F:μσmM 352.6 36.1 88.0 360.0 | H 1.912 | V -3.630 | pL 40.338 | vL 1778.731 | ∇ 9.397\n",
      "U 8 | F 262144 | FPS 0801 | D 331 | rR:μσmM 0.02 0.07 0.00 0.38 | F:μσmM 355.8 17.8 246.0 360.0 | H 1.887 | V -4.257 | pL 39.745 | vL 1740.124 | ∇ 10.140\n",
      "U 9 | F 294912 | FPS 0799 | D 372 | rR:μσmM 0.05 0.16 0.00 0.93 | F:μσmM 345.3 53.0 29.0 360.0 | H 1.890 | V -4.811 | pL 39.368 | vL 1711.328 | ∇ 10.306\n",
      "U 10 | F 327680 | FPS 0795 | D 413 | rR:μσmM 0.01 0.08 0.00 0.64 | F:μσmM 356.5 24.2 144.0 360.0 | H 1.895 | V -5.229 | pL 39.303 | vL 1706.105 | ∇ 10.261\n",
      "Status saved\n",
      "U 11 | F 360448 | FPS 0801 | D 454 | rR:μσmM 0.03 0.12 0.00 0.73 | F:μσmM 350.7 39.1 108.0 360.0 | H 1.888 | V -5.647 | pL 38.898 | vL 1679.361 | ∇ 10.416\n",
      "U 12 | F 393216 | FPS 0797 | D 495 | rR:μσmM 0.02 0.09 0.00 0.55 | F:μσmM 354.9 27.7 182.0 360.0 | H 1.888 | V -6.222 | pL 38.869 | vL 1669.976 | ∇ 11.844\n",
      "U 13 | F 425984 | FPS 0801 | D 536 | rR:μσmM 0.04 0.14 0.00 0.69 | F:μσmM 348.2 47.8 125.0 360.0 | H 1.879 | V -6.729 | pL 38.229 | vL 1627.255 | ∇ 11.416\n",
      "U 14 | F 458752 | FPS 0797 | D 577 | rR:μσmM 0.01 0.03 0.00 0.18 | F:μσmM 359.2 4.7 327.0 360.0 | H 1.889 | V -7.071 | pL 38.375 | vL 1634.168 | ∇ 13.061\n",
      "U 15 | F 491520 | FPS 0796 | D 618 | rR:μσmM 0.06 0.17 0.00 0.87 | F:μσmM 342.5 56.0 52.0 360.0 | H 1.878 | V -7.567 | pL 37.732 | vL 1594.718 | ∇ 11.792\n",
      "U 16 | F 524288 | FPS 0798 | D 659 | rR:μσmM 0.01 0.07 0.00 0.57 | F:μσmM 356.1 23.1 173.0 360.0 | H 1.866 | V -7.989 | pL 37.516 | vL 1577.080 | ∇ 13.212\n",
      "U 17 | F 557056 | FPS 0806 | D 700 | rR:μσmM 0.02 0.09 0.00 0.56 | F:μσmM 353.9 28.8 178.0 360.0 | H 1.862 | V -8.358 | pL 37.422 | vL 1572.058 | ∇ 14.036\n",
      "U 18 | F 589824 | FPS 0843 | D 739 | rR:μσmM 0.03 0.13 0.00 0.79 | F:μσmM 350.6 45.6 83.0 360.0 | H 1.870 | V -8.835 | pL 37.283 | vL 1559.030 | ∇ 13.406\n",
      "U 19 | F 622592 | FPS 0836 | D 778 | rR:μσmM 0.06 0.17 0.00 0.76 | F:μσmM 341.5 56.2 96.0 360.0 | H 1.864 | V -9.078 | pL 36.684 | vL 1523.699 | ∇ 13.788\n",
      "U 20 | F 655360 | FPS 0834 | D 817 | rR:μσmM 0.04 0.13 0.00 0.86 | F:μσmM 349.1 43.9 57.0 360.0 | H 1.859 | V -9.437 | pL 36.809 | vL 1524.742 | ∇ 14.090\n",
      "Status saved\n",
      "U 21 | F 688128 | FPS 0831 | D 857 | rR:μσmM 0.06 0.16 0.00 0.83 | F:μσmM 342.9 53.8 69.0 360.0 | H 1.862 | V -9.832 | pL 36.382 | vL 1504.446 | ∇ 13.628\n",
      "U 22 | F 720896 | FPS 0841 | D 896 | rR:μσmM 0.08 0.19 0.00 0.93 | F:μσmM 335.9 60.9 30.0 360.0 | H 1.865 | V -10.162 | pL 36.452 | vL 1501.024 | ∇ 14.030\n",
      "U 23 | F 753664 | FPS 0839 | D 935 | rR:μσmM 0.08 0.18 0.00 0.88 | F:μσmM 337.6 59.6 49.0 360.0 | H 1.864 | V -10.732 | pL 35.701 | vL 1455.765 | ∇ 16.485\n",
      "U 24 | F 786432 | FPS 0836 | D 974 | rR:μσmM 0.09 0.21 0.00 0.87 | F:μσmM 331.0 72.2 52.0 360.0 | H 1.855 | V -11.229 | pL 35.143 | vL 1421.836 | ∇ 16.198\n",
      "U 25 | F 819200 | FPS 0844 | D 1013 | rR:μσmM 0.08 0.21 0.00 0.83 | F:μσmM 333.1 69.6 68.0 360.0 | H 1.868 | V -11.583 | pL 35.533 | vL 1439.413 | ∇ 15.134\n",
      "U 26 | F 851968 | FPS 0841 | D 1052 | rR:μσmM 0.07 0.18 0.00 0.80 | F:μσmM 339.5 58.5 82.0 360.0 | H 1.868 | V -11.923 | pL 34.920 | vL 1407.694 | ∇ 15.966\n",
      "U 27 | F 884736 | FPS 0844 | D 1091 | rR:μσmM 0.11 0.24 0.00 0.92 | F:μσmM 323.3 81.8 32.0 360.0 | H 1.861 | V -12.368 | pL 34.580 | vL 1383.391 | ∇ 17.011\n",
      "U 28 | F 917504 | FPS 0845 | D 1129 | rR:μσmM 0.10 0.22 0.00 0.88 | F:μσmM 327.9 73.2 47.0 360.0 | H 1.849 | V -13.028 | pL 34.319 | vL 1368.796 | ∇ 18.173\n",
      "U 29 | F 950272 | FPS 0836 | D 1169 | rR:μσmM 0.09 0.19 0.00 0.84 | F:μσmM 334.0 63.7 64.0 360.0 | H 1.852 | V -13.263 | pL 34.115 | vL 1354.309 | ∇ 19.807\n",
      "U 30 | F 983040 | FPS 0841 | D 1208 | rR:μσmM 0.14 0.26 0.00 0.89 | F:μσmM 314.3 88.8 43.0 360.0 | H 1.847 | V -13.513 | pL 33.724 | vL 1330.418 | ∇ 17.274\n",
      "Status saved\n",
      "U 31 | F 1015808 | FPS 0835 | D 1247 | rR:μσmM 0.18 0.28 0.00 0.88 | F:μσmM 302.0 96.0 47.0 360.0 | H 1.849 | V -13.898 | pL 33.147 | vL 1296.855 | ∇ 20.464\n",
      "U 32 | F 1048576 | FPS 0836 | D 1286 | rR:μσmM 0.25 0.31 0.00 0.92 | F:μσmM 278.8 106.6 34.0 360.0 | H 1.831 | V -14.204 | pL 32.389 | vL 1252.261 | ∇ 18.669\n",
      "U 33 | F 1081344 | FPS 0845 | D 1325 | rR:μσmM 0.30 0.35 0.00 0.94 | F:μσmM 258.0 121.3 26.0 360.0 | H 1.835 | V -14.446 | pL 31.613 | vL 1216.914 | ∇ 18.363\n",
      "U 34 | F 1114112 | FPS 0835 | D 1364 | rR:μσmM 0.31 0.30 0.00 0.94 | F:μσmM 261.5 103.4 22.0 360.0 | H 1.840 | V -14.911 | pL 31.431 | vL 1198.860 | ∇ 15.278\n",
      "U 35 | F 1146880 | FPS 0833 | D 1403 | rR:μσmM 0.35 0.33 0.00 0.93 | F:μσmM 243.6 115.5 30.0 360.0 | H 1.840 | V -15.401 | pL 30.445 | vL 1150.545 | ∇ 20.437\n",
      "U 36 | F 1179648 | FPS 0842 | D 1442 | rR:μσmM 0.41 0.35 0.00 0.96 | F:μσmM 222.7 126.3 17.0 360.0 | H 1.837 | V -15.790 | pL 30.009 | vL 1138.310 | ∇ 17.400\n",
      "U 37 | F 1212416 | FPS 0835 | D 1482 | rR:μσmM 0.46 0.33 0.00 0.94 | F:μσmM 205.5 118.2 25.0 360.0 | H 1.805 | V -16.128 | pL 28.164 | vL 1036.646 | ∇ 19.266\n",
      "U 38 | F 1245184 | FPS 0837 | D 1521 | rR:μσmM 0.61 0.31 0.00 0.98 | F:μσmM 149.9 112.3 8.0 360.0 | H 1.757 | V -16.435 | pL 25.346 | vL 914.932 | ∇ 17.866\n",
      "U 39 | F 1277952 | FPS 0838 | D 1560 | rR:μσmM 0.66 0.27 0.00 0.97 | F:μσmM 131.4 101.9 11.0 360.0 | H 1.731 | V -16.598 | pL 23.109 | vL 811.640 | ∇ 18.728\n",
      "U 40 | F 1310720 | FPS 0830 | D 1599 | rR:μσmM 0.75 0.23 0.00 0.97 | F:μσmM 98.3 87.9 11.0 360.0 | H 1.647 | V -16.737 | pL 20.061 | vL 695.442 | ∇ 20.458\n",
      "Status saved\n",
      "U 41 | F 1343488 | FPS 0833 | D 1639 | rR:μσmM 0.79 0.19 0.00 0.98 | F:μσmM 81.7 73.6 10.0 360.0 | H 1.628 | V -17.054 | pL 16.810 | vL 568.642 | ∇ 19.693\n",
      "U 42 | F 1376256 | FPS 0837 | D 1678 | rR:μσmM 0.85 0.13 0.00 0.98 | F:μσmM 59.5 52.1 9.0 360.0 | H 1.512 | V -16.789 | pL 10.994 | vL 368.279 | ∇ 20.431\n",
      "U 43 | F 1409024 | FPS 0827 | D 1717 | rR:μσmM 0.89 0.09 0.00 0.98 | F:μσmM 42.4 37.1 8.0 360.0 | H 1.384 | V -16.125 | pL 6.652 | vL 257.376 | ∇ 19.925\n",
      "U 44 | F 1441792 | FPS 0829 | D 1757 | rR:μσmM 0.92 0.06 0.48 0.98 | F:μσmM 30.6 22.4 9.0 207.0 | H 1.185 | V -15.252 | pL 1.202 | vL 115.344 | ∇ 17.362\n",
      "U 45 | F 1474560 | FPS 0834 | D 1796 | rR:μσmM 0.94 0.03 0.65 0.98 | F:μσmM 23.2 13.1 7.0 139.0 | H 0.957 | V -13.878 | pL -1.995 | vL 57.896 | ∇ 14.969\n",
      "U 46 | F 1507328 | FPS 0824 | D 1836 | rR:μσmM 0.95 0.02 0.77 0.98 | F:μσmM 19.5 9.1 8.0 90.0 | H 0.774 | V -12.491 | pL -3.263 | vL 38.152 | ∇ 11.526\n",
      "U 47 | F 1540096 | FPS 0832 | D 1875 | rR:μσmM 0.96 0.02 0.81 0.98 | F:μσmM 17.3 6.8 8.0 78.0 | H 0.605 | V -11.237 | pL -3.746 | vL 29.725 | ∇ 8.763\n",
      "U 48 | F 1572864 | FPS 0832 | D 1915 | rR:μσmM 0.96 0.01 0.84 0.98 | F:μσmM 15.6 5.7 7.0 63.0 | H 0.459 | V -9.871 | pL -3.379 | vL 24.355 | ∇ 7.732\n",
      "U 49 | F 1605632 | FPS 0825 | D 1955 | rR:μσmM 0.96 0.01 0.89 0.98 | F:μσmM 14.5 4.2 7.0 43.0 | H 0.326 | V -8.848 | pL -3.295 | vL 19.819 | ∇ 6.144\n",
      "U 50 | F 1638400 | FPS 0831 | D 1994 | rR:μσmM 0.97 0.01 0.88 0.98 | F:μσmM 13.8 3.7 7.0 48.0 | H 0.242 | V -7.977 | pL -2.823 | vL 16.520 | ∇ 4.958\n",
      "Status saved\n",
      "U 51 | F 1671168 | FPS 0829 | D 2034 | rR:μσmM 0.97 0.01 0.92 0.98 | F:μσmM 13.4 3.1 7.0 31.0 | H 0.177 | V -7.182 | pL -2.251 | vL 11.895 | ∇ 3.877\n",
      "U 52 | F 1703936 | FPS 0826 | D 2073 | rR:μσmM 0.97 0.01 0.93 0.98 | F:μσmM 13.2 2.9 7.0 30.0 | H 0.141 | V -6.711 | pL -1.871 | vL 8.718 | ∇ 3.498\n",
      "U 53 | F 1736704 | FPS 0830 | D 2113 | rR:μσmM 0.97 0.01 0.89 0.98 | F:μσmM 12.9 2.9 7.0 45.0 | H 0.098 | V -6.208 | pL -1.517 | vL 6.133 | ∇ 3.342\n",
      "U 54 | F 1769472 | FPS 0831 | D 2152 | rR:μσmM 0.97 0.02 0.00 0.98 | F:μσmM 13.0 7.4 7.0 360.0 | H 0.071 | V -5.884 | pL -0.925 | vL 24.502 | ∇ 3.158\n",
      "U 55 | F 1802240 | FPS 0826 | D 2192 | rR:μσmM 0.97 0.01 0.94 0.98 | F:μσmM 12.6 2.5 7.0 24.0 | H 0.042 | V -5.511 | pL -0.865 | vL 2.678 | ∇ 2.541\n",
      "U 56 | F 1835008 | FPS 0832 | D 2231 | rR:μσmM 0.97 0.01 0.94 0.98 | F:μσmM 12.6 2.4 7.0 22.0 | H 0.031 | V -5.314 | pL -0.585 | vL 1.712 | ∇ 2.473\n",
      "U 57 | F 1867776 | FPS 0832 | D 2271 | rR:μσmM 0.97 0.01 0.95 0.98 | F:μσmM 12.6 2.5 7.0 21.0 | H 0.024 | V -5.209 | pL -0.520 | vL 1.230 | ∇ 2.551\n",
      "U 58 | F 1900544 | FPS 0825 | D 2310 | rR:μσmM 0.97 0.01 0.94 0.98 | F:μσmM 12.7 2.4 7.0 24.0 | H 0.019 | V -5.083 | pL -0.222 | vL 0.809 | ∇ 2.512\n",
      "U 59 | F 1933312 | FPS 0833 | D 2350 | rR:μσmM 0.97 0.01 0.90 0.98 | F:μσmM 12.6 2.5 7.0 40.0 | H 0.018 | V -4.966 | pL -0.060 | vL 0.759 | ∇ 2.372\n",
      "Number of frames:  1966080\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 20\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Evaluate decay lr and cliprange    \n",
    "    # frac = 1.0 - (update - 1.0) / nupdates\n",
    "    # algo.lrs = lrs(frac)\n",
    "    # algo.clipranges = clip_epss(frac)\n",
    "    # algo.kl_betas = None # Clipped-PPO, not full KL\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        rreturn_total +=return_per_episode['mean']\n",
    "        i+=1\n",
    "        if i >= window:\n",
    "            rreturn_mavg = rreturn_total / i\n",
    "            if rreturn_mavg >= threshold:\n",
    "                break_flag = True \n",
    "                break\n",
    "            else:\n",
    "                i = 0\n",
    "                rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()        \n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_clip_impsampling_reshaped', 'seed': 2, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 3000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'equal', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x141fb6320>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_ac.utils.penv import ParallelEnv\n",
    "\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, Run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-DoorKey-6x6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 1323.0 | FPS 4388 | D 0.0 | R:μσmM 0.97 0.01 0.94 0.98 | F:μσmM 13.2 2.8 8.0 24.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import array2gif\n",
    "from pathlib import Path\n",
    "import numpy\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'env':args.env,\n",
    "'model':args.model,\n",
    "'seed':15,\n",
    "'shift':0,\n",
    "'argmax':False,\n",
    "'pause':0.1,\n",
    "'gif':args.model,\n",
    "'episodes':5\n",
    "}\n",
    "\n",
    "args = DictList(args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment, agent and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Load environment\n",
    "\n",
    "env = utils.make_env(args.env, args.seed)\n",
    "for _ in range(args.shift):\n",
    "    env.reset()\n",
    "print(\"Environment loaded\\n\")\n",
    "\n",
    "# Load agent\n",
    "\n",
    "model_dir = utils.get_model_dir(args.model)\n",
    "agent = utils.Agent(env.observation_space, env.action_space, model_dir, device, args.argmax)\n",
    "\n",
    "print(\"Agent loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run the agent\n",
    "\n",
    "if args.gif:\n",
    "   from array2gif import write_gif\n",
    "   frames = []\n",
    "\n",
    "# Create a window to view the environment\n",
    "env.render('human')\n",
    "\n",
    "for episode in range(args.episodes):\n",
    "    obs = env.reset()\n",
    "    done2 = False\n",
    "    while True:\n",
    "        env.render('human')\n",
    "        if args.gif:\n",
    "            frames.append(numpy.moveaxis(env.render(\"rgb_array\"), 2, 0))\n",
    "            \n",
    "\n",
    "        action = agent.get_action(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        agent.analyze_feedback(reward, done)\n",
    "        \n",
    "        if done or env.window.closed:\n",
    "            if episode == 4:\n",
    "                done2 = True\n",
    "            break\n",
    "    if done2 == True:\n",
    "        env.close()\n",
    "        break\n",
    "    #if env.window.closed:\n",
    "    #    break\n",
    "print('doneeee')\n",
    "if args.gif:\n",
    "    print(\"Saving gif... \", end=\"\")\n",
    "    utils.create_folders_if_necessary(\"./animation\")\n",
    "    #Path(\"./animation\").mkdir(parents=True, exist_ok=True)\n",
    "    write_gif(numpy.array(frames), \"./animation/\"+args.gif+\".gif\")\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "show_animation(args.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test_env = wrap_env(env)\n",
    "observation = test_env.reset()\n",
    "\n",
    "done = False\n",
    "episode_reward = 0\n",
    "episode_length = 0\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(observation)\n",
    "    observation, reward, done, info = test_env.step(action)\n",
    "    episode_reward += reward\n",
    "    episode_length += 1\n",
    "\n",
    "print('Total reward:', episode_reward)\n",
    "print('Total length:', episode_length)\n",
    "\n",
    "test_env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 2nd environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "\n",
    "add_frames = 3000000\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':frames, # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':3e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':True\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load previous loggers and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_clip_impsampling_reshaped', 'seed': 1, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 6000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14454d320>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05]}\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "model_name = args.model or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "\n",
    "#txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load existing environments, model and training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden models loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "if \"optimizer_state\" in status:\n",
    "    algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U 51 | F 1671168 | FPS 0839 | D 39 | rR:μσmM 0.24 0.41 0.00 0.99 | F:μσmM 109.7 58.6 2.0 144.0 | H 0.532 | V -7.241 | pL 27.431 | vL 995.653 | ∇ 121.329\n",
      "U 52 | F 1703936 | FPS 0843 | D 77 | rR:μσmM 0.29 0.42 0.00 0.98 | F:μσmM 103.0 60.0 3.0 144.0 | H 1.031 | V -9.197 | pL 24.177 | vL 901.324 | ∇ 70.773\n",
      "U 53 | F 1736704 | FPS 0836 | D 117 | rR:μσmM 0.35 0.44 0.00 0.98 | F:μσmM 94.9 62.4 3.0 144.0 | H 1.024 | V -9.330 | pL 25.412 | vL 870.412 | ∇ 51.599\n",
      "U 54 | F 1769472 | FPS 0836 | D 156 | rR:μσmM 0.39 0.43 0.00 0.98 | F:μσmM 89.2 61.4 3.0 144.0 | H 1.138 | V -9.225 | pL 24.577 | vL 844.264 | ∇ 39.088\n",
      "U 55 | F 1802240 | FPS 0839 | D 195 | rR:μσmM 0.60 0.37 0.00 0.98 | F:μσmM 60.6 53.8 3.0 144.0 | H 1.183 | V -11.117 | pL 18.733 | vL 638.589 | ∇ 29.005\n",
      "U 56 | F 1835008 | FPS 0837 | D 234 | rR:μσmM 0.77 0.27 0.00 0.99 | F:μσmM 36.3 40.0 2.0 144.0 | H 1.111 | V -11.202 | pL 12.463 | vL 434.587 | ∇ 23.575\n",
      "U 57 | F 1867776 | FPS 0815 | D 274 | rR:μσmM 0.87 0.12 0.00 0.99 | F:μσmM 20.5 19.4 2.0 144.0 | H 0.905 | V -11.837 | pL 2.049 | vL 141.989 | ∇ 17.411\n",
      "U 58 | F 1900544 | FPS 0833 | D 314 | rR:μσmM 0.91 0.07 0.00 0.99 | F:μσmM 15.1 11.5 2.0 144.0 | H 0.701 | V -10.802 | pL -2.048 | vL 63.004 | ∇ 12.222\n",
      "U 59 | F 1933312 | FPS 0825 | D 353 | rR:μσmM 0.93 0.04 0.62 0.99 | F:μσmM 11.7 6.5 2.0 61.0 | H 0.505 | V -9.460 | pL -3.911 | vL 46.064 | ∇ 7.533\n",
      "U 60 | F 1966080 | FPS 0823 | D 393 | rR:μσmM 0.93 0.03 0.69 0.99 | F:μσmM 10.5 4.9 2.0 50.0 | H 0.372 | V -8.405 | pL -4.002 | vL 40.809 | ∇ 5.161\n",
      "Status saved\n",
      "U 61 | F 1998848 | FPS 0832 | D 433 | rR:μσmM 0.94 0.02 0.81 0.99 | F:μσmM 9.6 3.7 2.0 31.0 | H 0.262 | V -7.397 | pL -3.896 | vL 36.149 | ∇ 3.920\n",
      "U 62 | F 2031616 | FPS 0821 | D 473 | rR:μσmM 0.94 0.02 0.76 0.99 | F:μσmM 9.2 3.4 2.0 38.0 | H 0.202 | V -6.591 | pL -3.295 | vL 28.867 | ∇ 3.395\n",
      "U 63 | F 2064384 | FPS 0824 | D 512 | rR:μσmM 0.94 0.02 0.82 0.99 | F:μσmM 8.9 2.9 2.0 28.0 | H 0.141 | V -6.034 | pL -3.061 | vL 24.948 | ∇ 2.761\n",
      "U 64 | F 2097152 | FPS 0830 | D 552 | rR:μσmM 0.95 0.02 0.82 0.99 | F:μσmM 8.7 2.7 2.0 29.0 | H 0.093 | V -5.418 | pL -2.625 | vL 20.280 | ∇ 2.189\n",
      "U 65 | F 2129920 | FPS 0821 | D 592 | rR:μσmM 0.95 0.02 0.82 0.99 | F:μσmM 8.6 2.5 2.0 29.0 | H 0.063 | V -4.932 | pL -2.171 | vL 15.343 | ∇ 1.665\n",
      "U 66 | F 2162688 | FPS 0826 | D 631 | rR:μσmM 0.95 0.02 0.86 0.99 | F:μσmM 8.5 2.5 2.0 23.0 | H 0.042 | V -4.493 | pL -1.763 | vL 11.032 | ∇ 1.212\n",
      "U 67 | F 2195456 | FPS 0829 | D 671 | rR:μσmM 0.95 0.02 0.78 0.99 | F:μσmM 8.6 2.6 2.0 35.0 | H 0.036 | V -4.323 | pL -1.440 | vL 8.383 | ∇ 1.271\n",
      "U 68 | F 2228224 | FPS 0819 | D 711 | rR:μσmM 0.95 0.02 0.84 0.99 | F:μσmM 8.5 2.4 2.0 26.0 | H 0.026 | V -3.988 | pL -1.197 | vL 6.145 | ∇ 0.885\n",
      "U 69 | F 2260992 | FPS 0816 | D 751 | rR:μσmM 0.95 0.02 0.84 0.99 | F:μσmM 8.5 2.4 2.0 25.0 | H 0.022 | V -3.829 | pL -0.993 | vL 4.774 | ∇ 0.832\n",
      "Number of frames:  2293760\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        rreturn_total +=return_per_episode['mean']\n",
    "        i+=1\n",
    "        if i >= window:\n",
    "            rreturn_mavg = rreturn_total / i\n",
    "            if rreturn_mavg >= threshold:\n",
    "                break_flag = True \n",
    "                break\n",
    "            else:\n",
    "                i = 0\n",
    "                rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate 2nd environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_clip_impsampling_reshaped', 'seed': 2, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 6000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14454d320>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-WallGapS6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 947.0 | FPS 3614 | D 0.0 | R:μσmM 0.94 0.02 0.88 0.99 | F:μσmM 9.3 3.3 2.0 20.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate 1st environment and test CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_clip_impsampling_reshaped', 'seed': 3, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 6000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14454d320>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 3\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environments, agent and logs, run agent and print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-DoorKey-6x6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 20899.0 | FPS 4311 | D 4.0 | R:μσmM 0.51 0.34 0.00 0.95 | F:μσmM 184.5 120.6 19.0 360.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue learning on 3rd environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-5x5-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-5x5-v0'\n",
    "#env_id = 'MiniGrid-RedBlueDoors-6x6-v0'\n",
    "#env_id = 'MiniGrid-DistShift1-v0'\n",
    "env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "#model = 'MiniGrid-DoorKey-6x6-v0_meta_RIM_5_3_frames_500k_tasks_2_recur_64_16_proc_16_RMSProp_lr_7e4_gae_099_newloop_changeseed'\n",
    "model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "\n",
    "add_frames = 3000000\n",
    "frames = frames + add_frames\n",
    "\n",
    "## Hyper-parameters\n",
    "args = {\n",
    "# General parameters\n",
    "'algo':'ppopc',\n",
    "'env':env_id,\n",
    "'model':model,\n",
    "'seed':1,\n",
    "'log_interval':1,\n",
    "'save_interval':10,\n",
    "'procs':processes,\n",
    "'frames':frames, # default 1e7\n",
    "# Parameters for main algorithm\n",
    "'epochs':4,\n",
    "'batch_size':256,\n",
    "'frames_per_proc':2048, # 128 for PPO and 5 per A2C\n",
    "'discount':0.99,\n",
    "#'lr':3e-4,#0.0001, # for Adam\n",
    "'lr':0.0007, # for RMSProp\n",
    "#'gae_lambda':0.95, # 1 means no gae, for Adam\n",
    "'gae_lambda':0.99, # 1 means no gae, for RMSProp\n",
    "'entropy_coef': 0.01,\n",
    "'value_loss_coef':0.5,\n",
    "'max_grad_norm':0.5,\n",
    "'optim_eps':1e-8,\n",
    "'optim_alpha':0.99,\n",
    "'clip_eps':0.2,\n",
    "'recurrence':1, # if > 1, a LSTM is added\n",
    "'text':False, # add a GRU for text input\n",
    "# Model Parameters\n",
    "'cascade_depth':8,\n",
    "'flow_factor':1.0,\n",
    "'mesh_factor':4.0,\n",
    "'lr_decay':True,\n",
    "'imp_sampling':'clipped',\n",
    "'imp_clips': [-5,5],\n",
    "'dynamic_neglogpacs':False,\n",
    "'optimizer_type':'rmsprop',\n",
    "#'optimizer_type':'adam',\n",
    "'scheduler_flag':False,\n",
    "'var_init':'saved', #'equal',\n",
    "'reshape_reward':True\n",
    "}\n",
    "\n",
    "#args = utils.dotdict(args)\n",
    "args = DictList(args)\n",
    "\n",
    "args.mem = args.recurrence > 1\n",
    "\n",
    "# Create decay learning rates and clip ranges\n",
    "if isinstance(args.lr, float):\n",
    "    if args.lr_decay:\n",
    "        args.lrs = decayfn_arr(args.lr,1.0/args.mesh_factor, args.cascade_depth) # learning rates exponentially smaller for deeper policies in cascade\n",
    "        args.lrs_fn = decayfn_arr_2(args.lr,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.lrs = constfn_arr(args.lr,args.cascade_depth)\n",
    "        args.lrs_fn = None\n",
    "else: assert callable(args.lr)\n",
    "if isinstance(args.clip_eps, float):\n",
    "    if args.lr_decay:\n",
    "        args.clipranges = decayfn_arr(args.clip_eps,1.0/args.mesh_factor, args.cascade_depth)\n",
    "    else:\n",
    "        args.clipranges = constfn_arr(args.clip_eps, args.cascade_depth)\n",
    "\n",
    "else: assert callable(args.clip_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-SimpleCrossingS9N2-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped', 'seed': 1, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 9000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14296e680>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05]}\n",
      "\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "default_model_name = f\"{args.env}_{args.algo}_seed{args.seed}_{date}\"\n",
    "\n",
    "model_name = args.model or default_model_name\n",
    "model_dir = utils.get_model_dir(model_name)\n",
    "\n",
    "# Load loggers and Tensorboard writer\n",
    "\n",
    "txt_logger = utils.get_txt_logger(model_dir)\n",
    "csv_file, csv_logger = utils.get_csv_logger(model_dir)\n",
    "tb_writer = tensorboardX.SummaryWriter(model_dir)\n",
    "\n",
    "# Log command and all script arguments\n",
    "\n",
    "#txt_logger.info(\"{}\\n\".format(\" \".join(sys.argv)))\n",
    "txt_logger.info(\"{}\\n\".format(args))\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "txt_logger.info(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environments loaded\n",
      "\n",
      "Training status loaded\n",
      "\n",
      "Observations preprocessor loaded\n",
      "Model loaded\n",
      "\n",
      "ACModel(\n",
      "  (image_conv): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden policies initialized with previous saved states\n",
      "Hidden models loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load environments\n",
    "\n",
    "envs = []\n",
    "for i in range(args.procs):\n",
    "    envs.append(utils.make_env(args.env, args.seed + 10000 * i))\n",
    "txt_logger.info(\"Environments loaded\\n\")\n",
    "\n",
    "# Load training status\n",
    "\n",
    "try:\n",
    "    status = utils.get_status(model_dir)\n",
    "except OSError:\n",
    "    status = {\"num_frames\": 0, \"update\": 0}\n",
    "txt_logger.info(\"Training status loaded\\n\")\n",
    "\n",
    "# Load observations preprocessor\n",
    "\n",
    "obs_space, preprocess_obss = utils.get_obss_preprocessor(envs[0].observation_space)\n",
    "if \"vocab\" in status:\n",
    "    preprocess_obss.vocab.load_vocab(status[\"vocab\"])\n",
    "txt_logger.info(\"Observations preprocessor loaded\")\n",
    "\n",
    "# Reshape reward function\n",
    "if args.reshape_reward:\n",
    "    def reshape_reward(obs, action, reward, done):\n",
    "        if not done:\n",
    "            reward = -1\n",
    "        else:\n",
    "            reward = 1\n",
    "        return reward\n",
    "else:\n",
    "    reshape_reward = None\n",
    "\n",
    "# Load model\n",
    "\n",
    "acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "if \"model_state\" in status:\n",
    "    acmodel.load_state_dict(status[\"model_state\"])\n",
    "acmodel.to(device)\n",
    "txt_logger.info(\"Model loaded\\n\")\n",
    "txt_logger.info(\"{}\\n\".format(acmodel))\n",
    "\n",
    "# Create hidden policies\n",
    "\n",
    "acmodels = [acmodel]\n",
    "\n",
    "for k in range(1, args.cascade_depth):\n",
    "    hidden_acmodel = ACModel(obs_space=obs_space, action_space=envs[0].action_space, use_memory=args.mem, use_text=args.text)\n",
    "    hidden_acmodel.to(device)\n",
    "    # Initialise all hidden policy networks to same as visible policy\n",
    "    if args.var_init == \"equal\":\n",
    "        print(\"Hidden policies initialized as visible policy\")\n",
    "        hidden_acmodel.load_state_dict(acmodel.state_dict())\n",
    "    elif args.var_init == \"saved\":\n",
    "        print(\"Hidden policies initialized with previous saved states\")\n",
    "        hidden_acmodel.load_state_dict(status[\"hidden_model_states\"][k-1])\n",
    "    acmodels.append(hidden_acmodel)\n",
    "txt_logger.info(\"Hidden models loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer loaded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load algo\n",
    "\n",
    "if args.algo == \"a2c\":\n",
    "    algo = torch_ac.A2CAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_alpha, args.optim_eps, preprocess_obss)\n",
    "elif args.algo == \"ppo\":\n",
    "    algo = torch_ac.PPOAlgo(envs, acmodel, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss)\n",
    "elif args.algo == \"ppopc\":\n",
    "    algo = torch_ac.PPOPCAlgo(envs, acmodels, device, args.frames_per_proc, args.discount, args.lr, args.gae_lambda,\n",
    "                            args.entropy_coef, args.value_loss_coef, args.max_grad_norm, args.recurrence,\n",
    "                            args.optim_eps, args.clip_eps, args.epochs, args.batch_size, preprocess_obss, args.cascade_depth, args.flow_factor, args.mesh_factor, args.imp_sampling, args.imp_clips, args.dynamic_neglogpacs, args.lrs, args.lrs_fn, args.clipranges, args.optimizer_type, args.scheduler_flag, reshape_reward)                              \n",
    "else:\n",
    "    raise ValueError(\"Incorrect algorithm name: {}\".format(args.algo))\n",
    "\n",
    "if \"optimizer_state\" in status:\n",
    "    algo.optimizer.load_state_dict(status[\"optimizer_state\"])\n",
    "txt_logger.info(\"Optimizer loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U 61 | F 1998848 | FPS 0762 | D 42 | rR:μσmM 0.59 0.39 0.00 0.96 | F:μσmM 137.8 126.7 14.0 324.0 | H 0.858 | V -13.677 | pL 28.726 | vL 1118.038 | ∇ 62.790\n",
      "U 62 | F 2031616 | FPS 0752 | D 86 | rR:μσmM 0.65 0.35 0.00 0.96 | F:μσmM 120.3 114.1 15.0 324.0 | H 1.017 | V -14.820 | pL 24.913 | vL 932.368 | ∇ 46.410\n",
      "U 63 | F 2064384 | FPS 0758 | D 129 | rR:μσmM 0.68 0.32 0.00 0.96 | F:μσmM 111.4 105.2 14.0 324.0 | H 1.081 | V -15.295 | pL 23.323 | vL 854.402 | ∇ 40.493\n",
      "U 64 | F 2097152 | FPS 0791 | D 171 | rR:μσmM 0.71 0.29 0.00 0.96 | F:μσmM 99.4 95.7 14.0 324.0 | H 1.156 | V -15.511 | pL 21.258 | vL 781.626 | ∇ 37.196\n",
      "U 65 | F 2129920 | FPS 0791 | D 212 | rR:μσmM 0.72 0.29 0.00 0.96 | F:μσmM 97.4 94.7 14.0 324.0 | H 1.188 | V -16.531 | pL 20.457 | vL 729.243 | ∇ 35.055\n",
      "U 66 | F 2162688 | FPS 0728 | D 257 | rR:μσmM 0.74 0.25 0.00 0.96 | F:μσmM 91.4 85.8 13.0 324.0 | H 1.241 | V -16.742 | pL 19.262 | vL 673.609 | ∇ 37.096\n",
      "U 67 | F 2195456 | FPS 0774 | D 300 | rR:μσmM 0.76 0.25 0.00 0.96 | F:μσmM 82.8 81.6 15.0 324.0 | H 1.225 | V -17.037 | pL 17.501 | vL 608.815 | ∇ 35.975\n",
      "U 68 | F 2228224 | FPS 0769 | D 342 | rR:μσmM 0.77 0.22 0.00 0.96 | F:μσmM 82.4 74.8 14.0 324.0 | H 1.283 | V -17.738 | pL 16.769 | vL 571.613 | ∇ 35.392\n",
      "U 69 | F 2260992 | FPS 0793 | D 383 | rR:μσmM 0.77 0.23 0.00 0.96 | F:μσmM 81.1 77.6 14.0 324.0 | H 1.315 | V -17.827 | pL 16.503 | vL 565.214 | ∇ 36.379\n",
      "U 70 | F 2293760 | FPS 0795 | D 425 | rR:μσmM 0.77 0.23 0.00 0.96 | F:μσmM 82.3 78.3 14.0 324.0 | H 1.327 | V -18.261 | pL 16.694 | vL 572.774 | ∇ 35.229\n",
      "Status saved\n",
      "U 71 | F 2326528 | FPS 0803 | D 466 | rR:μσmM 0.78 0.22 0.00 0.96 | F:μσmM 76.7 73.9 14.0 324.0 | H 1.289 | V -18.413 | pL 14.827 | vL 506.102 | ∇ 38.105\n",
      "U 72 | F 2359296 | FPS 0779 | D 508 | rR:μσmM 0.82 0.17 0.00 0.96 | F:μσmM 65.0 58.1 13.0 324.0 | H 1.247 | V -18.321 | pL 12.711 | vL 427.683 | ∇ 35.921\n",
      "U 73 | F 2392064 | FPS 0791 | D 549 | rR:μσmM 0.80 0.20 0.00 0.96 | F:μσmM 72.6 69.5 13.0 324.0 | H 1.304 | V -18.971 | pL 13.879 | vL 475.705 | ∇ 37.716\n",
      "U 74 | F 2424832 | FPS 0811 | D 589 | rR:μσmM 0.80 0.19 0.00 0.96 | F:μσmM 69.8 64.7 13.0 324.0 | H 1.277 | V -18.988 | pL 13.195 | vL 449.854 | ∇ 37.305\n",
      "U 75 | F 2457600 | FPS 0796 | D 631 | rR:μσmM 0.82 0.19 0.00 0.96 | F:μσmM 63.2 65.0 13.0 324.0 | H 1.254 | V -18.920 | pL 12.726 | vL 452.270 | ∇ 40.750\n",
      "U 76 | F 2490368 | FPS 0751 | D 674 | rR:μσmM 0.84 0.16 0.00 0.96 | F:μσmM 56.8 55.3 13.0 324.0 | H 1.222 | V -19.110 | pL 10.203 | vL 368.282 | ∇ 37.076\n",
      "U 77 | F 2523136 | FPS 0815 | D 714 | rR:μσmM 0.83 0.18 0.00 0.96 | F:μσmM 58.8 60.0 13.0 324.0 | H 1.245 | V -19.287 | pL 10.458 | vL 379.486 | ∇ 42.254\n",
      "U 78 | F 2555904 | FPS 0803 | D 755 | rR:μσmM 0.85 0.15 0.00 0.96 | F:μσmM 52.8 53.6 13.0 324.0 | H 1.176 | V -19.306 | pL 9.026 | vL 351.936 | ∇ 38.897\n",
      "U 79 | F 2588672 | FPS 0796 | D 796 | rR:μσmM 0.87 0.14 0.00 0.96 | F:μσmM 47.3 47.6 13.0 324.0 | H 1.090 | V -19.294 | pL 6.957 | vL 293.469 | ∇ 37.951\n",
      "U 80 | F 2621440 | FPS 0766 | D 839 | rR:μσmM 0.88 0.13 0.00 0.96 | F:μσmM 44.3 45.2 13.0 324.0 | H 1.029 | V -19.313 | pL 6.361 | vL 288.109 | ∇ 40.299\n",
      "Status saved\n",
      "U 81 | F 2654208 | FPS 0772 | D 882 | rR:μσmM 0.89 0.11 0.00 0.96 | F:μσmM 39.8 39.9 13.0 324.0 | H 0.951 | V -19.004 | pL 4.220 | vL 238.561 | ∇ 38.662\n",
      "U 82 | F 2686976 | FPS 0734 | D 926 | rR:μσmM 0.90 0.09 0.00 0.96 | F:μσmM 37.5 33.8 13.0 324.0 | H 0.894 | V -18.934 | pL 3.117 | vL 204.620 | ∇ 37.086\n",
      "U 83 | F 2719744 | FPS 0805 | D 967 | rR:μσmM 0.91 0.07 0.39 0.96 | F:μσmM 31.6 24.0 13.0 221.0 | H 0.791 | V -17.986 | pL 0.088 | vL 137.676 | ∇ 33.311\n",
      "U 84 | F 2752512 | FPS 0800 | D 1008 | rR:μσmM 0.92 0.07 0.00 0.96 | F:μσmM 29.9 26.2 13.0 324.0 | H 0.733 | V -17.774 | pL 0.084 | vL 158.071 | ∇ 35.285\n",
      "U 85 | F 2785280 | FPS 0817 | D 1048 | rR:μσmM 0.93 0.07 0.15 0.96 | F:μσmM 26.9 23.7 13.0 306.0 | H 0.626 | V -17.042 | pL -1.349 | vL 132.184 | ∇ 33.158\n",
      "U 86 | F 2818048 | FPS 0811 | D 1088 | rR:μσmM 0.93 0.05 0.41 0.96 | F:μσmM 25.7 18.4 13.0 211.0 | H 0.554 | V -16.471 | pL -2.007 | vL 109.101 | ∇ 34.729\n",
      "U 87 | F 2850816 | FPS 0802 | D 1129 | rR:μσmM 0.93 0.04 0.52 0.96 | F:μσmM 23.8 15.4 13.0 174.0 | H 0.486 | V -16.077 | pL -3.073 | vL 94.512 | ∇ 28.328\n",
      "U 88 | F 2883584 | FPS 0769 | D 1172 | rR:μσmM 0.94 0.04 0.29 0.96 | F:μσmM 21.6 14.0 13.0 254.0 | H 0.363 | V -15.119 | pL -3.532 | vL 92.637 | ∇ 27.280\n",
      "U 89 | F 2916352 | FPS 0803 | D 1213 | rR:μσmM 0.94 0.03 0.49 0.96 | F:μσmM 21.2 11.9 13.0 184.0 | H 0.336 | V -14.862 | pL -3.906 | vL 83.080 | ∇ 25.364\n",
      "Number of frames:  2949120\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "num_frames = status[\"num_frames\"]\n",
    "update = status[\"update\"]\n",
    "start_time = time.time()\n",
    "#nupdates = args.frames // (args.procs * args.frames_per_proc)\n",
    "\n",
    "# Moving average parameters\n",
    "threshold = 0.90\n",
    "window = 10\n",
    "rreturn_total = 0\n",
    "i = 0\n",
    "\n",
    "while num_frames < args.frames:\n",
    "\n",
    "    update_start_time = time.time()\n",
    "\n",
    "    # Collect experiences for cascade policies\n",
    "    exps, logs1 = algo.collect_experiences_cascade()\n",
    "\n",
    "    # Update model parameters\n",
    "    logs2 = algo.update_parameters_cascade(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "    update_end_time = time.time()\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # Print logs\n",
    "\n",
    "    if update % args.log_interval == 0:\n",
    "        fps = logs[\"num_frames\"]/(update_end_time - update_start_time)\n",
    "        duration = int(time.time() - start_time)\n",
    "        return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "        rreturn_per_episode = utils.synthesize(logs[\"reshaped_return_per_episode\"])\n",
    "        num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "        # Moving average to break loop if mean reward threshold reached\n",
    "        #rreturn_total +=rreturn_per_episode['mean']\n",
    "        rreturn_total +=return_per_episode['mean']\n",
    "        i+=1\n",
    "        if i >= window:\n",
    "            rreturn_mavg = rreturn_total / i\n",
    "            if rreturn_mavg >= threshold:\n",
    "                break_flag = True \n",
    "                break\n",
    "            else:\n",
    "                i = 0\n",
    "                rreturn_total = 0\n",
    "\n",
    "        header = [\"update\", \"frames\", \"FPS\", \"duration\"]\n",
    "        data = [update, num_frames, fps, duration]\n",
    "        # header += [\"rreturn_\" + key for key in rreturn_per_episode.keys()]\n",
    "        # data += rreturn_per_episode.values()\n",
    "        header += [\"rreturn_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "        header += [\"num_frames_\" + key for key in num_frames_per_episode.keys()]\n",
    "        data += num_frames_per_episode.values()\n",
    "        header += [\"entropy\", \"value\", \"policy_loss\", \"value_loss\", \"grad_norm\"]\n",
    "        data += [logs[\"entropy\"], logs[\"value\"], logs[\"policy_loss\"], logs[\"value_loss\"], logs[\"grad_norm\"]]\n",
    "\n",
    "        txt_logger.info(\n",
    "            \"U {} | F {:06} | FPS {:04.0f} | D {} | rR:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {} | H {:.3f} | V {:.3f} | pL {:.3f} | vL {:.3f} | ∇ {:.3f}\"\n",
    "            .format(*data))\n",
    "\n",
    "        header += [\"return_\" + key for key in return_per_episode.keys()]\n",
    "        data += return_per_episode.values()\n",
    "\n",
    "        if status[\"num_frames\"] == 0:\n",
    "            csv_logger.writerow(header)\n",
    "        csv_logger.writerow(data)\n",
    "        csv_file.flush()\n",
    "\n",
    "        for field, value in zip(header, data):\n",
    "            tb_writer.add_scalar(field, value, num_frames)\n",
    "\n",
    "    # Save status\n",
    "\n",
    "    if args.save_interval > 0 and update % args.save_interval == 0:\n",
    "        status = {\"num_frames\": num_frames, \"update\": update,\n",
    "                  \"model_state\": acmodels[0].state_dict(), \"optimizer_state\": algo.optimizer.state_dict()}\n",
    "        if hasattr(preprocess_obss, \"vocab\"):\n",
    "            status[\"vocab\"] = preprocess_obss.vocab.vocab\n",
    "        if args.cascade_depth > 1:\n",
    "            status[\"hidden_model_states\"] = [acmodels[k].state_dict() for k in range(1, args.cascade_depth)]\n",
    "        utils.save_status(status, model_dir)\n",
    "        txt_logger.info(\"Status saved\")\n",
    "\n",
    "print(\"Number of frames: \", num_frames)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate 3rd environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-SimpleCrossingS9N2-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped', 'seed': 2, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 9000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14296e680>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "env_id = 'MiniGrid-SimpleCrossingS9N2-v0'\n",
    "\n",
    "args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 2\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-SimpleCrossingS9N2-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 3572.0 | FPS 3408 | D 1.0 | R:μσmM 0.90 0.07 0.60 0.96 | F:μσmM 34.5 26.7 13.0 145.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate 1st environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-DoorKey-6x6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped', 'seed': 3, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 9000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14296e680>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "#env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 3\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-DoorKey-6x6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 27381.0 | FPS 3574 | D 7.0 | R:μσmM 0.30 0.31 0.00 0.93 | F:μσmM 261.9 106.8 27.0 360.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluate 2nd environment and test CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algo': 'ppopc', 'env': 'MiniGrid-WallGapS6-v0', 'model': 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped', 'seed': 3, 'log_interval': 1, 'save_interval': 10, 'procs': 16, 'frames': 9000000.0, 'epochs': 4, 'batch_size': 256, 'frames_per_proc': 2048, 'discount': 0.99, 'lr': 0.0007, 'gae_lambda': 0.99, 'entropy_coef': 0.01, 'value_loss_coef': 0.5, 'max_grad_norm': 0.5, 'optim_eps': 1e-08, 'optim_alpha': 0.99, 'clip_eps': 0.2, 'recurrence': 1, 'text': False, 'cascade_depth': 8, 'flow_factor': 1.0, 'mesh_factor': 4.0, 'lr_decay': True, 'imp_sampling': 'clipped', 'imp_clips': [-5, 5], 'dynamic_neglogpacs': False, 'optimizer_type': 'rmsprop', 'scheduler_flag': False, 'var_init': 'saved', 'reshape_reward': True, 'mem': False, 'lrs': [0.0007, 0.000175, 4.375e-05, 1.09375e-05, 2.734375e-06, 6.8359375e-07, 1.708984375e-07, 4.2724609375e-08], 'lrs_fn': <function decayfn_arr_2.<locals>.f at 0x14296e680>, 'clipranges': [0.2, 0.05, 0.0125, 0.003125, 0.00078125, 0.0001953125, 4.8828125e-05, 1.220703125e-05], 'episodes': 100, 'argmax': False, 'worst_episodes_to_show': None}\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#env_id = 'MiniGrid-Empty-Random-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-6x6-v0'\n",
    "#env_id = 'MiniGrid-DoorKey-8x8-v0'\n",
    "#env_id = 'MiniGrid-Empty-8x8-v0'\n",
    "#env_id = 'MiniGrid-LavaGapS5-v0'\n",
    "env_id = 'MiniGrid-WallGapS6-v0'\n",
    "\n",
    "args.model = 'test_cascade_8_frames_2048_doorkey_wallgap_crossing_clip_impsampling_reshaped'\n",
    "## Hyper-parameters\n",
    "args.env = env_id\n",
    "args.episodes = 100\n",
    "args.seed = 3\n",
    "args.argmax = False\n",
    "args.worst_episodes_to_show = None\n",
    "print(args)\n",
    "\n",
    "\n",
    "# Set seed for all randomness sources\n",
    "utils.seed(args.seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: MiniGrid-WallGapS6-v0 \n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_1 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_2 completed\n",
      "\n",
      "Environments loaded\n",
      "Agent loaded\n",
      "Agent run_3 completed\n",
      "\n",
      "F 1175.0 | FPS 3473 | D 0.0 | R:μσmM 0.93 0.04 0.75 0.99 | F:μσmM 11.7 6.8 2.0 40.0\n"
     ]
    }
   ],
   "source": [
    "num_frames_list = []\n",
    "fps_list = []\n",
    "duration_list = []\n",
    "return_per_episode_list = []\n",
    "num_frames_per_episode_list = []\n",
    "seed_list = [10, 20, 30]\n",
    "\n",
    "print(\"Env:\", args.env, \"\\n\")\n",
    "\n",
    "for n, seed in enumerate(seed_list):\n",
    "\n",
    "    # Load environments\n",
    "\n",
    "    envs = []\n",
    "    for i in range(args.procs):\n",
    "        env = utils.make_env(args.env, seed + 10000 * i)\n",
    "        envs.append(env)\n",
    "    env = ParallelEnv(envs)\n",
    "    print(\"Environments loaded\")\n",
    "\n",
    "    # Load agent\n",
    "\n",
    "    model_dir = utils.get_model_dir(args.model)\n",
    "    agent = utils.Agent(obs_space=env.observation_space, action_space=env.action_space, model_dir=model_dir, device=device, argmax=args.argmax)\n",
    "    print(\"Agent loaded\")\n",
    "\n",
    "    # Initialize logs\n",
    "\n",
    "    logs = {\"num_frames_per_episode\": [], \"return_per_episode\": []}\n",
    "\n",
    "    # Run agent\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    obss = env.reset()\n",
    "\n",
    "    log_done_counter = 0\n",
    "    log_episode_return = torch.zeros(args.procs, device=device)\n",
    "    log_episode_num_frames = torch.zeros(args.procs, device=device)\n",
    "\n",
    "    while log_done_counter < args.episodes:\n",
    "        actions = agent.get_actions(obss)\n",
    "        obss, rewards, dones, _ = env.step(actions)\n",
    "        agent.analyze_feedbacks(rewards, dones)\n",
    "\n",
    "        log_episode_return += torch.tensor(rewards, device=device, dtype=torch.float)\n",
    "        log_episode_num_frames += torch.ones(args.procs, device=device)\n",
    "\n",
    "        for i, done in enumerate(dones):\n",
    "            if done:\n",
    "                log_done_counter += 1\n",
    "                logs[\"return_per_episode\"].append(log_episode_return[i].item())\n",
    "                logs[\"num_frames_per_episode\"].append(log_episode_num_frames[i].item())\n",
    "\n",
    "        mask = 1 - torch.tensor(dones, device=device, dtype=torch.float)\n",
    "        log_episode_return *= mask\n",
    "        log_episode_num_frames *= mask\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Agent run_{} completed\\n\" .format(n+1))\n",
    "\n",
    "    num_frames = sum(logs[\"num_frames_per_episode\"])\n",
    "    fps = num_frames/(end_time - start_time)\n",
    "    duration = int(end_time - start_time)\n",
    "    return_per_episode = utils.synthesize(logs[\"return_per_episode\"])\n",
    "    num_frames_per_episode = utils.synthesize(logs[\"num_frames_per_episode\"])\n",
    "\n",
    "    # Acumulate logs per agent\n",
    "\n",
    "    num_frames_list.append(num_frames)\n",
    "    fps_list.append(fps)\n",
    "    duration_list.append(duration)\n",
    "    return_per_episode_list.append(np.fromiter(return_per_episode.values(), float))\n",
    "    num_frames_per_episode_list.append(np.fromiter(num_frames_per_episode.values(), float))\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "num_frames_tot = np.array(num_frames_list, ndmin=2)\n",
    "fps_tot = np.array(fps_list, ndmin=2)\n",
    "duration_tot = np.array(duration_list, ndmin=2)\n",
    "return_per_episode_tot = np.array(return_per_episode_list, ndmin=2)\n",
    "num_frames_per_episode_tot = np.array(num_frames_per_episode_list, ndmin=2)\n",
    "\n",
    "# Print logs\n",
    "\n",
    "print(\"F {} | FPS {:.0f} | D {} | R:μσmM {:.2f} {:.2f} {:.2f} {:.2f} | F:μσmM {:.1f} {:.1f} {} {}\"\n",
    "      .format(np.median(num_frames_tot, axis=0)[0], np.median(fps_tot, axis=0)[0], np.median(duration_tot, axis=0)[0], *np.median(return_per_episode_tot, axis=0), *np.median(num_frames_per_episode_tot, axis=0)))\n",
    "\n",
    "#return_per_episode_tot = np.array(return_per_episode_tot, ndim=2)\n",
    "\n",
    "# Print worst episodes\n",
    "if args.worst_episodes_to_show:\n",
    "    n = args.worst_episodes_to_show\n",
    "    if n > 0:\n",
    "        print(\"\\n{} worst episodes:\".format(n))\n",
    "\n",
    "        indexes = sorted(range(len(logs[\"return_per_episode\"])), key=lambda k: logs[\"return_per_episode\"][k])\n",
    "        for i in indexes[:n]:\n",
    "            print(\"- episode {}: R={}, F={}\".format(i, logs[\"return_per_episode\"][i], logs[\"num_frames_per_episode\"][i]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "test_minigrid_sb3_curriculum.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfm-experiments",
   "language": "python",
   "name": "tfm-experiments"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "5d2efec84aee61a766032e9dfbe418d90107ced57033c9077d1ba6267f248fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
